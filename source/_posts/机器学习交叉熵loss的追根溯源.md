---
title: 机器学习交叉熵loss的追根溯源
date: 2020-08-26 23:41:35
mathjax: true
tags:
- 交叉熵
- 最大似然
- KL散度
- 相对熵
categories:
- 机器学习
---

>虽说交叉熵也算是机器学习常用的损失函数了，其想达到的目标很好理解，但是其中蕴藏的数学原理，我却一直似懂非懂，从未曾深入探究过。本文就本着钻研精神，来细细探究​交叉熵损失函数的来源以及具体的数学原理。当然，本文可能限于姿势水平，有些问题比较小白，当然我个人也希望讲解的更基础一点，而且还有一些问题往往也只能达到“知其然，而不知其所以然”的水平，如有错误或者疏漏，也欢迎各位大佬斧正。

<!--more-->

在具体讲解前，我引出几个小问题，大家感兴趣的话也可以先思考一下，我也会在本文中随着讲解慢慢解答这几个问题。

- 什么是信息熵？什么是交叉熵？什么是相对熵(KL​散度)？
- 如何从最优编码bit​数的角度理解上述概念?
- 交叉熵被作为机器学习损失函数的必然性？
- 分类任务为什么使用交叉熵，MSE​不行吗?

## 伯努利分布

二元概率模型: 如果随机变量$X$只有两个取值$0,1$, 有$P(X=0)=p, p(X=1)=1-p$, 那么$X$关于$p$条件下的概率密度函数为
$$
f(x|p)=p^{x}(1-p)^{1-x}
$$

注意到这只是一个二元的, 那$x$不就两种情况嘛, 大家代入一下是很显然的~

## 最大似然(MLE)

什么是最大似然方法呢？举个简单的例子，氪金游戏的一大特色就是概率，比如抽卡的，大家是不是经常怀疑卡池里根本没有放入这张卡或者自己几率被暗调了？实际上这里大家就已经用到了最大似然方法。因为大家**根据当前已经发生的事情推测抽到这张卡的概率为0**，这本身就是一个求最大似然的过程。那我们不妨以此为例来引入$MLE$，一张卡$A$被抽到的概率为$p$, 那么每次抽卡只有两种情况:抽到$A$和没抽到$A$，记一次抽卡的结果为一个随机变量$X$,那么显然，这就是一个伯努利分布，我们容易得到
$$
f(x|p)=p^{x}(1-p)^{1-x}
$$
那么最大似然是在做一件什么事？它希望依据目前的情况来推演$p$的值，使得当前事件发生的概率最大。也就是说假设我们观测到了一个随机过程，比如$x={x_1,x_2,x_3, \cdots,x_n}$，每一个$x_i$对应的都是一个随机变量$X$，他们之间相互独立。对于抽卡来说，就是我们抽了$n$次卡，那么最大似然的公式就如下:
$$
p = \mathop{\operatorname{argmax}}\limits_{p} \prod_{i=1}^n f(x_i|p)
$$
因为$x_i$都是已经发生的，那么只有$p$是变量，之后就是求极值了。那么这个式子为什么这样呢？其实这本身蕴含了一种假设，就是**已经发生的事件发生的概率是最大**。比如大家抽卡时的思想"我抽了100抽都没中，卡池里根本没有这张卡！！",  那么此时我们就是依据最大似然来推断了$p$的值，也就是$p$就是0。具体的用公式来求解的话，对于100次抽卡就有
$$
\begin{aligned}
p &= \mathop{\operatorname{argmax}}\limits_{p} \prod_{i=1}^{100} (1-p) \\
&=\mathop{\operatorname{argmax}}\limits_{p} (1-p)^{100} \\
&=0
\end{aligned}
$$

## 交叉熵

接下来讲讲交叉熵(CE)，相信大家接触机器学习中的分类任务，可能首先接触到的就是这么一个损失函数。
$$
l=-\frac{1}{N}\sum_{i=1}^Ny\ln \widehat y
$$
其思想很自然，但是其中蕴藏了什么知识呢？如果稍微追根溯源一下的话，我们就会接触到`信息熵`这个概念(如果你和我一样是一个本科学过通信原理的计算机人，希望你能回想起被通信原理支配的恐惧)，那么信息熵(以$2$为底时)的表示如下:
$$
H(x)=-\sum_{i=1}^Np(x)\log_2 {p(x)}
$$
信息熵衡量的是信息量的期望，或者说叫信道的不确定性，单位是bit。最简单的，比如发送方在信道只发送$0,1$字符，如果发送$1$字符的概率为$1$, 发送$0$字符的概率为$0$，那么传递的信息量依据上式计算就是$0$(注意$\lim\limits_{x\to0^+}  xlnx=0$)。这似乎是显然的，因为如果信道只发送字符$1$,那么接受者似乎接收不到任何有价值的信息, 因为我知道你一定会发$1$，这里面就没有任何不确定性。那么什么时候不确定性最大呢？显然是$P(X=1)=P(X=0)=\frac{1}{2}$的时候，因为这时候$0,1$等概率，我猜都不好猜，不确定性最大。数学证明也很简单，拉格朗日乘数法秒解。不过我这里并不想从这个角度来引入信息熵，实际上对于学过计网或者通原的同学来说，大家肯定对信道编码不陌生。

首先我引入一个问题:

> 对于一个字符集$\{s_1,s_2,s_3, \cdots, s_n\}$，每个符号被发送的概率依次为$\{p(s_1),p(s_2),p(s_3),\cdots, p(s_n)\}$，那么字符的最小编码的平均bit数是多少呢？

实际上这个平均bit数就是信息熵，但是我这里不给出证明，因为想要对其严格证明实在过于繁琐，我尝试写了一页证明，也没写完。浪费了一下午的时间QAQ。另外如果给出证明，那这篇博客放眼望去全是公式，肯定没人喜欢看...而我也更多的是想和大家分享一种我自己或者我看到的见解，而不是班门弄斧的敲数学公式，毕竟数学很容易出错...有兴趣的同学可以尝试证明之，提示:数列均值极限定理，斯特林数，大数定理。

这里我仅引入一个特例来说明他确实是最小编码的平均bit数。大家应该知道哈夫曼编码，实际上哈夫曼编码就是一种最优的编码方式。它的长度是最短的。那么实际上哈夫曼编码如果对于这么一个字符集比如$\{s_1,s_2,s_3,s_4\}$，他们的概率又都是$\{\frac{1}{2},\frac{1}{4},\frac{1}{8},\frac{1}{8}\}$来说，其平均查找长度就可以写成信息熵的形式。

<img src="G:\tsotfsk.github.io\source\_posts\推荐系统常用的loss函数汇总\image-20200829201110798.png" alt="image-20200829201110798" style="zoom:50%;" />

显然，平均编码(查找)长度就是
$$
\begin{aligned}
L &= \frac{1}{2} \times 1 + \frac{1}{4} \times 2+\frac{1}{8} \times 3 + \frac{1}{8} \times 3 \\

 &= \frac{1}{2} \times \log_22 + \frac{1}{4} \times \log_24 +\frac{1}{8} \times \log_28 + \frac{1}{8} \times \log_28 \\
 &= -\sum_{i=1}^{n}p(s_i)\log_2 p(s_i)
\end{aligned}
$$
也就是说信息熵是可以表达最短平均编码长度，其中$-\log p(x)$可以反映字符$x$的信息编码长度。实际上在我刚才举的关于信道的例子中，我们默认信道是不会出错的，也就是不存在干扰。但是真实的信道显然不会如此，不然也不会做一些差错编码以及增加校验帧了。那么假设我们信道依然发送$0,1$字符，但是信道可能出错，那么就会有如下的情况发生。

<img src="G:\tsotfsk.github.io\source\_posts\推荐系统常用的loss函数汇总\image-20200829124157734.png" alt="image-20200829124157734" style="zoom:50%;" />

假设左侧作为发送方，信道存在差错，对于发送方，其发送的信息满足于一个伯努利分布$X \sim B(p)$，对于接收方，其收到的信息满足一个伯努利分布$Y \sim B(q)$。那么现在问题来了，现在对于接收方来说，它得到的信息熵改变了吗？变多了还是变少了。我们将此时由这两个分布共同产生的信息熵定义为交叉熵。记为$H(X,Y)$，那么其公式如下
$$
H(X,Y)=-\sum_{i = 1}^{|S|}p(s_i)\log q(s_i)
$$
为什么长这样呢？实际上作为信息的接收方，我们如果拿得到的分布来做编码，那么此时的平均编码长度就是上式子~显然如果$Y$与$X$的分布不相等，那么这个平均编码的长度是要大于$X$平均编码的长度的。因为我们知道$X$的信息熵对应的编码是最优编码，你换了一套编码自然会比这个大，这也符合我们的直觉，那就是引入噪声使得信息熵增大了，其加大了不确定度，也增大了平均编码长度。也就是说有
$$
H(X,Y)-H(X)>0
$$
严格证明参考**Gibbs inequality**。那么这个差值又是什么呢？实际上这个差值叫$KL$散度，也叫相对熵。相对熵描述的更像是一个信息差。那么相对熵就有如下等式
$$
H(X|Y) = H(X,Y)-H(X)
$$
其衡量的也是两个分布之间的差异，这个差异是非对称的。如下有个图比较直观

<img src="G:\tsotfsk.github.io\source\_posts\推荐系统常用的loss函数汇总\图片1-1598712911224.png" alt="图片1" style="zoom:50%;" />

从文氏图也能比较清楚的看到，相对熵更像是**一个分布比另外一个分布多出来的信息量**。所以严格来讲，$KL$散度描述的并不是两个分布的距离，而更像是两个分布的相对差异。

从平均编码长度的角度来说。**信息熵描述了最优编码的长度。交叉熵描述了非最优编码的平均长度。而相对熵则描述的是非最优编码比最优编码多出来的长度。**

### 交叉熵被作为机器学习损失函数的必然性？

首先，这里有一个误区。我们机器学习真的使用的是交叉熵吗？不是。实际上我们真正使用的是$KL$散度。为什么呢？因为$KL$散度之于机器学习，其描述的就是**输出的数据分布的与模型输入的分布的差距 **，那么将其作为损失函数的思想是很自然的，因为我们希望模型能够捕捉到输入数据的分布，来达到输出的分布与输入的保持一致。但是，为什么我们用到的都是交叉熵呢？因为KL​散度和交叉熵的最小化在机器学习中是等价的。注意到交叉熵的公式
$$
H(X,Y) = H(X|Y)+H(X)
$$
在机器学习中，$X$就是数据，数据的分布是一个先验分布，换句话说，它是已知的。那么$H(X)$就是一个定值。而$Y$的分布就是我们模型输出的结果分布，它是未知的。那么机器学习最小化KL​散度就是在最小化交叉熵。

### 分类任务为什么往往使用交叉熵，MSE不行吗?

这里先抛出一个观点：

> 交叉熵是对多项分布的$MLE$, $MSE$是对高斯分布的$MLE$，显然分类是一个多项分布，应该使用交叉熵

我第一次见到这句话的时候，简直犹如醍醐灌顶。不得不说这个解释真的是一针见血。下面我们就来简单证明一下这个观点。

> 1.交叉熵是多项分布的MLE

首先依据我们开篇MLE的定义，我们这里仅以二项(伯努利)分布为例。
$$
\begin{aligned}
p &= \mathop{\operatorname{argmax}}\limits_{p}   f(x|p) \\
&=\mathop{\operatorname{argmax}}\limits_{p}  p^{x}(1-p)^{1-x} \\
&=\mathop{\operatorname{argmax}}\limits_{p} \log p^{x}(1-p)^{1-x} \\
&=\mathop{\operatorname{argmax}}\limits_{p} x\log p + (1-p)\log(1-p) \\
&=\mathop{\operatorname{argmin}}\limits_{p} -(x\log p + (1-p)\log(1-p))
\end{aligned}
$$
证毕。

> 2.MSE是高斯分布的MLE

MSE就是平方损失。假设我们有一组数据${x_1,x_2,\cdots,x_n}$。我们企图用$x_i=\theta(x_i)+\epsilon_i$来拟合这些$x$，我们认为这些误差满足高斯分布$\epsilon\sim N(0,\sigma^2)$。且他们之间是独立同分布。那么对于误差的高斯分布的最大似然，就有
$$
\begin{aligned}
\theta &= \mathop{\operatorname{argmax}}\limits_{\theta} \prod_{i=1}^n f(\epsilon_i|\theta) \\
&= \mathop{\operatorname{argmax}}\limits_{\theta} \prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{\epsilon_i^2}{2\sigma^2}} \\
&= \mathop{\operatorname{argmax}}\limits_{\theta} \log\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{\epsilon_i^2}{2\sigma^2}} \\
&= \mathop{\operatorname{argmax}}\limits_{\theta} \sum_{i=1}^n \log \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{\epsilon_i^2}{2\sigma^2}} \\
&= \mathop{\operatorname{argmax}}\limits_{\theta} \sum_{i=1}^n n\log \frac{1}{\sqrt{2\pi}\sigma} - \sum_{i=1}^n{\frac{\epsilon_i^2}{2\sigma^2}} \\ 
&= \mathop{\operatorname{argmin}}\limits_{\theta} \sum_{i=1}^n \epsilon_i^2 \\ 
&= \mathop{\operatorname{argmin}}\limits_{\theta} \sum_{i=1}^n (x-\theta(x))^2
\end{aligned}
$$
证毕。

那么结论就很显然了。事实上，之于推荐系统，我们常常会有"正样例"和“负样例”，那么实际上这也是一种二分类。所以推荐系统也往往使用交叉熵来作为损失函数。另外按照它的思想，我们模型就会拟合正样例和负样例的数据分布。也正因如此，我们的测试数据往往需要和训练数据保持一致的分布才能达到最好的效果。





