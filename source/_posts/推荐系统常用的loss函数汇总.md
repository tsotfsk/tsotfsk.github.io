---
title: 推荐系统常用的loss函数汇总
date: 2020-08-26 23:41:35
mathjax: true
tags:
- bpr
- logloss
- word2vec
- hingloss
- bce
- pairwise
- pointwise 
categories:
- 推荐系统
---

>虽说推荐系统常用的那几个Loss函数形式十分简单，但是各个loss之间的联系，以及loss中蕴藏的数学原理，我却未曾探究过，私以为如果无法准确理解这些loss，那么也就不能很好地设计属于自己模型的loss函数。本文本着钻研精神，来细细探究推荐系统中常见的那几个loss的来源以及具体的数学原理。同样，由于本人阅读论文数量有限，所以此文可能无法覆盖所有的loss函数，后期也会不断地进行更新。同样，本文可能限于姿势水平，有些问题比较小白，而且一些问题往往也只能达到“知其然，而不知其所以然”的水平，后期也会继续补充。

<!--more-->

在具体讲解前，我有几个十分好奇的问题，先做为引入，感兴趣的大家也可以先思考一下，我也会在本文中随着讲解慢慢解答这几个问题。

- cross entropy？ 信息熵？ 通信原理？
- bce的来源？最大似然？
- loss函数为什么往往是乘法而不是加法？log是不是显得多此一举？
- word2vec的loss来源，和softmax过的bce相关？还和sigmod有关？
- 不同loss对embedding聚类效果的影响？
- 系数对长尾商品的影响？bce引入系数影响popular和unpopular？weight bce？



最大似然



sigmod的前世今生

## 伯努利分布

二元概率模型: 如果随机变量$X$只有两个取值$0,1$, 有$P(X=0)=p, p(X=1)=1-p$, 那么显然$X$关于$p$条件下的概率密度函数为
$$
f(x|p)=p^{x}(1-p)^{1-x}
$$

## 最大似然(MLE)

什么是最大似然方法呢？举个简单的例子，氪金游戏的一大特色就是概率，比如抽卡的，大家是不是经常怀疑卡池里根本没有放入这张卡或者自己几率被暗调了？实际上这里大家就已经用到了最大似然方法。因为大家**根据当前已经发生的事情推测抽到这张卡的概率为0**，这本身就是一个求最大似然的过程。那我们不妨以此为例来引入MLE，一张卡$A$被抽到的概率为$p$, 那么每次抽卡只有两种情况:抽到$A$和没抽到$A$，记一次抽卡的结果为一个随机变量$X$,那么显然，这就是一个伯努利分布，我们容易得到
$$
f(x|p)=p^{x}(1-p)^{1-x}
$$
那么最大似然是在做一件什么事？它希望依据目前的情况来推演$p$的值，使得当前事件发生的概率最大。也就是说假设我们观测到了一个随机过程，比如$x={x_1,x_2,x_3, \cdots,x_n}$，每一个$x_i$对应的都是一个随机变量$X$，他们之间相互独立。对于抽卡来说，就是我们抽了$n$次卡，那么最大似然的意思就是希望刚才发生的情况发生的概率最大。
$$
p = \operatorname{argmax}\prod_{i=1}^n f(x_i|p)
$$
因为$x_i$都是已经发生的，那么只有$p$是变量，之后就是求极值了。那么这个式子为什么这样呢？其实这是一个典型的频率派的观点。这本身蕴含了一种假设，就是**已经发生的事件发生的概率是最大的**这个假设。亦如大家抽卡时的思想"我抽了100抽都没中，卡池里根本没有这张卡！！"。实际上我们的深度学习模型也基本都是频率派，他们假设数据是一个随机变量，而参数是一个常量，通过最大似然类似的思想来寻找这个$p$，也就是模型的参数。

## 二元交叉熵(BCE)

最大似然我们也引入了，接下来讲讲BCE，相信大家接触机器学习中的分类任务，可能首先接触到的就是这么一个损失函数。
$$
l=\frac{1}{N}\sum_{i=1}^Ny\log \widehat y
$$
其思想很自然，但是其中到底又隐藏了什么知识呢？