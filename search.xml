<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>KDD CUP 2020 Debiasing 赛道总结</title>
    <url>/2021/01/11/KDD%20CUP%202020%20Debiasing%20%E8%B5%9B%E9%81%93%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<blockquote>
<p>距离KDD CUP 2020的比赛结束已经有半年了，正巧这学期选了一门复杂网络的课，大作业有一个选题是需要用GNN来做一些事，于是就考虑复盘一下这个比赛过程。由于我之前主要是在队伍中负责图神经网络的部分，当时做的不太理想，所以这次复盘也主要是对之前的工作进行反思以及改进。</p>
</blockquote>
<a id="more"></a>
<h2 id="1-题目描述"><a href="#1-题目描述" class="headerlink" title="1.题目描述"></a>1.题目描述</h2><p>此赛道关注于如何在推荐时平衡热门商品以及冷门商品，以对抗推荐系统中经常遇到的马太效应。</p>
<p><strong>使用数据</strong></p>
<p>数据采样自手机淘宝某促销阶段连续两周的点击数据。量级大概是3万用户，11万商品，然后有100万条点击记录。数据文件信息如下</p>
<p><strong>1)特征文件</strong></p>
<ul>
<li>商品信息: <item_id>, <text_embedding>, <image_embedding></image_embedding></text_embedding></item_id></li>
<li>用户信息: <user_id>, <user_age_level>, <user_gender>, <user_city_level></user_city_level></user_gender></user_age_level></user_id></li>
</ul>
<p><strong>2)训练文件</strong></p>
<ul>
<li>交互信息: <user_id>, <item_id>, <time></time></item_id></user_id></li>
</ul>
<p><strong>3)测试文件</strong></p>
<ul>
<li><p>交互信息: <user_id>, <item_id>, <time></time></item_id></user_id></p>
</li>
<li><p>预测信息: <user_id>,  <q_time></q_time></user_id></p>
</li>
</ul>
<p><strong>评测方式</strong></p>
<p>已知用户某$k-1$个时间点的交互数据，预测在第$k$个时间点用户会点击的商品。具体的，对每个用户召回$50$个商品，以最不热门的$50\%$商品的NDCG来衡量我们方法的好坏。</p>
<h2 id="3-赛题解析"><a href="#3-赛题解析" class="headerlink" title="3. 赛题解析"></a>3. 赛题解析</h2><h3 id="1-任务定义"><a href="#1-任务定义" class="headerlink" title="1) 任务定义"></a>1) 任务定义</h3><p>首先这可以看作是一个序列推荐的任务，因为是给定一个q_time，预测用户会在这个q_time点击什么，但这并不是一个传统意义上的next item的而推荐，因为后续分析数据后发现，q_time对应的商品并不是用户曝光数据中的最后一个商品。同时我们发现用户特征信息缺失严重，所以仅使用了商品的多模态信息，以及用户的交互记录信息。</p>
<h3 id="2-什么是推荐系统的bias？"><a href="#2-什么是推荐系统的bias？" class="headerlink" title="2) 什么是推荐系统的bias？"></a>2) 什么是推荐系统的bias？</h3><p>题目中所描述的偏差，主要是两类:</p>
<p><strong>选择性偏差</strong>：如果对用户历史进行建模，那收集的商品仅是所有商品的一个子集，模型之后则总会引导用户去购买出现过的商品。因为未曾被购买过的的商品对模型是未知的。(冷启动问题)</p>
<p><strong>流行度偏差</strong>：商品的点击呈长尾分布，热门商品在正样本中频繁出现，其也就更容易被推荐，进而加剧马太效应。</p>
<h3 id="3-如何缓解bias？"><a href="#3-如何缓解bias？" class="headerlink" title="3) 如何缓解bias？"></a>3) 如何缓解bias？</h3><p>1) <strong>充分利用文本和图片特征</strong>。首先注意到题目中给出了商品的文本和图片表达，应当注意到这两个属性理论上是无偏的，即两者均无法显式的反应商品的流行程度。</p>
<p>2) <strong>对于热门商品使用一定的权重惩罚</strong>。即在推荐时，当按照分数高低排序时，引入一个权重因子来削弱热门商品的分数。</p>
<h2 id="3-基于图的解决方案"><a href="#3-基于图的解决方案" class="headerlink" title="3. 基于图的解决方案"></a>3. 基于图的解决方案</h2><p>大致流程如下:</p>
<p><strong>1)</strong> 基于规则与共现得到第一个Item2Item的相似度矩阵。</p>
<p><strong>2)</strong> 基于text embedding 和image embedding 得到 第二个Item2Item的相似度矩阵。</p>
<p><strong>3)</strong> 依据两个相似度矩阵构图，取相似度top 50连边。图为有向有权图。</p>
<p><strong>4)</strong> 在图上采用不同方法进行随机游走得到序列，然后利用Word2Vec得到商品表达。</p>
<p><strong>5)</strong> 基于商品表达重构相似度矩阵，然后使用规则来为用户推荐商品。</p>
<p>后续会慢慢解释上述各种做法的原因。</p>
<h4 id="1-为何构图？"><a href="#1-为何构图？" class="headerlink" title="1) 为何构图？"></a>1) 为何构图？</h4><p>为什么图表示学习的方法可能是有用的呢？这点我们受到了baseline以及赵老师的启发。在比赛初期，赛题讨论区放出了一个ItemCF的方法，我们试验后发现其效果非常的好，然后队友又基于规则进行了改进，这些改进使得我们甚至在比赛初期拿到了rank1的好结果，那ItemCF的方法的思想是什么呢？很简单，其核心就是构造一个商品之间的相似度矩阵。原始的ItemCF是基于共现来计算的这个矩阵，而我们改进后的ItemCF则综合考虑了商品曝光数量，用户点击数量，商品间点击间隔，点击时间先后等多种特征来计算。</p>
<p><img src="/2021/01/11/KDD%20CUP%202020%20Debiasing%20%E8%B5%9B%E9%81%93%E6%80%BB%E7%BB%93/KDD CUP 2020 Debiasing 赛道总结\image-20210111200538815.png" alt="image-20210111200538815"></p>
<p>那么很显然，根据我们的改进过程，可以发现，这个<strong>相似度矩阵的质量，极大的影响了我们的推荐结果</strong>。实际上，我们后续的所有实验几乎都是在<strong>优化这个相似度矩阵</strong>。注意到，相似度矩阵本身是可以转化为图的，其中相似度可以作为边权，商品则作为了节点。那么应用图表示学习的方法来重构这个相似度矩阵的观点便油然而生了。</p>
<h4 id="2-如何构图？"><a href="#2-如何构图？" class="headerlink" title="2). 如何构图？"></a>2). 如何构图？</h4><ul>
<li><strong>多模态图(有向有权图)</strong>： 取图片向量0.95+文本向量0.05，取每个商品top50相似的连边。构图后共有108916个节点，5445800条边</li>
<li><strong>序列图(有向有权图)</strong>：取每个用户序列内商品的全连接子图的方式进行构图，权重来自改进后的ItemCF的物品相似度矩阵，取每个商品的top50相似商品连边。构图后共有117720个节点，5876842条边。</li>
</ul>
<p>为什么图片和文本要合在一起，然后取这样的比例？因为实际使用中发现文本向量质量不佳，图片向量的效果远好于文本向量，但稍微的引入一些文本向量，效果会提升那么一丢丢。</p>
<p>为何取top 50？实际上我们取过top 100 以及 top 500，发现大家的结果都差距不大，但是随着top k的k值的提升，学习代价是急剧增大的。碍于当时的硬件资源以及时间资源，我们最后取了50这个值。</p>
<p>为何图是有向的？实验中我们发现有向比无向要好，当然也可以用一个简单的例子来解释是：正向的话，买电脑之后更可能买一个鼠标。而逆向的话，买鼠标之后似乎不太可能买一台电脑。那么取Top 50后，鼠标是电脑的邻居，但是电脑可能就不是鼠标的邻居了。</p>
<h4 id="3-图表示学习方法"><a href="#3-图表示学习方法" class="headerlink" title="3). 图表示学习方法"></a>3). 图表示学习方法</h4><ul>
<li><strong>M | S</strong>：借鉴Node2Vec的概率转移方法在多模态图或者序列图上上游走生成路径，并学习表达。这是最自然的一种方法</li>
</ul>
<p><img src="/2021/01/11/KDD%20CUP%202020%20Debiasing%20%E8%B5%9B%E9%81%93%E6%80%BB%E7%BB%93/KDD CUP 2020 Debiasing 赛道总结\image-20210111184528736.png" alt="image-20210111184528736"></p>
<ul>
<li><strong>M+S</strong>：在M随机游走的路径与S随机游走的路径进行合并之后，再学习表达。之所以不采取图结构的直接合并再随机游走是因为二者相似度权重分布差异较大，无法直接进行概率比较而选择跳转。</li>
</ul>
<p><img src="/2021/01/11/KDD%20CUP%202020%20Debiasing%20%E8%B5%9B%E9%81%93%E6%80%BB%E7%BB%93/KDD CUP 2020 Debiasing 赛道总结\image-20210111184553759.png" alt="image-20210111184553759"></p>
<ul>
<li><strong>M&amp;S</strong>：采用在两张图之间来回跳转的方式进行游走，比如S-M-S-M-S-M-S-M，概率转移方法同Node2Vec。注意到，这样就可以缓解权重分布差异大的问题，从而学习到一个S和M的联合分布。同时，由于两张图上的节点并不是子集关系，那么如果游走到某一节点后，没有与之对应的另外一模态的邻边，那么当前序列就会终止。</li>
</ul>
<p><img src="/2021/01/11/KDD%20CUP%202020%20Debiasing%20%E8%B5%9B%E9%81%93%E6%80%BB%E7%BB%93/KDD CUP 2020 Debiasing 赛道总结\image-20210111184608944.png" alt="image-20210111184608944"></p>
<h2 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4. 实验结果"></a>4. 实验结果</h2><p>我们使用RNDCG@50<sup><a href="#myfootnote2">2</a></sup>来衡量我们推荐列表的效果</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Hit@50</th>
<th>NDCG@50</th>
<th>RHit@50</th>
<th>RNDCG@50</th>
<th>Improve</th>
</tr>
</thead>
<tbody>
<tr>
<td>ItemCF</td>
<td>0.1333</td>
<td>0.0538</td>
<td>0.0988</td>
<td>0.0391</td>
<td>-</td>
</tr>
<tr>
<td>ItemCF + M</td>
<td>0.1085</td>
<td>0.0397</td>
<td>0.1141</td>
<td>0.0419</td>
<td>7.1%</td>
</tr>
<tr>
<td>ItemCF + S</td>
<td>0.1696</td>
<td>0.0684</td>
<td>0.1346</td>
<td>0.0535</td>
<td>36.8%</td>
</tr>
<tr>
<td>ItemCF + M+S</td>
<td>0.1504</td>
<td>0.0615</td>
<td>0.1502</td>
<td>0.0595</td>
<td>52.3%</td>
</tr>
<tr>
<td>ItemCF + M&amp;S</td>
<td>0.1816</td>
<td>0.0705</td>
<td>0.1722</td>
<td>0.0646</td>
<td>65.2%</td>
</tr>
<tr>
<td>Blend MS results</td>
<td>0.2172</td>
<td>0.0903</td>
<td>0.1908</td>
<td>0.0773</td>
<td>97.9%</td>
</tr>
</tbody>
</table>
</div>
<p><strong>NOTE</strong>: Blend MS results指的是融合 M，S与M&amp;S的方法。注意到这这种融合还能有不俗的提升，说明融合的游走方式可以学到一些单纯的M和S学不到的信息。</p>
<h3 id="5-改进策略"><a href="#5-改进策略" class="headerlink" title="5. 改进策略"></a>5. 改进策略</h3><ul>
<li><p>ItemCF主要有两个流程。一是构造相似度矩阵，二是利用相似度矩阵来进行推荐。而我们只改进了第一个阶段，实际上我们可以利用深度学习的方法，比如YotubeDNN来将图方法得到的商品表达用于召回。</p>
</li>
<li><p>多路召回。实际上，我们提到的算法大都是在ItemCF的框架下进行的，应当尝试一些不同的比如UserCF，Swing，甚至最基础的基于embedding的召回方式。这种多路一方面可以体现在相似度矩阵的构造上，一方面也可以直接作用在最后的召回分数上。</p>
</li>
<li>划分粗排和精排两个阶段。实际上，简单的规则往往用于粗排，我们可以选择一个适当的召回集大小，然后再构造一些特征，利用一些深度学习的方法或者树方法进行精排。</li>
<li>优化元路径种类。S-M-S-M可能并不是最优的组合方式。</li>
</ul>
<h3 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h3><p>由于学期时间比较紧张，所以复盘中的各个结果，都是一套参数跑下来的，并没有进行过多的调参工作，那么实际的最优结果，肯定是都是要高于上表的。</p>
<p>1) <strong>切勿迷信论文</strong>。犹记得最初参加比赛时，自己还是一个懵懂的小白，那时候的自己迷信论文，总认为论文中的方法肯定是好的，不然怎么会发出来，然后就在github一顿搜索，然后代入数据取跑，发现结果都很一般。事实上，论文中的结果往往是在作者构造了一个精致的数据集的前提下产生的，他们可能本身就带有严重的bias，甚至在冷启动下根本无法运行！而且一种算法往往也很难fit到所有的数据集上，我在实验中甚至有的算法甚至几乎不推荐冷门商品！所以尽信书不如无书呀！实验是检验道理的唯一标准！</p>
<p>2)<strong>知其然还要知其所以然</strong>。有时候结果提升了，也还需要问自己一下为什么？这个在实验中多次被赵老师和王老师问到，但我一直没去思考，只是觉得”结果提升了，我哪知道为什么，这难道不是试着来的吗”。其实，很多时候，正是忽略了为什么，才导致下一步的提升很艰难。这里面自己走过了许多的弯路，比如LastItem的问题，每一阶段候选集的问题等等。有时候想破脑袋也没新的提升办法的时候，不如回过头来思考一下，前面的那么多的改进方案为什么work或者为什么不work，往往会柳暗花明又一村。</p>
<p>3) <strong>天涯何处无芳草，何必单恋一枝花</strong>。有时候一种方法不起作用，没必要死死纠缠，即使是自己代码写错了。我在当时的比赛中一直沉迷于PinSage以及GraphSage。但怎么测试都没能取到可行的结果，甚至结果非常的糟糕，一度怀疑人生，觉得自己菜的不行，耽误了很多时间。而且当时自己也一直相信GraphSage一定会比Node2Vec好。实际不然，现在回过头来思考Node2Vec以及GraphSage的原理，似乎在LinkPrediction这个问题上，真的难分伯仲。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] <a href="https://tianchi.aliyun.com/competition/entrance/231785/information" target="_blank" rel="noopener">KDD Cup 2020 Challenges for Modern E-Commerce Platform: Debiasing</a></p>
<p>[2] Aditya Grover and Jure Leskovec.node2vec: Scalable Feature Learning for Networks.</p>
<p>[3] <a href="https://tech.meituan.com/2020/08/20/kdd-cup-debiasing-practice.html" target="_blank" rel="noopener">KDD Cup 2020 Debiasing比赛冠军技术方案及在美团的实践</a></p>
<p>[4] <a href="https://tianchi.aliyun.com/forum/postDetail?spm=5176.12586969.1002.15.6c3f1f0bypIv8x&amp;postId=103530" target="_blank" rel="noopener">A simple itemCF Baseline, score:0.1169(phase0-2)</a></p>
<p><a name="myfootnote2">1</a>: RNDCG@50 只计算测试集中曝光量最少的那一半商品的的NDCG</p>
]]></content>
      <tags>
        <tag>KDD CUP</tag>
        <tag>比赛</tag>
        <tag>开源代码</tag>
        <tag>感悟</tag>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习01——马尔可夫过程</title>
    <url>/2020/11/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A001%E2%80%94%E2%80%94%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>马尔可夫过程</tag>
        <tag>马尔可夫奖励过程</tag>
        <tag>马尔可夫决策过程</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅ML20笔记</title>
    <url>/2020/10/08/ML%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<blockquote>
<p>之所以开这个目录并决定连载，是因为作为一个博客型选手，还没有完整的看过任何一个机器学习视频，书籍诸如西瓜书、花树什么的也没看过，所以底层建筑就非常的薄弱，很多东西都处在知其然而不知其所以然的地步。</p>
<p>虽说机器学习发展日新月异，大家都在追赶潮流，但我觉得基础知识的沉淀还是十分有必要的，所以开个坑来补补基础知识。不过本系列仅作为本人查漏补缺用，所以仅会记录一些我不太懂的东西，或者一些我认为解释的非常妙的东西。</p>
</blockquote>
<a id="more"></a>
<h3 id="1-机器学习的bias和variance"><a href="#1-机器学习的bias和variance" class="headerlink" title="1. 机器学习的bias和variance"></a>1. 机器学习的bias和variance</h3><p><img src="/2020/10/08/ML%E7%AC%94%E8%AE%B0/image-20201008172829594.png" alt="image-20201008172829594" style="zoom:33%;"></p>
<p>靶心是target,bias是模型预测的均值与靶心的位置，方差是预测位置周围点的散步情况</p>
<ul>
<li>过拟合是variance太大，比如高阶多项式，其往往容易过拟合。那么采取正则化是一种方案，<strong>扩大数据集(增加采样点)</strong>也是一种方案。</li>
<li>欠拟合是bias太大，比如常数多项式，variance是0，但是bias会很大，很多数据都不在点上。这种就是欠拟合。</li>
</ul>
<h3 id="2-adagrad的分子分母似乎是矛盾的"><a href="#2-adagrad的分子分母似乎是矛盾的" class="headerlink" title="2. adagrad的分子分母似乎是矛盾的?"></a>2. adagrad的分子分母似乎是矛盾的?</h3><p><img src="/2020/10/08/ML%E7%AC%94%E8%AE%B0/image-20201008174734502.png" alt="image-20201008174734502" style="zoom:40%;"></p>
<p>梯度越大的变量下降的越快？直觉是这样，但是并不代表离最低点的距离是一样的，因为$\mu$是一致的。实际上$W$矩阵如果有两个参数$w_1,w_2$，其距离最低点都是一个二次曲线的loss，那么他们相同位置可能斜率不一致，斜率大的参数并不一定离原点就远，如何衡量远近需要除以二次导，adagrad分母的求和某种意义上可以理解为sample和，那么就可以模拟二次导，比如绿色sample的都很大，说明二次导比较大。</p>
<h3 id="3-数据为什么要归一化？"><a href="#3-数据为什么要归一化？" class="headerlink" title="3.数据为什么要归一化？"></a>3.数据为什么要归一化？</h3><p><img src="/2020/10/08/ML%E7%AC%94%E8%AE%B0/image-20201008175803601.png" alt="image-20201008175803601" style="zoom:50%;"></p>
<p>圈圈都是俯视图，固定任何一个坐标做切面就可以理解了，左侧图受切入点影响，梯度不指向中心，会减速收敛，右侧任何位置梯度都指向圆心。</p>
<h3 id="4-GD如何work的"><a href="#4-GD如何work的" class="headerlink" title="4.GD如何work的?"></a>4.GD如何work的?</h3><p><img src="/2020/10/08/ML%E7%AC%94%E8%AE%B0/image-20201008181150478.png" alt="image-20201008181150478" style="zoom:50%;"></p>
<p>核心就是泰勒展开做一阶近似，在loss中选择一个点的邻域，固定半径r，如何找到loss最低点？找的最低点就是梯度反向。相当于点乘，$cos \theta=-1$，loss最小。</p>
]]></content>
      <categories>
        <category>ML笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>李宏毅</tag>
      </tags>
  </entry>
  <entry>
    <title>word2vec的NCELoss详解</title>
    <url>/2020/08/31/word2vec%E7%9A%84NCELoss%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<blockquote>
<p>word2vec设计的nce(noise-contrastive estimation)损失函数，其实在推荐系统以及图神经网络等领域也是十分常见的，那么为什么这么设计呢？它又有什么思想可以追溯呢？本文就谈谈我的见解</p>
</blockquote>
<h2 id="sigmod函数"><a href="#sigmod函数" class="headerlink" title="sigmod函数"></a>sigmod函数</h2><p>sigmod可以说是无人不知，无人不晓，那么我现在很好奇的问大家一句？sigmod为什么长这样？机器学习为什么使用sigmod？如何推导sigmod？嗯，这些都是我不知道的东西，所以我决定首先引入sigmod。</p>
<script type="math/tex; mode=display">
\sigma=\frac{1}{1+e^{-x}}</script>]]></content>
  </entry>
  <entry>
    <title>机器学习交叉熵loss的追根溯源</title>
    <url>/2020/08/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%A4%E5%8F%89%E7%86%B5loss%E7%9A%84%E8%BF%BD%E6%A0%B9%E6%BA%AF%E6%BA%90/</url>
    <content><![CDATA[<blockquote>
<p>虽说交叉熵也算是机器学习常用的损失函数了，其想达到的目标很好理解，但是其中蕴藏的数学原理，我却一直似懂非懂，从未曾深入探究过。本文就本着钻研精神，来细细探究​交叉熵损失函数的来源以及具体的数学原理。当然，本文可能限于姿势水平，有些问题比较小白，当然我个人也希望讲解的更基础一点，而且还有一些问题往往也只能达到“知其然，而不知其所以然”的水平，如有错误或者疏漏，也欢迎各位大佬斧正。</p>
</blockquote>
<a id="more"></a>
<p>在具体讲解前，我引出几个小问题，大家感兴趣的话也可以先思考一下，我也会在本文中随着讲解慢慢解答这几个问题。</p>
<ul>
<li>什么是信息熵？什么是交叉熵？什么是相对熵(KL​散度)？</li>
<li>如何从编码bit​数的角度理解上述概念?</li>
<li>交叉熵被作为机器学习损失函数的必然性？</li>
<li>分类任务为什么使用交叉熵，MSE​不行吗?</li>
</ul>
<h2 id="伯努利分布"><a href="#伯努利分布" class="headerlink" title="伯努利分布"></a>伯努利分布</h2><p>二元概率模型: 如果随机变量$X$只有两个取值$0,1$, 有$P(X=0)=p, p(X=1)=1-p$, 那么$X$关于$p$条件下的概率密度函数为</p>
<script type="math/tex; mode=display">
f(x|p)=p^{x}(1-p)^{1-x}</script><p>注意到这只是一个二元的, 那$x$不就两种情况嘛, 大家代入一下是很显然的~</p>
<h2 id="最大似然-MLE"><a href="#最大似然-MLE" class="headerlink" title="最大似然(MLE)"></a>最大似然(MLE)</h2><p>什么是最大似然方法呢？举个简单的例子，氪金游戏的一大特色就是概率，比如抽卡的，大家是不是经常怀疑卡池里根本没有放入这张卡或者自己几率被暗调了？实际上这里大家就已经用到了最大似然方法。因为大家<strong>根据当前已经发生的事情推测抽到这张卡的概率为0</strong>，这本身就是一个求最大似然的过程。那我们不妨以此为例来引入$MLE$，一张卡$A$被抽到的概率为$p$, 那么每次抽卡只有两种情况:抽到$A$和没抽到$A$，记一次抽卡的结果为一个随机变量$X$,那么显然，这就是一个伯努利分布，我们容易得到</p>
<script type="math/tex; mode=display">
f(x|p)=p^{x}(1-p)^{1-x}</script><p>那么最大似然是在做一件什么事？它希望依据目前的情况来推演$p$的值，使得当前事件发生的概率最大。也就是说假设我们观测到了一个随机过程，比如$x={x_1,x_2,x_3, \cdots,x_n}$，每一个$x_i$对应的都是一个随机变量$X$，他们之间相互独立。对于抽卡来说，就是我们抽了$n$次卡，那么最大似然的公式就如下:</p>
<script type="math/tex; mode=display">
p = \mathop{\operatorname{argmax}}\limits_{p} \prod_{i=1}^n f(x_i|p)</script><p>因为$x_i$都是已经发生的，那么只有$p$是变量，之后就是求极值了。那么这个式子为什么这样呢？其实这本身蕴含了一种假设，就是<strong>已经发生的事件发生的概率是最大</strong>。比如大家抽卡时的思想”我抽了100抽都没中，卡池里根本没有这张卡！！”,  那么此时我们就是依据最大似然来推断了$p$的值，也就是$p$就是0。具体的用公式来求解的话，对于100次抽卡就有</p>
<script type="math/tex; mode=display">
\begin{aligned}
p &= \mathop{\operatorname{argmax}}\limits_{p} \prod_{i=1}^{100} (1-p) \\
&=\mathop{\operatorname{argmax}}\limits_{p} (1-p)^{100} \\
&=0
\end{aligned}</script><h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><p>接下来讲讲交叉熵(CE)，相信大家接触机器学习中的分类任务，可能首先接触到的就是这么一个损失函数。</p>
<script type="math/tex; mode=display">
l=-\frac{1}{N}\sum_{i=1}^Ny\ln \widehat y</script><p>其思想很自然，但是其中蕴藏了什么知识呢？如果稍微追根溯源一下的话，我们就会接触到<code>信息熵</code>这个概念(如果你和我一样是一个本科学过通信原理的计算机人，希望你能回想起被通信原理支配的恐惧)，那么信息熵(以$2$为底时)的表示如下:</p>
<script type="math/tex; mode=display">
H(x)=-\sum_{i=1}^Np(x)\log_2 {p(x)}</script><p>信息熵衡量的是信息量的期望，或者说叫信道的不确定性，单位是bit。最简单的，比如发送方在信道只发送$0,1$字符，如果发送$1$字符的概率为$1$, 发送$0$字符的概率为$0$，那么传递的信息量依据上式计算就是$0$(注意$\lim\limits_{x\to0^+}  xlnx=0$)。这似乎是显然的，因为如果信道只发送字符$1$,那么接受者似乎接收不到任何有价值的信息, 因为我知道你一定会发$1$，这里面就没有任何不确定性。那么什么时候不确定性最大呢？显然是$P(X=1)=P(X=0)=\frac{1}{2}$的时候，因为这时候$0,1$等概率，我猜都不好猜，不确定性最大。数学证明也很简单，拉格朗日乘数法秒解。不过我这里并不想从这个角度来引入信息熵，实际上对于学过计网或者通原的同学来说，大家肯定对信道编码不陌生。</p>
<p>首先我引入一个问题:</p>
<blockquote>
<p>对于一个字符集${s_1,s_2,s_3, \cdots, s_n}$，每个符号被发送的概率依次为${p(s_1),p(s_2),p(s_3),\cdots, p(s_n)}$，那么字符的最小编码的平均bit数是多少呢？</p>
</blockquote>
<p>实际上这个平均bit数就是信息熵，但是我这里不给出证明，因为想要对其严格证明实在过于繁琐，我尝试写了一页证明，也没写完。浪费了一下午的时间QAQ。另外如果给出证明，那这篇博客放眼望去全是公式，肯定没人喜欢看…而我也更多的是想和大家分享一种我自己或者我看到的见解，而不是班门弄斧的敲数学公式，毕竟数学很容易出错…有兴趣的同学可以尝试证明之，提示:数列均值极限定理，斯特林数，大数定理。</p>
<p>这里我仅引入一个特例来说明他确实是最小编码的平均bit数。大家应该知道哈夫曼编码，实际上哈夫曼编码就是一种最优的编码方式。它的长度是最短的。那么实际上哈夫曼编码如果对于这么一个字符集比如${s_1,s_2,s_3,s_4}$，他们的概率又都是${\frac{1}{2},\frac{1}{4},\frac{1}{8},\frac{1}{8}}$来说，其平均查找长度就可以写成信息熵的形式。</p>
<p><img src="/2020/08/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%A4%E5%8F%89%E7%86%B5loss%E7%9A%84%E8%BF%BD%E6%A0%B9%E6%BA%AF%E6%BA%90/image-20200829201110798.png" alt="image-20200829201110798" style="zoom:60%;"></p>
<p>显然，平均编码(查找)长度就是</p>
<script type="math/tex; mode=display">
\begin{aligned}
L &= \frac{1}{2} \times 1 + \frac{1}{4} \times 2+\frac{1}{8} \times 3 + \frac{1}{8} \times 3 \\

 &= \frac{1}{2} \times \log_22 + \frac{1}{4} \times \log_24 +\frac{1}{8} \times \log_28 + \frac{1}{8} \times \log_28 \\
 &= -\sum_{i=1}^{n}p(s_i)\log_2 p(s_i)
\end{aligned}</script><p>也就是说信息熵是可以表达最短平均编码长度，其中$-\log p(x)$可以反映字符$x$的信息编码长度。实际上在我刚才举的关于信道的例子中，我们默认信道是不会出错的，也就是不存在干扰。但是真实的信道显然不会如此，不然也不会做一些差错编码以及增加校验帧了。那么假设我们信道依然发送$0,1$字符，但是信道可能出错，那么就会有如下的情况发生。</p>
<p><img src="/2020/08/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%A4%E5%8F%89%E7%86%B5loss%E7%9A%84%E8%BF%BD%E6%A0%B9%E6%BA%AF%E6%BA%90/image-20200829124157734.png" alt="image-20200829124157734" style="zoom:60%;"></p>
<p>假设左侧作为发送方，信道存在差错，其发送的信息满足于一个伯努利分布$X \sim B(p)$，对于接收方，其收到的信息满足一个伯努利分布$Y \sim B(q)$。那么现在问题来了，现在对于接收方来说，它得到的信息熵改变了吗？变多了还是变少了。我们将此时由这两个分布共同产生的信息熵定义为交叉熵。记为$H(X,Y)$，那么其公式如下</p>
<script type="math/tex; mode=display">
H(X,Y)=-\sum_{i = 1}^{|S|}p(s_i)\log q(s_i)</script><p>为什么长这样呢？实际上作为信息的接收方，我们如果拿得到的分布来做编码，那么此时的平均编码长度就是上式子~显然如果$Y$与$X$的分布不相等，那么这个平均编码的长度是要大于$X$平均编码的长度的。因为我们知道$X$的信息熵对应的编码是最优编码，你换了一套编码自然会比这个大，这也符合我们的直觉，那就是引入噪声使得信息熵增大了，其加大了不确定度，也增大了平均编码长度。也就是说有</p>
<script type="math/tex; mode=display">
H(X,Y)-H(X)>0</script><p>严格的数学证明参考<strong>Gibbs inequality</strong>。那么这个差值又是什么呢？实际上这个差值叫$KL$散度，也叫相对熵。相对熵描述的更像是一个信息差。那么相对熵就有如下等式</p>
<script type="math/tex; mode=display">
H(X|Y) = H(X,Y)-H(X)</script><p>其衡量的也是两个分布之间的差异，这个差异是非对称的。如下有个图比较直观</p>
<p><img src="/2020/08/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%A4%E5%8F%89%E7%86%B5loss%E7%9A%84%E8%BF%BD%E6%A0%B9%E6%BA%AF%E6%BA%90/图片1-1598712911224.png" alt="图片1" style="zoom:50%;"></p>
<p>从文氏图也能比较清楚的看到，相对熵更像是<strong>一个分布比另外一个分布多出来的信息量</strong>。所以严格来讲，$KL$散度描述的并不是两个分布的距离，而更像是两个分布的相对差异。</p>
<p>从平均编码长度的角度来说。<strong>信息熵描述了最优编码的长度。交叉熵描述了非最优编码的平均长度。而相对熵则描述的是非最优编码比最优编码多出来的长度。</strong></p>
<h3 id="交叉熵被作为机器学习损失函数的必然性？"><a href="#交叉熵被作为机器学习损失函数的必然性？" class="headerlink" title="交叉熵被作为机器学习损失函数的必然性？"></a>交叉熵被作为机器学习损失函数的必然性？</h3><p>首先，这里有一个误区。我们机器学习真的使用的是交叉熵吗？不是。实际上我们真正使用的是$KL$散度。为什么呢？因为$KL$散度描述的就是两个分部之间的差异。那么对于机器学习，其描述的就是<strong>输出的数据分布的与模型输入的分布的差异</strong>，那么将其作为损失函数的思想是很自然的，因为我们希望模型能够捕捉到输入数据的分布，来达到输出的分布与输入的保持一致。但是，为什么我们用到的都是交叉熵呢？因为KL​散度和交叉熵的最小化在机器学习中是等价的。注意到交叉熵的公式</p>
<script type="math/tex; mode=display">
H(X,Y) = H(X|Y)+H(X)</script><p>在机器学习中，$X$就是数据，数据的分布是一个先验分布，换句话说，它是已知的。那么$H(X)$就是一个定值。而$Y$的分布就是我们模型输出的结果分布，它是未知的。那么机器学习最小化KL​散度就是在最小化交叉熵。</p>
<h3 id="分类任务为什么使用交叉熵，MSE不行吗"><a href="#分类任务为什么使用交叉熵，MSE不行吗" class="headerlink" title="分类任务为什么使用交叉熵，MSE不行吗?"></a>分类任务为什么使用交叉熵，MSE不行吗?</h3><p>这里先抛出一个观点：</p>
<blockquote>
<p>交叉熵是对多项分布的$MLE$, $MSE$是对高斯分布的$MLE$，显然分类是一个多项分布，应该使用交叉熵</p>
</blockquote>
<p>我第一次见到这句话的时候，简直犹如醍醐灌顶。不得不说这个解释真的是一针见血。下面我们就来简单证明一下这个观点。</p>
<blockquote>
<p>1.交叉熵是多项分布的MLE</p>
</blockquote>
<p>首先依据我们开篇MLE的定义，我们这里仅以二项分布为例，加入我们有一组数据${x_1,x_2,\cdots,x_n}$，每个数据都服从二项分布$x_i\sim B(n,p)$。并且这些数据是独立同分布。那么有:</p>
<script type="math/tex; mode=display">
\begin{aligned}
p &= \mathop{\operatorname{argmax}}\limits_{p}\prod_{i=1}^n f(x_i|p) \\
&=\mathop{\operatorname{argmax}}\limits_{p}  \prod_{i=1}^n p^{x_i}(1-p)^{1-x_i} \\
&=\mathop{\operatorname{argmax}}\limits_{p} \log \prod_{i=1}^n p^{x_i}(1-p)^{1-x_i} \\
&=\mathop{\operatorname{argmax}}\limits_{p} \sum_{i=1}^n x_i\log p + (1-x_i)\log(1-p) \\
&=\mathop{\operatorname{argmin}}\limits_{p} \sum_{i=1}^n -(x_i\log p + (1-x_i)\log(1-p))
\end{aligned}</script><p>证毕。</p>
<blockquote>
<p>2.MSE是高斯分布的MLE</p>
</blockquote>
<p>MSE就是平方损失。假设我们有一组数据${x_1,x_2,\cdots,x_n}$。我们企图用$x_i=\theta(x_i)+\epsilon_i$来拟合这些$x$，我们认为这些误差满足高斯分布$\epsilon_i \sim N(0,\sigma^2)$。并且这些误差是独立同分布。那么对于误差的高斯分布的最大似然，就有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta &= \mathop{\operatorname{argmax}}\limits_{\theta} \prod_{i=1}^n f(\epsilon_i|\theta) \\
&= \mathop{\operatorname{argmax}}\limits_{\theta} \prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{\epsilon_i^2}{2\sigma^2}} \\
&= \mathop{\operatorname{argmax}}\limits_{\theta} \log\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{\epsilon_i^2}{2\sigma^2}} \\
&= \mathop{\operatorname{argmax}}\limits_{\theta} \sum_{i=1}^n \log \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{\epsilon_i^2}{2\sigma^2}} \\
&= \mathop{\operatorname{argmax}}\limits_{\theta} \sum_{i=1}^n n\log \frac{1}{\sqrt{2\pi}\sigma} - \sum_{i=1}^n{\frac{\epsilon_i^2}{2\sigma^2}} \\ 
&= \mathop{\operatorname{argmin}}\limits_{\theta} \sum_{i=1}^n \epsilon_i^2 \\ 
&= \mathop{\operatorname{argmin}}\limits_{\theta} \sum_{i=1}^n (x-\theta(x))^2
\end{aligned}</script><p>证毕。</p>
<p>那么结论就很显然了。另外，我们也可以更显然的知道MSE一般用于回归的任务。事实上，之于推荐系统，我们也会有”正样例”和“负样例”，那么这也是一种二分类。所以推荐系统也往往也会使用交叉熵来作为损失函数，也叫logloss​。另外按照它的思想，我们模型就会拟合正样例和负样例的数据分布。也正因如此，我们的测试数据往往需要和训练数据保持一致的分布才能达到最好的效果。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>交叉熵</tag>
        <tag>最大似然</tag>
        <tag>KL散度</tag>
        <tag>相对熵</tag>
      </tags>
  </entry>
  <entry>
    <title>一些神奇的python库</title>
    <url>/2020/08/10/%E4%B8%80%E4%BA%9B%E7%A5%9E%E5%A5%87%E7%9A%84python%E5%BA%93/</url>
    <content><![CDATA[<blockquote>
<p>主要是记录一些自己觉得科研或者项目可能用到的，但是又似乎不是那么大众的库，会随时更新。这里每一个库都会举一个小例子。因为记性不好，记不住api，所以也算是之后要用的话，一个简单的小笔记吧。</p>
</blockquote>
<a id="more"></a>
<h2 id="sympy-符号计算，可以输出为latex公式"><a href="#sympy-符号计算，可以输出为latex公式" class="headerlink" title="sympy 符号计算，可以输出为latex公式"></a>sympy 符号计算，可以输出为latex公式</h2><blockquote>
<p>sympy 可以做一些符号计算，如求解线性方程，求导，求积分等。更方便的是他提供公式的简化，以及公式的latex输出，而且也可以用作简单的制图。如果论文中有繁琐的公式求导的部分，可以直接用这玩意来写…然后扔进latex里。当然，如果你是一个深度学习新手，面对复杂的公式，也可以通过这种方式来验证自己的高数水平</p>
</blockquote>
<ul>
<li><p>toy example</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">from</span> sympy <span class="keyword">import</span> *</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&gt;&gt;&gt;x, y, z = symbols(<span class="string">"x y z"</span>)</span><br><span class="line">&gt;&gt;&gt;expr = x**<span class="number">2</span> + y**<span class="number">2</span> + z**<span class="number">2</span></span><br><span class="line">&gt;&gt;&gt;result = diff(expr, x).subs(x,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;result</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">2</span>*x</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>API<ul>
<li><code>print_latex</code>:输出为latex公式</li>
<li><code>simplify</code>:化简公式</li>
</ul>
</li>
</ul>
<h2 id="line-profiler"><a href="#line-profiler" class="headerlink" title="line_profiler"></a>line_profiler</h2><blockquote>
<p>line_profiler 可以逐行查看运行时间。当你的代码出现性能瓶颈的时候，你就可以清晰的定位到，具体是哪个函数的哪一行慢，以便更好地做优化。当然它有一些小缺点，就是需要引入函数装饰器，但是不需要import这个库，所以其无法通过直接运行.py文件来看(会报错)，必须通过指定的命令来运行。另外在一些异步的程序中，可能会有一些时间错位的问题，这个似乎是无法避免的，除非把程序改为同步。最典型的例子就是pytorch的GPU与CPU的切换，这是一个异步过程。如果需要准确测速，需要将其设置为阻塞式的同步过程。</p>
</blockquote>
<ul>
<li>toy example</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 不用import line_profiler</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@profile</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_time</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>​    之后</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kernprof -l -v main.py  # 分析</span><br></pre></td></tr></table></figure>
<p>​    分析后会展示出结果，同时会生成 <code>main.py.lprof</code>的文件供之后随时查看，如果要运行这个文件:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python -m line_profiler main.py.lprof</span><br></pre></td></tr></table></figure>
<h2 id="memory-profiler"><a href="#memory-profiler" class="headerlink" title="memory_profiler"></a>memory_profiler</h2><blockquote>
<p>与line_profiler 功能类似，可以逐行查看内存的占用，优点是不同于line_profiler，其虽然也要加函数装饰器，但是可以通过import 这个库，然后直接运行.py文件来查看</p>
</blockquote>
<ul>
<li>toy example</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> memory_profiler</span><br><span class="line"></span><br><span class="line"><span class="meta">@profile(precision=4,stream=open('memory_profiler.log','w+'))</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_memory</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h2 id="pdb"><a href="#pdb" class="headerlink" title="pdb"></a>pdb</h2><blockquote>
<p>一行set_trace()即可实现断点添加的功能，适合偶尔使用，虽然不如很多IDE提供的debug功能丰富，但是日常快速使用足以。进入断点后就类似ipython了，是一个可交互的shell，随便打印什么看什么都行，还有一些命令可以帮助我们更好的debug。</p>
</blockquote>
<ul>
<li>toy example</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pdb</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">result = foo()</span><br><span class="line">pdb.set_trace()</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<ul>
<li>常用的命令：<ul>
<li><code>n</code>: 运行断点下一条语句</li>
<li><code>c</code>: 运行至下一个断点或结束</li>
<li><code>s</code>: 运行断点下一条语句，如果是函数，则进入函数</li>
<li><code>q</code>: 退出debug</li>
</ul>
</li>
</ul>
<h2 id="pipreqs，pigar"><a href="#pipreqs，pigar" class="headerlink" title="pipreqs，pigar"></a>pipreqs，pigar</h2><blockquote>
<p>用来做项目打包生成requirements.txt的时候要用到的库，没啥特殊的地方。pipreqs生成的比较简洁，但是要注意编码问题，可能会报gbk的错误，所以一般要显式的使用utf-8，而pigar可以具体到哪一行使用了这个库。看情况选择使用吧。</p>
</blockquote>
<ul>
<li><code>pipreqs</code>的toy example</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd target_path  # 进入需要打包的路径</span><br><span class="line">pipreqs ./ --encoding=utf-8</span><br></pre></td></tr></table></figure>
<ul>
<li><code>pigar</code>的toy example</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pigir  # 简单明了，生成当前所在目录的依赖，然后到requirements.txt里</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> pigar -p requirements_path -P target_path</span></span><br><span class="line">pigar -p ../dev-requirements.txt -P ../</span><br></pre></td></tr></table></figure>
<h1 id="dis"><a href="#dis" class="headerlink" title="dis"></a>dis</h1><blockquote>
<p>非常的神奇的库，可以直接将代码转化为汇编。之所以列出这个库，仅仅是因为我觉得它神奇，目前我还不知道要怎么用它…</p>
</blockquote>
<ul>
<li>toy example</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">import</span> dis</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(a, b = <span class="number">0</span>)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> a + b</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dis.dis(add)</span><br><span class="line">  <span class="number">2</span>           <span class="number">0</span> LOAD_FAST                <span class="number">0</span> (a)</span><br><span class="line">              <span class="number">2</span> LOAD_FAST                <span class="number">1</span> (b)</span><br><span class="line">              <span class="number">4</span> BINARY_ADD</span><br><span class="line">              <span class="number">6</span> RETURN_VALUE</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>花里胡哨的东西</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>科研</tag>
        <tag>sympy</tag>
        <tag>line_profiler</tag>
        <tag>memory_profiler</tag>
        <tag>pdb</tag>
        <tag>pipreqs</tag>
        <tag>pigar</tag>
        <tag>dis</tag>
      </tags>
  </entry>
  <entry>
    <title>Cmder高亮以及修改背景</title>
    <url>/2020/08/10/Cmder%E9%AB%98%E4%BA%AE%E4%BB%A5%E5%8F%8A%E4%BF%AE%E6%94%B9%E8%83%8C%E6%99%AF/</url>
    <content><![CDATA[<blockquote>
<p>开篇日常碎碎念: cmder是一个很好的终端模拟器，它基于ConEmu，可以美化系统自带的cmd命令行，然后还可以使用git，还自带有很多linux的shell命令，还可以作为vscode的运行终端。其实关于cmder的主题美化，官方的wiki也给出了一些可行插件，比如Terminus，但是它启动很慢，而且还有各种各样的恶性bug，所以属实难搞。同样，与cmder相关高亮插件对常使用python的玩家也不太友好，比如conda虚拟环境无法正常显示… 基于此，自己还是决定来简单配置一些比较easy的高亮和以及主题美化</p>
</blockquote>
<a id="more"></a>
<h2 id="Cmder-lambda替换与高亮"><a href="#Cmder-lambda替换与高亮" class="headerlink" title="Cmder lambda替换与高亮"></a>Cmder lambda替换与高亮</h2><ul>
<li><p>首先找到cmder的安装位置, 这里我的在<code>D:\cmder</code>中，然后进入<code>D:\cmder\vendor</code>目录下找到<code>clink.lua</code>这个文件</p>
</li>
<li><p>定位到如下函数</p>
</li>
</ul>
<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="keyword">local</span> <span class="function"><span class="keyword">function</span> <span class="title">set_prompt_filter</span><span class="params">()</span></span></span><br><span class="line">    <span class="comment">-- get_cwd() is differently encoded than the clink.prompt.value, so everything other than</span></span><br><span class="line">    <span class="comment">-- pure ASCII will get garbled. So try to parse the current directory from the original prompt</span></span><br><span class="line">    <span class="comment">-- and only if that doesn't work, use get_cwd() directly.</span></span><br><span class="line">    <span class="comment">-- The matching relies on the default prompt which ends in X:\PATH\PATH&gt;</span></span><br><span class="line">    <span class="comment">-- (no network path possible here!)</span></span><br><span class="line">    <span class="keyword">local</span> old_prompt = clink.prompt.value</span><br><span class="line">    <span class="keyword">local</span> cwd = old_prompt:<span class="built_in">match</span>(<span class="string">'.*(.:[^&gt;]*)&gt;'</span>)</span><br><span class="line">    <span class="keyword">if</span> cwd == <span class="literal">nil</span> <span class="keyword">then</span> cwd = clink.get_cwd() <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">-- environment systems like pythons virtualenv change the PROMPT and usually</span></span><br><span class="line">    <span class="comment">-- set some variable. But the variables are differently named and we would never</span></span><br><span class="line">    <span class="comment">-- get them all, so try to parse the env name out of the PROMPT.</span></span><br><span class="line">    <span class="comment">-- envs are usually put in round or square parentheses and before the old prompt</span></span><br><span class="line">    <span class="keyword">local</span> env = old_prompt:<span class="built_in">match</span>(<span class="string">'.*%(([^%)]+)%).+:'</span>)</span><br><span class="line">    <span class="comment">-- also check for square brackets</span></span><br><span class="line">    <span class="keyword">if</span> env == <span class="literal">nil</span> <span class="keyword">then</span> env = old_prompt:<span class="built_in">match</span>(<span class="string">'.*%[([^%]]+)%].+:'</span>) <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">-- build our own prompt</span></span><br><span class="line">    <span class="comment">-- orig: $E[1;32;40m$P$S&#123;git&#125;&#123;hg&#125;$S$_$E[1;30;40m&#123;lamb&#125;$S$E[0m</span></span><br><span class="line">    <span class="comment">-- color codes: "\x1b[1;37;40m"</span></span><br><span class="line">    <span class="keyword">local</span> cmder_prompt = <span class="string">"\x1b[1;36;40m&#123;cwd&#125; &#123;git&#125;&#123;hg&#125;&#123;svn&#125; \n\x1b[1;37;44m&#123;lamb&#125;\x1b[0;34;40m\x1b[0m"</span></span><br><span class="line">    <span class="keyword">local</span> lambda = <span class="string">"tsotfsk"</span></span><br><span class="line">    cmder_prompt = <span class="built_in">string</span>.<span class="built_in">gsub</span>(cmder_prompt, <span class="string">"&#123;cwd&#125;"</span>, verbatim(cwd))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> env ~= <span class="literal">nil</span> <span class="keyword">then</span></span><br><span class="line">        lambda = <span class="string">"["</span>..env..<span class="string">"]"</span>..lambda</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    clink.prompt.value = <span class="built_in">string</span>.<span class="built_in">gsub</span>(cmder_prompt, <span class="string">"&#123;lamb&#125;"</span>, verbatim(lambda))</span><br><span class="line">    <span class="comment">-- clink.prompt.value = string.gsub(cmder_prompt, "&#123;lamb&#125;", "tsotfsk")</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>虽然没学过lua，但是这段代码不难理解。</p>
</li>
<li><p>替换$\lambda$的话就把<code>lambda</code>赋值为自己想要的就可以了。比如我这里我直接用了我自己的ID</p>
</li>
<li><p>如果要做高亮之类的话，主要修改的是<code>cmder_prompt</code>这个变量。这里我先给出我目前的效果，如果嫌麻烦或者觉得我的还不错的话，可以直接复制我这段代码覆盖原函数，然后改一下<code>lambda</code>这个变量即可。</p>
<p><img src="/2020/08/10/Cmder%E9%AB%98%E4%BA%AE%E4%BB%A5%E5%8F%8A%E4%BF%AE%E6%94%B9%E8%83%8C%E6%99%AF/terminal.png" alt="terminal"></p>
</li>
<li><p>如果还想了解一下如何个性化定制，就需要研究那段字符串到底表达了什么。熟悉linux的小伙伴应该知道了，开头的<code>\x1b</code>是一个转义字符(16进制的ESC)，后面紧跟的<code>[</code>表示序列开头。那么这里我就简单列一些做高亮之类的，会用到的终端控制字符(来自百度)</p>
<blockquote>
<p>1.一些特效</p>
<p>\x1b[1m         设置高亮度</p>
<p>\x1b[4m         下划线</p>
<p>\x1b[5m         闪烁</p>
<p>\x1b[7m         反显</p>
<p>\x1b[8m         消隐</p>
<p>\x1b[30m — \x1b[37m   设置前景色</p>
<p>\x1b[40m — \x1b[47m   设置背景色</p>
<p>2.文字背景色彩数字: (颜色范围:40 - 47)</p>
<p>40:   黑色</p>
<p>41:   深红色</p>
<p>42:   绿色</p>
<p>43:   黄色</p>
<p>44:   蓝色</p>
<p>45:   紫色</p>
<p>46:   深绿色</p>
<p>47:   白色</p>
<p>3.文字前景色数字: (颜色范围: 30 - 39)</p>
<p>30:   黑色</p>
<p>31:   红色</p>
<p>32:   绿色</p>
<p>33:   黄色</p>
<p>34:   蓝色</p>
<p>35:   紫色</p>
<p>36:   深绿色</p>
<p>37:   白色</p>
</blockquote>
<p>那么<code>cmder_prompt</code>变量后的这段字符串就很好理解了，我们只需要比葫芦画瓢就可以自定义一些简单的高亮，比如我这里的的字符串为</p>
<p><code>&quot;\x1b[1;36;40m{cwd} {git}{hg}{svn} \n\x1b[1;37;44m{lamb}\x1b[0;34;40m\x1b[0m&quot;</code></p>
<p>从左向右开始：</p>
<ul>
<li><p><code>\x1b[1;36;40m{cwd}</code>：<code>1</code>设置了颜色为高亮度，<code>36</code>是前景色为深绿色，<code>40</code>表示后景色为黑色, <code>{cwd}</code>是工作路径变量，它是待定的，这个我们不关心。会在后面对其进行赋值，我们也不需要改动它</p>
</li>
<li><p><code>{git}{hg}{svn} \n</code>: 这里定义了你可能使用到的工具，它会被放在工作路径后面，我们也无需修改, <code>\n</code>表示在此处换行</p>
</li>
<li><p><code>\x1b[1;37;44m{lamb}</code>: 这里颜色部分不再赘述，最后的<code>{lamb}</code>就是你的lambda变量</p>
</li>
<li><p><code>\x1b[0;34;40m</code>: 这里最后会有一个乱码<code></code>，在我这里呈现的是一个三角形的符号，因为比较好看。但是不同字体展示出的这个结果可能是不一样的，如果要改成和我一样的，请在cmder的settings的font部分，选择可以呈现出这种形状的字体，注意，这些更改是会实时渲染的，所以可以开一个cmder的终端，然后在这里一边选(鼠标滚轮)，一边看终端的改变就可以了。我简单试了下，系统自带的很多字体都可以达到这种效果的，不用额外下载。比如<code>Marlett</code></p>
<p><img src="/2020/08/10/Cmder%E9%AB%98%E4%BA%AE%E4%BB%A5%E5%8F%8A%E4%BF%AE%E6%94%B9%E8%83%8C%E6%99%AF/fonts.png" alt="fonts"></p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><code>\x1b[0m</code>也就是关闭所有属性。后面该是啥是啥了，相当于结束。</p>
<p>依据这些，我们就可以简单的做一些定制化的操作了~</p>
</li>
</ul>
<h2 id="更换cmder主题"><a href="#更换cmder主题" class="headerlink" title="更换cmder主题"></a>更换cmder主题</h2><p>cmder自带的一些主题属实难看，经常使用一些ide的同学可能会使用一些比如OneDark之类的主题，这些主题在cmder实际上都是可以复刻的！！</p>
<p>由于cmder基于conemu，有大佬已经收集了一些conemu的16位色的主题复刻:<a href="https://github.com/martinlindhe/base16-conemu" target="_blank" rel="noopener">base16-conemu</a>，那我们当然可以拿来用！！</p>
<ul>
<li><p>首先下载下来，找到自己喜欢的主题，复制对应的xml的内容</p>
</li>
<li><p>然后从cmder的setting找到 <code>conemu.xml</code>的文件位置，如上图，我的就是<code>D:\cmder\vendor\conemu-maximus5\ConEmu.xml</code></p>
</li>
<li><p>打开这个xml，检索到如下头部内容，可以检索<code>&lt;key name=&quot;Colors&quot;</code></p>
</li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">key</span> <span class="attr">name</span>=<span class="string">"Colors"</span> <span class="attr">modified</span>=<span class="string">"2015-03-19 13:53:09"</span> <span class="attr">build</span>=<span class="string">"150310"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span> <span class="attr">name</span>=<span class="string">"Count"</span> <span class="attr">type</span>=<span class="string">"dword"</span> <span class="attr">data</span>=<span class="string">"00000001"</span>/&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>将你的内容复制到这之后就可以了，但还有两步很关键的步骤要做~</p>
</li>
<li><p>首先要把这里的<code>data</code>改成你当前加入的主题数！！，第一次加的话，就默认在<code>data</code>上加个1就行了</p>
</li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">value</span> <span class="attr">name</span>=<span class="string">"Count"</span> <span class="attr">type</span>=<span class="string">"long"</span> <span class="attr">data</span>=<span class="string">"4"</span>/&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>更改你复制xml部分的第一行的name</li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">key</span> <span class="attr">name</span>=<span class="string">"Palette1"</span> <span class="attr">modified</span>=<span class="string">"2020-08-09 10:06:06"</span> <span class="attr">build</span>=<span class="string">"191012"</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>把这里的<code>Palette1</code>也是顺序更改就可以了，主要看你当前加入了多少主题。比如改成<code>Palette2</code></p>
<p>那么至此，我们就把主题添加到了cmder中，之后主要在cmder的setting的颜色部分，选择你的方案就可以了，注意下面的颜色的Auto属性不要更改。</p>
<p><img src="/2020/08/10/Cmder%E9%AB%98%E4%BA%AE%E4%BB%A5%E5%8F%8A%E4%BF%AE%E6%94%B9%E8%83%8C%E6%99%AF/theme.png" alt="theme"></p>
]]></content>
      <categories>
        <category>花里胡哨的东西</category>
      </categories>
      <tags>
        <tag>cmder</tag>
        <tag>base16</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title>EffectivePython读书笔记</title>
    <url>/2020/03/07/EffectivePython%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<blockquote>
<p>对python的高级编程还了解不多，所以来取取经。这本书看起来挺快的，大概花了我一天的时间吧。这里整理下笔记，以便以后能用到。这里略过了一些比如线程和协程之类的使用。</p>
</blockquote>
<a id="more"></a>
<h3 id="2-遵循PEP8风格"><a href="#2-遵循PEP8风格" class="headerlink" title="2.遵循PEP8风格"></a>2.遵循PEP8风格</h3><p>这里我只列几个我不曾注意到的。参考<a href="https://www.cnblogs.com/bymo/p/9567140.html#_label2_0" target="_blank" rel="noopener">PEP 8 中文</a></p>
<ul>
<li><p>import 不要用相对位置。如果导入系统没有正确的配置（比如包里的一个目录在sys.path里的路径后），使用绝对路径会更加可读并且性能更好（至少能提供更好的错误信息）:</p>
</li>
<li><p>二元运算符换行放行首</p>
</li>
<li><p>算术表达式问题。总是在二元运算符两边加一个空格：赋值（=），增量赋值（+=，-=），比较（==,&lt;,&gt;,!=,&lt;&gt;,&lt;=,&gt;=,in,not,in,is,is not），布尔（and, or, not）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">i = i + <span class="number">1</span></span><br><span class="line">submitted += <span class="number">1</span></span><br><span class="line">x = x*<span class="number">2</span> - <span class="number">1</span></span><br><span class="line">hypot2 = x*x + y*y</span><br><span class="line">c = (a+b) * (a-b)</span><br></pre></td></tr></table></figure>
</li>
<li><p>文档字符单行的话三引号不换行，两行及以上第二个应当换行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""Return a foobang</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Optional plotz says to frobnicate the bizbaz first.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>from M import * 导入的模块应该使用all机制去防止内部的接口对外暴露，或者使用在全局变量前加下划线的方式</p>
</li>
<li><p>从Exception继承异常，而不是BaseException。</p>
</li>
<li><p>使用 ”.startswith() 和 ”.endswith() 代替通过字符串切割的方法去检查前缀和后缀。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">推荐: <span class="keyword">if</span> foo.startswith(<span class="string">'bar'</span>):</span><br><span class="line">糟糕: <span class="keyword">if</span> foo[:<span class="number">3</span>] == <span class="string">'bar'</span>:</span><br></pre></td></tr></table></figure>
</li>
<li><p>对象类型的比较应该用isinstance()而不是直接比较type。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">正确: <span class="keyword">if</span> isinstance(obj, int):</span><br><span class="line">糟糕: <span class="keyword">if</span> type(obj) <span class="keyword">is</span> type(<span class="number">1</span>):</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="3-了解bytes、str和unicode的区别"><a href="#3-了解bytes、str和unicode的区别" class="headerlink" title="3.了解bytes、str和unicode的区别"></a>3.了解bytes、str和unicode的区别</h3><p><img src="/2020/03/07/EffectivePython%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/编码.png" alt></p>
<p>程序核心部分应使用Unicode，编码与解码应放在外围部分。</p>
<h3 id="9-用生成器表达式来改写数据量较大的列表推导"><a href="#9-用生成器表达式来改写数据量较大的列表推导" class="headerlink" title="9.用生成器表达式来改写数据量较大的列表推导"></a>9.用生成器表达式来改写数据量较大的列表推导</h3><p>优点就是生成器不用一次载入，格式是(…)</p>
<h3 id="13-try-except-else-finally"><a href="#13-try-except-else-finally" class="headerlink" title="13.try/except/else/finally"></a>13.try/except/else/finally</h3><p>try代码不应过多，只做主要判断，应写在else中。</p>
<h3 id="14-尽量用异常来表示特殊情况，而不要返回None"><a href="#14-尽量用异常来表示特殊情况，而不要返回None" class="headerlink" title="14.尽量用异常来表示特殊情况，而不要返回None"></a>14.尽量用异常来表示特殊情况，而不要返回None</h3><p>因为None可以通过if条件语句。</p>
<h3 id="17-在参数上迭代时，要小心"><a href="#17-在参数上迭代时，要小心" class="headerlink" title="17.在参数上迭代时，要小心"></a>17.在参数上迭代时，要小心</h3><p>因为迭代器只能迭代一次。</p>
<h3 id="20-用None和文档字符串来描述具有动态默认值的参数"><a href="#20-用None和文档字符串来描述具有动态默认值的参数" class="headerlink" title="20.用None和文档字符串来描述具有动态默认值的参数"></a>20.用None和文档字符串来描述具有动态默认值的参数</h3><p>也就是参数默认值不要给一个具体的，比如函数，或者空字典，因为默认参数在载入的时候只初始化一遍。如果return的恰好也是默认参数会出问题。</p>
<h3 id="21-用只能以关键字形式指定的参数来确保代码明晰"><a href="#21-用只能以关键字形式指定的参数来确保代码明晰" class="headerlink" title="21.用只能以关键字形式指定的参数来确保代码明晰"></a>21.用只能以关键字形式指定的参数来确保代码明晰</h3><p>尽量不要采用位置赋值，而要采用关键字赋值，易读性强也更容易维护。</p>
<p>为了避免关键字参数被按位置参数赋值，可以使用<em>*kwargs来提取参数，但是可读性差一点，要求增加函数文档，python3的话可以通过在位置参数和关键字参数中间增加一个` </em> `</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">(number,dvisor, *, ignore=False)</span></span></span><br></pre></td></tr></table></figure>
<h3 id="25-用super初始化父类"><a href="#25-用super初始化父类" class="headerlink" title="25.用super初始化父类"></a>25.用super初始化父类</h3><p>python 3支持不带参数的super，所以没必要加参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">super().__init__(...)</span><br></pre></td></tr></table></figure>
<h3 id="26-只在私用mix-in组件的时候进行多重继承"><a href="#26-只在私用mix-in组件的时候进行多重继承" class="headerlink" title="26.只在私用mix-in组件的时候进行多重继承"></a>26.只在私用mix-in组件的时候进行多重继承</h3><p>这个了解一下吧，mix-in提供的就是一系列通用功能。</p>
<h3 id="27-多用public属性，少用private属性"><a href="#27-多用public属性，少用private属性" class="headerlink" title="27.多用public属性，少用private属性"></a>27.多用public属性，少用private属性</h3><p>这个挺重要的，python的private属性实际上是假的，比如类Server含有一个<code>self.__post</code>的私有属性，虽然我无法通过函数去返回这个，但是我可以直接调用<code>_Server__post</code>来获得这个属性，那何必自欺欺人呢，大家都是成年人了。真需要的话不如加一个_，就当是提醒别人注意吧。</p>
<h3 id="29-用纯属性来取代get和set方法"><a href="#29-用纯属性来取代get和set方法" class="headerlink" title="29. 用纯属性来取代get和set方法"></a>29. 用纯属性来取代get和set方法</h3><p>这不是python风格的…我也挺想吐槽了，根本没必要。</p>
<h3 id="32-用-getarr-、-getattribute-和-setattr-实现按需生成的属性"><a href="#32-用-getarr-、-getattribute-和-setattr-实现按需生成的属性" class="headerlink" title="32. 用__getarr__、__getattribute__ 和__setattr__实现按需生成的属性"></a>32. 用<code>__getarr__</code>、<code>__getattribute__</code> 和<code>__setattr__</code>实现按需生成的属性</h3><p>这些就是在获取类中不存在的值的时候定义的操作，比如data中没有foo这个变量，那么当我调用data.foo的时候会去调用<code>__getarr__</code> 然后我在里面使用 setattr(self, name, value)就可以将foo这个变量添加到类的<code>__dict__</code> 中。</p>
<h3 id="41-考虑使用concurrent-futures-来实现真正的平行计算"><a href="#41-考虑使用concurrent-futures-来实现真正的平行计算" class="headerlink" title="41.考虑使用concurrent.futures 来实现真正的平行计算"></a>41.考虑使用concurrent.futures 来实现真正的平行计算</h3><p>这里大概讲的多进程的东西，还提到了不要使用multiprocessing，说是有一些复杂特性，要避开？但python的多进程本来就挺麻烦的…</p>
<h3 id="42-用functools-wraps定义函数修饰器"><a href="#42-用functools-wraps定义函数修饰器" class="headerlink" title="42. 用functools.wraps定义函数修饰器"></a>42. 用functools.wraps定义函数修饰器</h3><p>之所以要用这个，是因为修饰器会更改函数的名字，这可能影响调试器等的效果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trace</span><span class="params">(func)</span>:</span></span><br><span class="line"><span class="meta">	@wraps(func)</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">		<span class="comment"># ...</span></span><br><span class="line">	<span class="keyword">return</span> wrapper</span><br><span class="line">	</span><br><span class="line"><span class="meta">@trace</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci</span><span class="params">(n)</span></span></span><br><span class="line"><span class="function">	# ...</span></span><br></pre></td></tr></table></figure>
<h3 id="43-考虑以contextlib和with语句来改写可复用的try-finally代码"><a href="#43-考虑以contextlib和with语句来改写可复用的try-finally代码" class="headerlink" title="43.考虑以contextlib和with语句来改写可复用的try/finally代码"></a>43.考虑以contextlib和with语句来改写可复用的try/finally代码</h3><p>这个可以定义一宗情景管理器，比如以下代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@contextmanager</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">debug_logging</span><span class="params">(level)</span>:</span></span><br><span class="line">	logger = logging.getLogger()</span><br><span class="line">	old_level = logger.getEffectiveLevel()</span><br><span class="line">	logger . setLevel(level)</span><br><span class="line">	<span class="keyword">try</span>:</span><br><span class="line">		<span class="keyword">yield</span></span><br><span class="line">	<span class="keyword">finally</span>:</span><br><span class="line">		logger.setLevel(old_level)</span><br><span class="line"><span class="keyword">with</span> debug_logging(logging.DEBUG):</span><br><span class="line">    <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<p>这段代码定义了一个可以暂时提升logging的level的方法，在执行到yield的时候会进入with下面的语句执行，执行完毕后会返回，然后执行finally中的语句。</p>
<h3 id="44-用copyreg实现可靠的pickle操作"><a href="#44-用copyreg实现可靠的pickle操作" class="headerlink" title="44.用copyreg实现可靠的pickle操作"></a>44.用copyreg实现可靠的pickle操作</h3><p>pickle是不可靠的序列化(json是可靠的序列化)，所以其只能建立在相互信任的程序之间使用。它可以保存一些类啊什么的，使用copyreg是在类的定义或者什么改变的条件下，依然可以load的解决方法。</p>
<h3 id="46-使用内置算法与数据结构"><a href="#46-使用内置算法与数据结构" class="headerlink" title="46.使用内置算法与数据结构"></a>46.使用内置算法与数据结构</h3><p>这个用到再说吧，主要是有一些队列啊，有序字典(python 3.6+后字典都是有序的)，还有堆栈啊什么的，以后做leetcode可以用一用。</p>
<h3 id="51-为自编的模块定义根异常，以便将调用者与API相隔离"><a href="#51-为自编的模块定义根异常，以便将调用者与API相隔离" class="headerlink" title="51.为自编的模块定义根异常，以便将调用者与API相隔离"></a>51.为自编的模块定义根异常，以便将调用者与API相隔离</h3><p>这个没啥，注意到基类比如叫Error继承自Exception就可以了。</p>
<h3 id="53-用适当的方式打破循环依赖关系"><a href="#53-用适当的方式打破循环依赖关系" class="headerlink" title="53.用适当的方式打破循环依赖关系"></a>53.用适当的方式打破循环依赖关系</h3><p>我觉得还是不要写这种代码好了，如果真写了，再回来看吧</p>
<h3 id="56-用unittest来测试全部代码"><a href="#56-用unittest来测试全部代码" class="headerlink" title="56.用unittest来测试全部代码"></a>56.用unittest来测试全部代码</h3><p>保留意见</p>
<h3 id="57-用pdb来实现交互调试"><a href="#57-用pdb来实现交互调试" class="headerlink" title="57.用pdb来实现交互调试"></a>57.用pdb来实现交互调试</h3><p>这个要学一下，一个pdb.set_trace()就可以开始调试了，很方便啊。</p>
]]></content>
      <tags>
        <tag>python</tag>
        <tag>effective</tag>
      </tags>
  </entry>
  <entry>
    <title>GraphSAGE笔记</title>
    <url>/2020/02/19/GraphSAGE%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<blockquote>
<p>GraphSAGE是inductive learning，而原始的GCN一个是transductive learning，那么他们的差别在哪里呢？GraphSAGE又在讲一个什么东西呢？</p>
</blockquote>
<h2 id="一、什么是transductive什么是inductive"><a href="#一、什么是transductive什么是inductive" class="headerlink" title="一、什么是transductive什么是inductive"></a>一、什么是transductive什么是inductive</h2>]]></content>
  </entry>
  <entry>
    <title>矩阵的matmul运算</title>
    <url>/2020/01/26/%E7%9F%A9%E9%98%B5%E7%9A%84matmul%E8%BF%90%E7%AE%97/</url>
    <content><![CDATA[<blockquote>
<p>这个是一个很容易搞晕的东西，比如什么时候两个矩阵乘法是做广播(boardcast)，什么时候是做矩阵点积(matmul product)，有时候搞错了也找不到bug在哪里，所以我在这里总结一下。具体内容参考了<a href="https://pytorch.org/docs/stable/torch.html?highlight=matmul#torch.matmul" target="_blank" rel="noopener">matmul</a>和<a href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics" target="_blank" rel="noopener">boardcast</a>的官方文档。</p>
</blockquote>
<a id="more"></a>
<h3 id="矩阵的matmul操作"><a href="#矩阵的matmul操作" class="headerlink" title="矩阵的matmul操作"></a>矩阵的matmul操作</h3><ul>
<li><p>都是一维的话就是正常的点积,不存在广播</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自然是必须维度一致了</span></span><br><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.ones(<span class="number">5</span>)</span><br><span class="line">print(a.matmul(b))</span><br><span class="line"><span class="comment"># output =&gt; </span></span><br><span class="line"><span class="comment"># tensor(5.)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>都是二维就是做矩阵的正常的点积,不存在广播</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.ones(<span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line">b = torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a.matmul(b))</span><br><span class="line"><span class="comment"># output =&gt; </span></span><br><span class="line"><span class="comment"># tensor([[2., 2., 2.],</span></span><br><span class="line"><span class="comment">#        [2., 2., 2.],</span></span><br><span class="line"><span class="comment">#        [2., 2., 2.],</span></span><br><span class="line"><span class="comment">#        [2., 2., 2.]])</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>如果前者是一维的$m$，后者是二维$m \times n$的，则运算过程相当于将前者转化为$1 \times m$，然后运算后去掉补充的$1$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.ones(<span class="number">2</span>)</span><br><span class="line">b = torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a.matmul(b).shape)</span><br><span class="line"><span class="comment"># output =&gt; </span></span><br><span class="line"><span class="comment"># torch.Size([3])</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>与上一条对称，如果如果前者是二维的$m \times n$，后者是一维的$n$的情况</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.ones(<span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line">b = torch.ones(<span class="number">2</span>)</span><br><span class="line">print(a.matmul(b).shape)</span><br><span class="line"><span class="comment"># output =&gt; </span></span><br><span class="line"><span class="comment"># torch.Size([4])</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>如果其中一个二维的或者一维的，另一个是$N$维的，$N &gt; 2$，那么相当于做的事<code>batched matrix multiply</code>，当然其对称情况也同理，<strong>$N$维的只有后两维才参与运算！！</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 假如a是1维的，这个就很容易出错了，注意其对其方式</span></span><br><span class="line">a = torch.ones(      <span class="number">4</span>)  <span class="comment"># 如果是2,3或者5会报错！！！</span></span><br><span class="line">b = torch.ones(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 相当于是(1,4),与一个((2,3),(4,5))相乘得到((2,3),(1,5)),然后去掉1就是(2,3,5)</span></span><br><span class="line">print(a.matmul(b).shape)</span><br><span class="line"><span class="comment"># output =&gt; </span></span><br><span class="line"><span class="comment"># torch.Size([2, 3, 5])</span></span><br><span class="line"><span class="comment"># 如果a是二维的 </span></span><br><span class="line"><span class="comment"># a = torch.ones(    6,4)</span></span><br><span class="line"><span class="comment"># output =&gt; </span></span><br><span class="line"><span class="comment"># torch.Size([2,3,6,5])</span></span><br></pre></td></tr></table></figure>
<p>自然其对称情况也同理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.ones(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">b = torch.ones(      <span class="number">5</span>)  <span class="comment"># 注意这里应该是5了,因为扩展后是(5,1)</span></span><br><span class="line"><span class="comment"># 相当于是((2,3),(4,5))与一个(5,1)相乘得到((2,3),(4,1)),然后去掉1就是(2,3,4)</span></span><br><span class="line">print(a.matmul(b).shape)</span><br><span class="line"><span class="comment"># output =&gt; </span></span><br><span class="line"><span class="comment"># torch.Size([2, 3, 4])</span></span><br><span class="line"><span class="comment"># 如果b是二维的 </span></span><br><span class="line"><span class="comment"># a = torch.ones(    5,6)</span></span><br><span class="line"><span class="comment"># output =&gt; </span></span><br><span class="line"><span class="comment"># torch.Size([2,3,4,6])</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>如果是非单纯的batch点乘形式的（不是上面几种情况的），那么就要对一部分做boardcast，也就是广播，那么其必须就要满足广播的条件，那么广播的条件是什么呢？</p>
<ul>
<li>两个张量可以广播运算，当且仅当<strong>从最后一维向前看，他们的维度要么相等，要么其中有一个是$1$，要么其中一个维度不存在，也就是到头了。注意！！！这里的广播是点乘的广播！！！，所以实际是从倒数第三位开始看</strong></li>
</ul>
<p>也就是说我们可以把后两个维度看作是一个整体，比如说是一个元素$k$，那么一个$m\times n \times p \times q$的矩阵与一个$r \times q \times v$的矩阵如果使用<code>torch.matmul()</code>，那么可以理解为一个$m\times n$的矩阵与一个$r$的一维矩阵的运算，只不过这里矩阵里的元素不再是一个数，而是一个矩阵$k$，而这种运算是点乘而已。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.ones(<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">b = torch.ones(  <span class="number">6</span>,<span class="number">4</span>,<span class="number">5</span>) </span><br><span class="line"><span class="comment"># 注意这里的点积是发生在(3,4)和(4,5)之间的，广播是发生在1和6之间的，这里的1和6相当于广播中的最后一位。</span></span><br><span class="line">print(a.matmul(b).shape)</span><br><span class="line"><span class="comment"># output =&gt; </span></span><br><span class="line"><span class="comment"># torch.Size([2, 6, 3, 5])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我这里再举例一个复杂的例子</span></span><br><span class="line">a = torch.ones(<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>)</span><br><span class="line">b = torch.ones(  <span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">8</span>) </span><br><span class="line">print(a.matmul(b).shape)</span><br><span class="line"><span class="comment"># output =&gt; </span></span><br><span class="line"><span class="comment"># torch.Size([2, 2, 3, 4, 5, 6, 8])</span></span><br><span class="line"><span class="comment"># 可以自行分析一个这个例子，按照传统的加减乘除的广播类比到这里理解就可以了，也就是说相当于是</span></span><br><span class="line"><span class="comment"># (2,1,3,4,5) 和 (2,3,1,5)之间的广播。</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><code>torch.matmul()</code>运算的两个对象中，如果有任何一个是二维以内的矩阵，那么做的就是带batch的普通点积，如果两个都是二维以外的话，那么做的就是带有广播性质的点乘，它广播的内容是后两位的点积，那么可以将后两位构成的矩阵，当作一个整体，看作是前$N-2$维矩阵的元素，那么matmul运算就与$N-2$维矩阵的加减乘除的广播类似，只不过这里的运算不再是加减乘除，而是点积。</p>
]]></content>
      <categories>
        <category>pytorch备忘录</category>
      </categories>
      <tags>
        <tag>torch.matmul</tag>
        <tag>boardcast</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN变长序列的pack和unpack问题</title>
    <url>/2020/01/26/RNN%E5%8F%98%E9%95%BF%E5%BA%8F%E5%88%97%E7%9A%84pack%E5%92%8Cunpack%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://github.com/HarshTrivedi/packing-unpacking-pytorch-minimal-tutorial" target="_blank" rel="noopener">原文章</a>讲的非常简单易懂，这里直接复制粘贴了，以便随时查阅。<strong>重点就是要对序列长度进行降序排序，还有就是要设置一个padding，比如num_embeddings = num_words + 1， 其中第0个位置作为padding填充序列。</strong></p>
</blockquote>
<p>We want to run LSTM on a batch of 3 character sequences <code>[&#39;long_str&#39;, &#39;tiny&#39;, &#39;medium&#39;]</code>. Here are the steps. You would be interested in *ed steps only.</p>
<ul>
<li><strong>Step 1</strong>: Construct Vocabulary</li>
<li><strong>Step 2</strong>: Load indexed data (list of instances, where each instance is list of character indices)</li>
<li><strong>Step 3</strong>: Make Model</li>
<li><strong>Step 4</strong>: <strong>*</strong> Pad instances with 0s till max length sequence</li>
<li><strong>Step 5</strong>: <strong>*</strong> Sort instances by sequence length in descending order</li>
<li><strong>Step 6</strong>: <strong>*</strong> Embed the instances</li>
<li><strong>Step 7</strong>: <strong>*</strong> Call pack_padded_sequence with embeded instances and sequence lengths</li>
<li><strong>Step 8</strong>: <strong>*</strong> Forward with LSTM</li>
<li><strong>Step 9</strong>: <strong>*</strong> Call unpack_padded_sequences if required / or just pick last hidden vector</li>
<li>* Summary of Shape Transformations</li>
</ul>
<a id="more"></a>
<h3 id="We-want-to-run-LSTM-on-a-batch-following-3-character-sequences"><a href="#We-want-to-run-LSTM-on-a-batch-following-3-character-sequences" class="headerlink" title="We want to run LSTM on a batch following 3 character sequences"></a>We want to run LSTM on a batch following 3 character sequences</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">seqs = [<span class="string">'long_str'</span>,  <span class="comment"># len = 8</span></span><br><span class="line">        <span class="string">'tiny'</span>,      <span class="comment"># len = 4</span></span><br><span class="line">        <span class="string">'medium'</span>]    <span class="comment"># len = 6</span></span><br></pre></td></tr></table></figure>
<h3 id="Step-1-Construct-Vocabulary"><a href="#Step-1-Construct-Vocabulary" class="headerlink" title="Step 1: Construct Vocabulary"></a>Step 1: Construct Vocabulary</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># make sure &lt;pad&gt; idx is 0</span></span><br><span class="line">vocab = [<span class="string">'&lt;pad&gt;'</span>] + sorted(set([char <span class="keyword">for</span> seq <span class="keyword">in</span> seqs <span class="keyword">for</span> char <span class="keyword">in</span> seq]))</span><br><span class="line"><span class="comment"># =&gt; ['&lt;pad&gt;', '_', 'd', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'y']</span></span><br></pre></td></tr></table></figure>
<h3 id="Step-2-Load-indexed-data-list-of-instances-where-each-instance-is-list-of-character-indices"><a href="#Step-2-Load-indexed-data-list-of-instances-where-each-instance-is-list-of-character-indices" class="headerlink" title="Step 2: Load indexed data (list of instances, where each instance is list of character indices)"></a>Step 2: Load indexed data (list of instances, where each instance is list of character indices)</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vectorized_seqs = [[vocab.index(tok) <span class="keyword">for</span> tok <span class="keyword">in</span> seq]<span class="keyword">for</span> seq <span class="keyword">in</span> seqs]</span><br><span class="line"><span class="comment"># vectorized_seqs =&gt; [[6, 9, 8, 4, 1, 11, 12, 10],</span></span><br><span class="line"><span class="comment">#                     [12, 5, 8, 14],</span></span><br><span class="line"><span class="comment">#                     [7, 3, 2, 5, 13, 7]]</span></span><br></pre></td></tr></table></figure>
<h3 id="Step-3-Make-Model"><a href="#Step-3-Make-Model" class="headerlink" title="Step 3: Make Model"></a>Step 3: Make Model</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed = Embedding(len(vocab), <span class="number">4</span>) <span class="comment"># embedding_dim = 4</span></span><br><span class="line">lstm = LSTM(input_size=<span class="number">4</span>, hidden_size=<span class="number">5</span>, batch_first=<span class="literal">True</span>) <span class="comment"># input_dim = 4, hidden_dim = 5</span></span><br></pre></td></tr></table></figure>
<h3 id="Step-4-Pad-instances-with-0s-till-max-length-sequence"><a href="#Step-4-Pad-instances-with-0s-till-max-length-sequence" class="headerlink" title="Step 4: Pad instances with 0s till max length sequence"></a>Step 4: Pad instances with 0s till max length sequence</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get the length of each seq in your batch</span></span><br><span class="line">seq_lengths = LongTensor(list(map(len, vectorized_seqs)))</span><br><span class="line"><span class="comment"># seq_lengths =&gt; [ 8, 4,  6]</span></span><br><span class="line"><span class="comment"># batch_sum_seq_len: 8 + 4 + 6 = 18</span></span><br><span class="line"><span class="comment"># max_seq_len: 8</span></span><br><span class="line"></span><br><span class="line">seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()</span><br><span class="line"><span class="comment"># seq_tensor =&gt; [[0 0 0 0 0 0 0 0]</span></span><br><span class="line"><span class="comment">#                [0 0 0 0 0 0 0 0]</span></span><br><span class="line"><span class="comment">#                [0 0 0 0 0 0 0 0]]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, (seq, seqlen) <span class="keyword">in</span> enumerate(zip(vectorized_seqs, seq_lengths)):</span><br><span class="line">    seq_tensor[idx, :seqlen] = LongTensor(seq)</span><br><span class="line"><span class="comment"># seq_tensor =&gt; [[ 6  9  8  4  1 11 12 10]          # long_str</span></span><br><span class="line"><span class="comment">#                [12  5  8 14  0  0  0  0]          # tiny</span></span><br><span class="line"><span class="comment">#                [ 7  3  2  5 13  7  0  0]]         # medium</span></span><br><span class="line"><span class="comment"># seq_tensor.shape : (batch_size X max_seq_len) = (3 X 8)</span></span><br></pre></td></tr></table></figure>
<h3 id="Step-5-Sort-instances-by-sequence-length-in-descending-order"><a href="#Step-5-Sort-instances-by-sequence-length-in-descending-order" class="headerlink" title="Step 5: Sort instances by sequence length in descending order"></a>Step 5: Sort instances by sequence length in descending order</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">seq_lengths, perm_idx = seq_lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">seq_tensor = seq_tensor[perm_idx]</span><br><span class="line"><span class="comment"># seq_tensor =&gt; [[ 6  9  8  4  1 11 12 10]           # long_str</span></span><br><span class="line"><span class="comment">#                [ 7  3  2  5 13  7  0  0]           # medium</span></span><br><span class="line"><span class="comment">#                [12  5  8 14  0  0  0  0]]          # tiny</span></span><br><span class="line"><span class="comment"># seq_tensor.shape : (batch_size X max_seq_len) = (3 X 8)</span></span><br></pre></td></tr></table></figure>
<h3 id="Step-6-Embed-the-instances"><a href="#Step-6-Embed-the-instances" class="headerlink" title="Step 6: Embed the instances"></a>Step 6: Embed the instances</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embedded_seq_tensor = embed(seq_tensor)</span><br><span class="line"><span class="comment"># embedded_seq_tensor =&gt;</span></span><br><span class="line"><span class="comment">#                       [[[-0.77578706 -1.8080667  -1.1168439   1.1059115 ]     l</span></span><br><span class="line"><span class="comment">#                         [-0.23622951  2.0361056   0.15435742 -0.04513785]     o</span></span><br><span class="line"><span class="comment">#                         [-0.6000342   1.1732816   0.19938554 -1.5976517 ]     n</span></span><br><span class="line"><span class="comment">#                         [ 0.40524676  0.98665565 -0.08621677 -1.1728264 ]     g</span></span><br><span class="line"><span class="comment">#                         [-1.6334635  -0.6100042   1.7509955  -1.931793  ]     _</span></span><br><span class="line"><span class="comment">#                         [-0.6470658  -0.6266589  -1.7463604   1.2675372 ]     s</span></span><br><span class="line"><span class="comment">#                         [ 0.64004815  0.45813003  0.3476034  -0.03451729]     t</span></span><br><span class="line"><span class="comment">#                         [-0.22739866 -0.45782727 -0.6643252   0.25129375]]    r</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#                        [[ 0.16031227 -0.08209462 -0.16297023  0.48121014]     m</span></span><br><span class="line"><span class="comment">#                         [-0.7303265  -0.857339    0.58913064 -1.1068314 ]     e</span></span><br><span class="line"><span class="comment">#                         [ 0.48159844 -1.4886451   0.92639893  0.76906884]     d</span></span><br><span class="line"><span class="comment">#                         [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     i</span></span><br><span class="line"><span class="comment">#                         [ 0.01795524 -0.59048957 -0.53800726 -0.6611691 ]     u</span></span><br><span class="line"><span class="comment">#                         [ 0.16031227 -0.08209462 -0.16297023  0.48121014]     m</span></span><br><span class="line"><span class="comment">#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     &lt;pad&gt;</span></span><br><span class="line"><span class="comment">#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]]    &lt;pad&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#                        [[ 0.64004815  0.45813003  0.3476034  -0.03451729]     t</span></span><br><span class="line"><span class="comment">#                         [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     i</span></span><br><span class="line"><span class="comment">#                         [-0.6000342   1.1732816   0.19938554 -1.5976517 ]     n</span></span><br><span class="line"><span class="comment">#                         [-1.284392    0.68294704  1.4064184  -0.42879772]     y</span></span><br><span class="line"><span class="comment">#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     &lt;pad&gt;</span></span><br><span class="line"><span class="comment">#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     &lt;pad&gt;</span></span><br><span class="line"><span class="comment">#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     &lt;pad&gt;</span></span><br><span class="line"><span class="comment">#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]]]   &lt;pad&gt;</span></span><br><span class="line"><span class="comment"># embedded_seq_tensor.shape : (batch_size X max_seq_len X embedding_dim) = (3 X 8 X 4)</span></span><br></pre></td></tr></table></figure>
<h3 id="Step-7-Call-pack-padded-sequence-with-embeded-instances-and-sequence-lengths"><a href="#Step-7-Call-pack-padded-sequence-with-embeded-instances-and-sequence-lengths" class="headerlink" title="Step 7: Call pack_padded_sequence with embeded instances and sequence lengths"></a>Step 7: Call pack_padded_sequence with embeded instances and sequence lengths</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># packed_input (PackedSequence is NamedTuple with 2 attributes: data and batch_sizes</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># packed_input.data =&gt;</span></span><br><span class="line"><span class="comment">#                         [[-0.77578706 -1.8080667  -1.1168439   1.1059115 ]     l</span></span><br><span class="line"><span class="comment">#                          [ 0.01795524 -0.59048957 -0.53800726 -0.6611691 ]     m</span></span><br><span class="line"><span class="comment">#                          [-0.6470658  -0.6266589  -1.7463604   1.2675372 ]     t</span></span><br><span class="line"><span class="comment">#                          [ 0.16031227 -0.08209462 -0.16297023  0.48121014]     o</span></span><br><span class="line"><span class="comment">#                          [ 0.40524676  0.98665565 -0.08621677 -1.1728264 ]     e</span></span><br><span class="line"><span class="comment">#                          [-1.284392    0.68294704  1.4064184  -0.42879772]     i</span></span><br><span class="line"><span class="comment">#                          [ 0.64004815  0.45813003  0.3476034  -0.03451729]     n</span></span><br><span class="line"><span class="comment">#                          [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     d</span></span><br><span class="line"><span class="comment">#                          [ 0.64004815  0.45813003  0.3476034  -0.03451729]     n</span></span><br><span class="line"><span class="comment">#                          [-0.23622951  2.0361056   0.15435742 -0.04513785]     g</span></span><br><span class="line"><span class="comment">#                          [ 0.16031227 -0.08209462 -0.16297023  0.48121014]     i</span></span><br><span class="line"><span class="comment">#                          [-0.22739866 -0.45782727 -0.6643252   0.25129375]]    y</span></span><br><span class="line"><span class="comment">#                          [-0.7303265  -0.857339    0.58913064 -1.1068314 ]     _</span></span><br><span class="line"><span class="comment">#                          [-1.6334635  -0.6100042   1.7509955  -1.931793  ]     u</span></span><br><span class="line"><span class="comment">#                          [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     s</span></span><br><span class="line"><span class="comment">#                          [-0.6000342   1.1732816   0.19938554 -1.5976517 ]     m</span></span><br><span class="line"><span class="comment">#                          [-0.6000342   1.1732816   0.19938554 -1.5976517 ]     t</span></span><br><span class="line"><span class="comment">#                          [ 0.48159844 -1.4886451   0.92639893  0.76906884]     r</span></span><br><span class="line"><span class="comment"># packed_input.data.shape : (batch_sum_seq_len X embedding_dim) = (18 X 4)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># packed_input.batch_sizes =&gt; [ 3,  3,  3,  3,  2,  2,  1,  1]</span></span><br><span class="line"><span class="comment"># visualization :</span></span><br><span class="line"><span class="comment"># l  o  n  g  _  s  t  r   #(long_str)</span></span><br><span class="line"><span class="comment"># m  e  d  i  u  m         #(medium)</span></span><br><span class="line"><span class="comment"># t  i  n  y               #(tiny)</span></span><br><span class="line"><span class="comment"># 3  3  3  3  2  2  1  1   (sum = 18 [batch_sum_seq_len])</span></span><br></pre></td></tr></table></figure>
<h3 id="Step-8-Forward-with-LSTM"><a href="#Step-8-Forward-with-LSTM" class="headerlink" title="Step 8: Forward with LSTM"></a>Step 8: Forward with LSTM</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">packed_output, (ht, ct) = lstm(packed_input)</span><br><span class="line"><span class="comment"># packed_output (PackedSequence is NamedTuple with 2 attributes: data and batch_sizes</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># packed_output.data :</span></span><br><span class="line"><span class="comment">#                          [[-0.00947162  0.07743231  0.20343193  0.29611713  0.07992904]   l</span></span><br><span class="line"><span class="comment">#                           [ 0.08596145  0.09205993  0.20892891  0.21788561  0.00624391]   m</span></span><br><span class="line"><span class="comment">#                           [ 0.16861682  0.07807446  0.18812777 -0.01148055 -0.01091915]   t</span></span><br><span class="line"><span class="comment">#                           [ 0.20994528  0.17932937  0.17748171  0.05025435  0.15717036]   o</span></span><br><span class="line"><span class="comment">#                           [ 0.01364102  0.11060348  0.14704391  0.24145307  0.12879576]   e</span></span><br><span class="line"><span class="comment">#                           [ 0.02610307  0.00965587  0.31438383  0.246354    0.08276576]   i</span></span><br><span class="line"><span class="comment">#                           [ 0.09527554  0.14521319  0.1923058  -0.05925677  0.18633027]   n</span></span><br><span class="line"><span class="comment">#                           [ 0.09872741  0.13324396  0.19446367  0.4307988  -0.05149471]   d</span></span><br><span class="line"><span class="comment">#                           [ 0.03895474  0.08449443  0.18839942  0.02205326  0.23149511]   n</span></span><br><span class="line"><span class="comment">#                           [ 0.14620507  0.07822411  0.2849248  -0.22616537  0.15480657]   g</span></span><br><span class="line"><span class="comment">#                           [ 0.00884941  0.05762182  0.30557525  0.373712    0.08834908]   i</span></span><br><span class="line"><span class="comment">#                           [ 0.12460691  0.21189159  0.04823487  0.06384943  0.28563985]   y</span></span><br><span class="line"><span class="comment">#                           [ 0.01368293  0.15872964  0.03759198 -0.13403234  0.23890573]   _</span></span><br><span class="line"><span class="comment">#                           [ 0.00377969  0.05943518  0.2961751   0.35107893  0.15148178]   u</span></span><br><span class="line"><span class="comment">#                           [ 0.00737647  0.17101538  0.28344846  0.18878219  0.20339936]   s</span></span><br><span class="line"><span class="comment">#                           [ 0.0864429   0.11173367  0.3158251   0.37537992  0.11876849]   m</span></span><br><span class="line"><span class="comment">#                           [ 0.17885767  0.12713005  0.28287745  0.05562563  0.10871304]   t</span></span><br><span class="line"><span class="comment">#                           [ 0.09486895  0.12772645  0.34048414  0.25930756  0.12044918]]  r</span></span><br><span class="line"><span class="comment"># packed_output.data.shape : (batch_sum_seq_len X hidden_dim) = (18 X 5)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># packed_output.batch_sizes =&gt; [ 3,  3,  3,  3,  2,  2,  1,  1] (same as packed_input.batch_sizes)</span></span><br><span class="line"><span class="comment"># visualization :</span></span><br><span class="line"><span class="comment"># l  o  n  g  _  s  t  r   #(long_str)</span></span><br><span class="line"><span class="comment"># m  e  d  i  u  m         #(medium)</span></span><br><span class="line"><span class="comment"># t  i  n  y               #(tiny)</span></span><br><span class="line"><span class="comment"># 3  3  3  3  2  2  1  1   (sum = 18 [batch_sum_seq_len])</span></span><br></pre></td></tr></table></figure>
<h3 id="Step-9-Call-unpack-padded-sequences-if-required-or-just-pick-last-hidden-vector"><a href="#Step-9-Call-unpack-padded-sequences-if-required-or-just-pick-last-hidden-vector" class="headerlink" title="Step 9: Call unpack_padded_sequences if required / or just pick last hidden vector"></a>Step 9: Call unpack_padded_sequences if required / or just pick last hidden vector</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># unpack your output if required</span></span><br><span class="line">output, input_sizes = pad_packed_sequence(packed_output, batch_first=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># output =&gt;</span></span><br><span class="line"><span class="comment">#                          [[[-0.00947162  0.07743231  0.20343193  0.29611713  0.07992904]   l</span></span><br><span class="line"><span class="comment">#                            [ 0.20994528  0.17932937  0.17748171  0.05025435  0.15717036]   o</span></span><br><span class="line"><span class="comment">#                            [ 0.09527554  0.14521319  0.1923058  -0.05925677  0.18633027]   n</span></span><br><span class="line"><span class="comment">#                            [ 0.14620507  0.07822411  0.2849248  -0.22616537  0.15480657]   g</span></span><br><span class="line"><span class="comment">#                            [ 0.01368293  0.15872964  0.03759198 -0.13403234  0.23890573]   _</span></span><br><span class="line"><span class="comment">#                            [ 0.00737647  0.17101538  0.28344846  0.18878219  0.20339936]   s</span></span><br><span class="line"><span class="comment">#                            [ 0.17885767  0.12713005  0.28287745  0.05562563  0.10871304]   t</span></span><br><span class="line"><span class="comment">#                            [ 0.09486895  0.12772645  0.34048414  0.25930756  0.12044918]]  r</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#                           [[ 0.08596145  0.09205993  0.20892891  0.21788561  0.00624391]   m</span></span><br><span class="line"><span class="comment">#                            [ 0.01364102  0.11060348  0.14704391  0.24145307  0.12879576]   e</span></span><br><span class="line"><span class="comment">#                            [ 0.09872741  0.13324396  0.19446367  0.4307988  -0.05149471]   d</span></span><br><span class="line"><span class="comment">#                            [ 0.00884941  0.05762182  0.30557525  0.373712    0.08834908]   i</span></span><br><span class="line"><span class="comment">#                            [ 0.00377969  0.05943518  0.2961751   0.35107893  0.15148178]   u</span></span><br><span class="line"><span class="comment">#                            [ 0.0864429   0.11173367  0.3158251   0.37537992  0.11876849]   m</span></span><br><span class="line"><span class="comment">#                            [ 0.          0.          0.          0.          0.        ]   &lt;pad&gt;</span></span><br><span class="line"><span class="comment">#                            [ 0.          0.          0.          0.          0.        ]]  &lt;pad&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#                           [[ 0.16861682  0.07807446  0.18812777 -0.01148055 -0.01091915]   t</span></span><br><span class="line"><span class="comment">#                            [ 0.02610307  0.00965587  0.31438383  0.246354    0.08276576]   i</span></span><br><span class="line"><span class="comment">#                            [ 0.03895474  0.08449443  0.18839942  0.02205326  0.23149511]   n</span></span><br><span class="line"><span class="comment">#                            [ 0.12460691  0.21189159  0.04823487  0.06384943  0.28563985]   y</span></span><br><span class="line"><span class="comment">#                            [ 0.          0.          0.          0.          0.        ]   &lt;pad&gt;</span></span><br><span class="line"><span class="comment">#                            [ 0.          0.          0.          0.          0.        ]   &lt;pad&gt;</span></span><br><span class="line"><span class="comment">#                            [ 0.          0.          0.          0.          0.        ]   &lt;pad&gt;</span></span><br><span class="line"><span class="comment">#                            [ 0.          0.          0.          0.          0.        ]]] &lt;pad&gt;</span></span><br><span class="line"><span class="comment"># output.shape : ( batch_size X max_seq_len X hidden_dim) = (3 X 8 X 5)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Or if you just want the final hidden state?</span></span><br><span class="line">print(ht[<span class="number">-1</span>])</span><br></pre></td></tr></table></figure>
<h3 id="Summary-of-Shape-Transformations"><a href="#Summary-of-Shape-Transformations" class="headerlink" title="Summary of Shape Transformations"></a>Summary of Shape Transformations</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># (batch_size X max_seq_len X embedding_dim) --&gt; Sort by seqlen ---&gt; (batch_size X max_seq_len X embedding_dim)</span></span><br><span class="line"><span class="comment"># (batch_size X max_seq_len X embedding_dim) ---&gt;      Pack     ---&gt; (batch_sum_seq_len X embedding_dim)</span></span><br><span class="line"><span class="comment"># (batch_sum_seq_len X embedding_dim)        ---&gt;      LSTM     ---&gt; (batch_sum_seq_len X hidden_dim)</span></span><br><span class="line"><span class="comment"># (batch_sum_seq_len X hidden_dim)           ---&gt;    UnPack     ---&gt; (batch_size X max_seq_len X hidden_dim)</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch备忘录</category>
      </categories>
      <tags>
        <tag>pack_padded_sequence</tag>
        <tag>pad_packed_sequence</tag>
        <tag>RNN变长序列</tag>
      </tags>
  </entry>
  <entry>
    <title>VS Code的python环境配置</title>
    <url>/2020/01/26/vscode%E7%9A%84python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<blockquote>
<p>我个人习惯把VS Code作为编辑器来写python, 但是使用过程中会遇到一些小小的问题，所以在这里把问题的解决方案记录下来，以便以后查阅，目前还在持续更新中。</p>
</blockquote>
<a id="more"></a>
<h3 id="pylint提示no-member"><a href="#pylint提示no-member" class="headerlink" title="pylint提示no-member"></a>pylint提示no-member</h3><p>比如对于一些第三方库，pytorch或者自己写的库之类的，pylint会经常提示一些函数no-member，我们可以在配置(setting.json)里面加入要忽略错误的第三方库。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">"python.linting.pylintArgs": [</span><br><span class="line"> # List of members which are set dynamically and missed by pylint inference</span><br><span class="line"> # system, and so shouldn't trigger E1101 when accessed. Python regular</span><br><span class="line"> # expressions are accepted.</span><br><span class="line"> # https://github.com/pytorch/pytorch/issues/701</span><br><span class="line"> 	<span class="string">"--generated-members=numpy.* ,torch.* ,cv2.* , cv.*"</span></span><br><span class="line"> ],</span><br></pre></td></tr></table></figure>
<h3 id="使用cmder作为默认终端"><a href="#使用cmder作为默认终端" class="headerlink" title="使用cmder作为默认终端"></a>使用cmder作为默认终端</h3><p>cmder集成了一些linux的指令，比如head,ls,cat,echo等，所以使用起来非常方便，可以很大的提高效率。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"># https://code.visualstudio.com/docs/editor/integrated-terminal#_configuration </span><br><span class="line">"terminal.integrated.shell.windows": "cmd.exe",</span><br><span class="line">"terminal.integrated.shellArgs.windows": ["/K", "D:/cmder/vendor/init.bat"] # 这个填自己电脑里cmder的位置就可</span><br></pre></td></tr></table></figure>
<p>顺便一提，如果你下的cmder不是便携版而是完整版，那它是自带git的，也省得下载git了，vscode配置git可以加上</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">"git.path": "d:/cmder/vendor/git-for-windows/bin/git.exe"</span><br></pre></td></tr></table></figure>
<h3 id="一些不错的插件"><a href="#一些不错的插件" class="headerlink" title="一些不错的插件"></a>一些不错的插件</h3><ul>
<li>名称: <strong>autoDocstring</strong><br>id: njpwerner.autodocstring<br>说明: Generates python docstrings<br>版本: 0.4.0<br>发布者: Nils Werner<br>VS Marketplace 链接: <a href="https://marketplace.visualstudio.com/items?itemName=njpwerner.autodocstring" target="_blank" rel="noopener">https://marketplace.visualstudio.com/items?itemName=njpwerner.autodocstring</a></li>
<li>名称: <strong>Todo Tree</strong><br>id: gruntfuggly.todo-tree<br>说明: Show TODO, FIXME, XXX, etc. comment tags in a tree view<br>版本: 0.0.169<br>发布者: Gruntfuggly<br>VS Marketplace 链接: <a href="https://marketplace.visualstudio.com/items?itemName=Gruntfuggly.todo-tree" target="_blank" rel="noopener">https://marketplace.visualstudio.com/items?itemName=Gruntfuggly.todo-tree</a></li>
<li>名称: Bracket Pair Colorizer<br>id: coenraads.bracket-pair-colorizer<br>说明: A customizable extension for colorizing matching brackets<br>版本: 1.0.61<br>发布者: CoenraadS<br>VS Marketplace 链接: <a href="https://marketplace.visualstudio.com/items?itemName=CoenraadS.bracket-pair-colorizer" target="_blank" rel="noopener">https://marketplace.visualstudio.com/items?itemName=CoenraadS.bracket-pair-colorizer</a></li>
</ul>
]]></content>
      <categories>
        <category>花里胡哨的东西</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title>一道有趣的数学题(三)</title>
    <url>/2020/01/16/%E4%B8%80%E9%81%93%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E5%AD%A6%E9%A2%98(%E4%B8%89)/</url>
    <content><![CDATA[<blockquote>
<p>最近无所事事，决定再写一篇有趣的数学题，然后就开始认真干活。题目的意思很简单，也不是很难，之所以分享这道题是因为这道题不仅题目简短有趣，其涉及到的一些知识也十分有趣，甚至这个结论的应用也十分有趣。当然，本文依然延续我之前啰里啰嗦的习惯，大佬的话选择性的跳过看看就行。</p>
</blockquote>
<p>题目如下：</p>
<script type="math/tex; mode=display">
求证:与{n! \over e}最接近的整数是(n-1)的倍数,其中n \in N^+</script><p>在解决这道题前，我们先对这道题简单的分析一下，首先要明确的就是”最接近”的含义，一个数$a$和某个整数$A$最接近的前提应该是</p>
<script type="math/tex; mode=display">
|a-A| \leq { 1 \over 2}</script><p>但是，要符合题目的条件，那么不可能存在两个整数和他最接近，也就是等号不会成立。因为两个相邻的整数显然不可能都是$n-1$的倍数，$n=1,2$除外，但容易验证这两种情况都不会使等号成立，也就是在本题中这个等号是取不到的。</p>
<p>那么如何下手呢？最自然的想法就是将$e^{-1}n!$写成两个数$A$与$B$的和，其中$A$是其最接近的整数，$B$是其小数部分，然后我们证明$A$是$n-1$的倍数，$B$的绝对值小于$1\over2$即可。那问题的解题方向也明了了，如何找到这两个$A$和$B$呢，注意这里有两个方向，也就是从$e^{-1}$入手或者$n!$入手，但显然从$e^{-1}$入手似乎比较直观，因为他并不是一个有理数。当然我们这里两种入手的解法我们都会给出。</p>
<a id="more"></a>
<h2 id="泰勒展开"><a href="#泰勒展开" class="headerlink" title="泰勒展开"></a><strong>泰勒展开</strong></h2><p>注意到$e^x$在$0$处的泰勒展开</p>
<script type="math/tex; mode=display">
e^x=1+x+{x^2\over2!}+{x^3\over3!}+\cdots+{x^n\over n!}+\cdots</script><p>当我们把$x$代入$-1$，左右同乘$n!$，那么就可以得到</p>
<script type="math/tex; mode=display">
e^{-1}n!=n!(1-1+ {1\over2!}-{1\over3!}+\cdots+{(-1)^n\over n!}+{(-1)^{n+1}\over (n+1)!}+\cdots)</script><p>注意到分母为$n!$的项是整数和小数的分界点，那么我们把这个式子以$n!$为分界，拆成两部分去看，那么就有</p>
<script type="math/tex; mode=display">
\begin{align*}
e^{-1}n!&=n!(1-1+ {1\over2!}-{1\over3!}+\cdots+{(-1)^n\over n!})+n!\left({(-1)^{n+1}\over (n+1)!}+{(-1)^{n+2}\over (n+2)!}+\cdots\right) \\
&=A+B
\end{align*}</script><p>先来讨论$A$，显然当$k&lt;n-1$，$n! \over k!$是整数，而且还是$n-1$的倍数，当$k=n-1$以及当$k=n$时</p>
<script type="math/tex; mode=display">
n!\left({(-1)^{n-1}\over (n-1)!}+{(-1)^n\over n!}\right)=(-1)^{n-1}(n-1)</script><p>显然也是$n-1$的倍数，那么$A$就是($n-1)$的倍数，于是我们解决的重点就放在了$B$上，注意到$B$其实一个交错级数，那么我们不妨稍微改写一下</p>
<script type="math/tex; mode=display">
\begin{align*}
B&=n!\left({(-1)^{n+1}\over (n+1)!}+{(-1)^{n+2}\over (n+2)!}+\cdots\right) \\
&=(-1)^{n+1}({1\over{n+1}}-{1\over{(n+1)(n+2)}}+{1\over{(n+1)(n+2)(n+3)}}-\cdots)
\end{align*}</script><p>那么就有</p>
<script type="math/tex; mode=display">
\begin{align*}
|B|&=|({1\over{n+1}}-{1\over{(n+1)(n+2)}})+({1\over{(n+1)(n+2)(n+3)}}- {1\over (n+1)(n+2)(n+3)(n+4)}) +\cdots | \\
&=({1\over{n+1}}-{1\over{(n+1)(n+2)}})+({1\over{(n+1)(n+2)(n+3)}}- {1\over (n+1)(n+2)(n+3)(n+4)}) +\cdots \\
&={1\over{n+1}}-({1\over{(n+1)(n+2)}}-{1\over{(n+1)(n+2)(n+3)}})-  \cdots \\
&<{1\over{n+1}}  \leq {1\over 2}
\end{align*}</script><p>所以</p>
<script type="math/tex; mode=display">
|e^{-1}n!-A|=|B|<{1\over2}</script><p>也就是$A$是$e^{-1}n!$其最接近的整数，而$A$是$n-1$的倍数我们在之前已经证明，那么证毕。</p>
<p>对于这种解法，其实同样可以考虑使用带拉格朗日余项的泰勒展开，我们将其展开到第$n+1$项，然后也可以得到结论的显然性：</p>
<script type="math/tex; mode=display">
e^{-1}n!=n!(1-1+ {1\over2!}-{1\over3!}+\cdots+{(-1)^n\over n!}+{(-1)^{n+1}e^{\theta}\over (n+1)!}),\theta\in(0,1)</script><p>那么仿照前面的思路，容易发现这个拉格朗日余项乘以$n!$后，当$n\rightarrow \infty$的时候是趋近于$0$的，也就是等式左边无限趋近于$A$，证明题目中的最接近的话吗，只需简单的代入几个$n$去验证再加以阐述即可。</p>
<h2 id="Gamma函数"><a href="#Gamma函数" class="headerlink" title="Gamma函数"></a><strong>Gamma函数</strong></h2><p>从$n!$入手的话要用到Gamma函数($\Gamma$函数)，这个函数相信大家应该不陌生，虽然可能了解的不是很多，但是大家应该至少都在大学数学教材中见过，它的形式如下:</p>
<script type="math/tex; mode=display">
\Gamma(x)= \int_{0}^{\infty}t^{x-1}e^{-t}dt</script><p>其中$x&gt;0$，那么这个函数和本题又有什么关系呢？注意到这个这个函数通过分部积分可以得到如下的递推式</p>
<script type="math/tex; mode=display">
\Gamma(x+1)=x\Gamma(x)</script><p>显然如果$x$是整数的话，那么根据这个递推式我们就可以得到</p>
<script type="math/tex; mode=display">
\Gamma(n+1)=n!\Gamma(1)</script><p>注意到</p>
<script type="math/tex; mode=display">
\Gamma(1)=\int_{0}^{\infty}e^{-t}dt=1</script><p>那么就有$\Gamma(n+1)=n!$，这是一个非常有趣的性质，也许你又学会了$n!$的一种表示方法~，那么对于本题就有</p>
<script type="math/tex; mode=display">
\begin{align*}
e^{-1}n!&=e^{-1}\Gamma(n+1) \\
&=\int_{0}^{\infty}t^ne^{-1-t}dt \\
&=\int_{1}^{\infty}(r-1)^ne^{-r}dr  \tag{令$r=-1-t$}\\
&= \int_{0}^{\infty} - \int_{0}^{1} (r-1)^ne^{-r}dr\\
&=\int_{0}^{\infty}\sum_{k=0}^{n}C_n^{k}r^k(-1)^{n-k}e^{-r}dr - \int_{0}^{1} (r-1)^ne^{-r}dr \tag{二项式定理} \\
&=\sum_{k=0}^{n}C_n^{k}(-1)^{n-k}\Gamma(k+1)-\int_{0}^{1} (r-1)^ne^{-r}dr \\
&=\sum_{k=0}^{n}C_n^{k}(-1)^{n-k}k!-\int_{0}^{1} (r-1)^ne^{-r}dr  ) \\
&=\sum_{k=0}^{n}{n!\over(n-k)! }(-1)^{n-k}-\int_{0}^{1} (r-1)^ne^{-r}dr   \\
&=A+B
\end{align*}</script><p>显然这里的$A$与上一种解法中的$A$是一样的，那么对于$B$呢？注意到</p>
<script type="math/tex; mode=display">
0 \leq |B|=\int_{0}^{1}(1-r)^ne^{-r}dt < \int_0^1 (1-r)^ndr={1\over n+1} \leq {1\over2}</script><p>同时注意到</p>
<script type="math/tex; mode=display">
{d\left(\int_{0}^{1}(1-r)^ne^{-r}\right) \over dn} =\int_{0}^{1}(1-r)^nln(1-r)e^{-r}<0</script><p>那么可知这个积分是单调递减的，那么$|B|&lt;{1\over2}$就十分显然了。至此，本题的两种解法都已经给出，一个从分母的$e$入手，通过无穷级数来探讨，另一种则通过分子的$n!$入手，通过Gamma函数来解决问题，两种解法都非常的精妙。但问题止步于此总有些遗憾，我们注意到根据我们的推导，题目中的式子当$n \rightarrow \infty$时有</p>
<script type="math/tex; mode=display">
\lim_{n \rightarrow \infty}e^{-1}n!=A=n!(1-1+ {1\over2!}-{1\over3!}+\cdots+{(-1)^n\over n!})</script><p>如果对概率论比较了解的同学，应该知道这是个什么东西，是滴，它的名字叫<strong>全错排数​</strong>。</p>
<h2 id="全错排数"><a href="#全错排数" class="headerlink" title="全错排数"></a><strong>全错排数</strong></h2><p>全错排数中最有名的例子，或许就是信封问题了，也许大家也不陌生，就是将$n$封信放入$n$个信封中，问全部装错的情况有多少种。我们这里使用容斥原理给出一种比较直观的解法。也许有的小伙伴对容斥定理还不太了解，那么就在此简单啰嗦两句。实际上从集合的角度去理解容斥原理比较容易，一个典型性的例子就是使用韦恩图(图网上找的)</p>
<div align="center">
    <img width="400" src="/2020/01/16/%E4%B8%80%E9%81%93%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E5%AD%A6%E9%A2%98(%E4%B8%89)/韦恩图.png">
</div>

<p>那么有</p>
<script type="math/tex; mode=display">
|A \cup B \cup C|=|A|+|B|+|C|-|A \cap B|-|B\cap C|-|A\cap C|+|A \cap B \cap C|</script><p>这个从图中很好理解，就是不断地减加，去掉多余的和补充多减的部分即可。如果拓展到$N$的话就是</p>
<script type="math/tex; mode=display">
\begin{align*}
&| A_1 \cup  A_2 \cup A_3 \cup \cdots  A_n| = \sum_{i}^{n}|A_i|-\sum_{i<j}^{n}|A_i \cap A_j|+\sum_{i<j<k}^{n}| A_i \cap A_j \cap A_k| -\cdots+(-1)^n| A_1 \cap  A_2 \cap  A_3 \cap \cdots  A_n|
\end{align*}</script><p>那么对于我们的信封问题，我们记$A_i$为第$i$封信放在第$i$个信封之后的排列数，那么所谓的全错拍数就是</p>
<script type="math/tex; mode=display">
\begin{align*}
D&= |\overline A_1 \cap \overline A_2 \cap \overline A_3 \cap \cdots \overline A_n| \\
&= S-| A_1 \cup  A_2 \cup A_3 \cup \cdots  A_n|  \\
&=A_n^{n}-C_n^{1}A_{n-1}^{n-1}+C_n^{2}A_{n-2}^{n-2}+\cdots+(-1)^{n-1}C_n^{n-1}A_{1}^{1}+(-1)^nC_n^{n} \\
&=n!(1-1+ {1\over2!}-{1\over3!}+\cdots+{(-1)^n\over n!})
\end{align*}</script><p>这个公式的倒数第二行其实也很好理解，因为我们把$A<em>i$看做第$i$封信放在第$i$个信封之后的排列数，那么显然一个放好后其他位置的排列数就是$A</em>{n-1}^{n-1}$，两个放好后其他位置的排列数就是$A_{n-2}^{n-2}$，以此类推，而这些排列数前面的组合数也很好理解，就是我挑选要取交集的的$A_i$的个数，我挑选一个就是$C_n^1$，我挑选两个就是$C_n^2$，以此类推。</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a><strong>应用</strong></h2><p>最后总结升华一下，既然这道题我们已经解决了，而且我们也通过这道题给大家介绍了一下全错排数，那么他们之间的关系又有什么应用呢？</p>
<p>注意到我们通过先前证明可以得到与$e^{-1}n!$最接近的整数就是全错排数，那么反之如果我们现在要去求解全错排数呢？我们可以通过去计算$e^{-1}n!$然后取最接近的整数，去代替求解全错排数！！！也就是说我们有</p>
<script type="math/tex; mode=display">
\lfloor e^{-1}n!+{1\over2} \rfloor=n!(1-1+ {1\over2!}-{1\over3!}+\cdots+{(-1)^n\over n!})</script><p>$\lfloor x\rfloor$表示对$x$取下整。其中补正的$1\over2$是为了让这个整数的活动区间固定在一个整数内。</p>
<p>不学计算机的同学可能不太理解这么做有什么意义，其实这可以大大降低计算的复杂度，因为可以看到的是，等式右侧充斥着阶乘和和加减法，它要计算接近$n$次的阶乘，然后再做$n$次加减法，还有一次乘法，但是呢，等式的左侧只需要计算一次阶乘！再外加一次乘法和一次加法！！！运算量就少了很多！！！</p>
]]></content>
      <categories>
        <category>数学杂谈</category>
      </categories>
      <tags>
        <tag>泰勒展开</tag>
        <tag>Gamma函数</tag>
        <tag>全错排数</tag>
      </tags>
  </entry>
  <entry>
    <title>一道有趣的数学题(二)</title>
    <url>/2019/12/30/%E4%B8%80%E9%81%93%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E5%AD%A6%E9%A2%98(%E4%BA%8C)/</url>
    <content><![CDATA[<blockquote>
<p>最近知乎关注太多数学问题了，总是莫名其妙给我推送很多数学题，所以俺又来分享看到的有趣的题啦~~~</p>
</blockquote>
<p>已知$x_1,x_2,\cdots, x_n$为不全为$0$的实数，那么求如下式子的最大值</p>
<script type="math/tex; mode=display">
{x_1x_2 + x_2x_3 + x_3x_4 + \cdots + x_{n-1}x_n \over x_1^2 + x_2^2 + x_3^2 + \cdots +x_n^2 }</script><a id="more"></a>
<p>这道题很容易陷入一个误区，就是直接使用均值不等式</p>
<script type="math/tex; mode=display">
x_1x_2 \leq {x_1^2 + x_2^2 \over 2}</script><p>那么依此类推是不是就可以得到结果了呢？注意到这么做的话，分子实际上就会变成</p>
<script type="math/tex; mode=display">
{x_1^2 \over 2}+x_2^2 + x_3^2 + \cdots +x_{n-1}^2 + {x_n^2 \over 2}</script><p>它并不会和分母消掉。那么究竟如何来做这道题呢？我这里先给出一个一般解法</p>
<h2 id="一般解法"><a href="#一般解法" class="headerlink" title="一般解法"></a><strong>一般解法</strong></h2><p>我们不妨从$n=2$开始考虑，那么显然</p>
<script type="math/tex; mode=display">
{x_1x_2  \over x_1^2 + x_2^2 } \leq {x_1x_2  \over 2x_1x_2 }={1\over2}</script><p>那么$n=3$呢?</p>
<script type="math/tex; mode=display">
{x_1x_2 + x_2x_3 \over x_1^2 + x_2^2 +x_3^2}</script><p>注意到，这里我们依然可以使用均值不等式，只要对分母一分为二就可以了，目的是为了配出分子的倍数</p>
<script type="math/tex; mode=display">
{x_1x_2 + x_2x_3 \over x_1^2 + x_2^2 +x_3^2}={x_1x_2 + x_2x_3 \over x_1^2 + {x_2^2 \over 2 } +{x_2^2 \over 2 }  +x_3^2} \leq {x_1x_2 + x_2x_3 \over \sqrt 2 x_1x_2 + \sqrt 2x_2x_3}= {\sqrt 2 \over 2}</script><p>注意等号成立的条件是$x_1^2 = {x_2^2 \over 2 } =x_3^2$，那$n=4$呢？</p>
<script type="math/tex; mode=display">
{x_1x_2 + x_2x_3 +x_3x_4 \over x_1^2 + x_2^2 +x_3^2 +x_4^2}</script><p>那么仿照前面的思路，我们知道可以对分母进行拆分，目的是让分母放缩后是分子的倍数，可是当$n=4$的时候似乎拆分方法不是那么直接，但我们知道，拆分项肯定在$x_2和x_3$这两项，为什么呢？因为注意每用一次不等式是两项放缩为一项，那么要放缩为分子这种三项的形式，就需要六项，但分母只有四项，所以还需要两项，我们不妨固定首尾，那么就只需要拆分中间的那些项就可以。既然拆分方法不明了，我们不妨用待定系数的方法</p>
<script type="math/tex; mode=display">
\begin{align*}
&{x_1x_2 + x_2x_3 +x_3x_4 \over (x_1^2 + {\lambda x_2^2}) + ({(1-\lambda) x_2^2} + {\mu x_3^2}) + {((1-\mu)x_3^2} +x_4^2)}\\
& \leq {x_1x_2 + x_2x_3 +x_3x_4 \over 2\sqrt \lambda x_1x_2 +2 \sqrt{(1-\lambda)\mu}x_2x_3 +  2 \sqrt {1-\mu} x_3x_4 }
\end{align*}</script><p>为了让分母这些系数相等，以便和分子消掉，那么就有</p>
<script type="math/tex; mode=display">
\begin{align*}
2\sqrt \lambda = 2 \sqrt{(1-\lambda)\mu}=2\sqrt{1-\mu} \tag{1}
\end{align*}</script><p>那么显然就有</p>
<script type="math/tex; mode=display">
\begin{align*}
&2\sqrt \lambda = 2\sqrt{1-\mu}\\
=>&\lambda = 1-\mu \\
&2 \sqrt{(1-\lambda)\mu}=2\sqrt{1-\mu}\\
=>&\mu = \sqrt {1-\mu} \\ 
=>&\mu = { \sqrt 5-1\over 2}(取\mu在[0,1]范围内的) \\
\end{align*}</script><p>那么对于我们要求的结果就有</p>
<script type="math/tex; mode=display">
{x_1x_2 + x_2x_3 +x_3x_4 \over 2\sqrt \lambda x_1x_2 +2 \sqrt{(1-\lambda)\mu}x_2x_3 +  2 \sqrt {1-\mu} x_3x_4 }  \leq {1\over 2\mu} = { \sqrt 5+1\over 4}</script><p>这样继续做下去显然没有什么问题，但注意到，越往后，我们求解这个方程的复杂度就会越高，也就是这个方程的变量就会越多，$n$项就是$n-2$个变量，求解起来比较繁琐，我自己试了一下，发现里面还是有很多问题的，懒于计算，这里留作待探索内容，放在文末，并给我的一些思考。那么除了这种做法是否还有其它做法呢？当然有！~</p>
<h2 id="二次型的解法"><a href="#二次型的解法" class="headerlink" title="二次型的解法"></a><strong>二次型的解法</strong></h2><p>假设我们令比值为$a$,也就是说</p>
<script type="math/tex; mode=display">
\begin{align*}
{x_1x_2 + x_2x_3 + x_3x_4 + \cdots + x_{n-1}x_n \over x_1^2 + x_2^2 + x_3^2 + \cdots +x_n^2 }= a 
\end{align*}</script><p>我们要求的是$a$的最大值，使得对任意的不全为$0$的$x_1,\cdots,x_n $都能满足</p>
<script type="math/tex; mode=display">
\\
x_1^2 + x_2^2 + x_3^2 + \cdots +x_n^2 - {1\over a}(x_1x_2+x_2x_3+\cdots+x_{n-1}x_n)\geq 0</script><p>注意到这其实是个二次型问题，我们令</p>
<script type="math/tex; mode=display">
f(x)=x_1^2 + x_2^2 + x_3^2 + \cdots +x_n^2 - {1\over a}(x_1x_2+x_2x_3+\cdots+x_{n-1}x_n)</script><p>二次型可以表示为$x^TAx$，将$f(x)$表示为这种形式</p>
<script type="math/tex; mode=display">
f(x)=
\begin{bmatrix}
x_1 & \cdots & x_n
\end{bmatrix}
\begin{bmatrix} 1 & -{ 1 \over2a}  \\
-{1 \over 2a}  & 1 & -{1 \over 2a}  \\
& -{1 \over 2a} & \ddots & \ddots \\
& & \ddots & \ddots & -{1 \over 2a} \\
& & & -{1 \over 2a} & 1
\end{bmatrix}_{ n \times n}
\begin{bmatrix}
x_1 \\
\vdots \\
x_n
\end{bmatrix}</script><p>那么对于任意的$x$这个$f(x)\geq0$，也就是说矩阵$A$要是半正定的(这就是半正定的定义)。而半正定矩阵的充要条件是有$n$个非负特征值，那么问题就转化为了，什么样的正数$a$可以使得$A$的特征值非负。</p>
<p>那么问题来了，这个矩阵$A$的特征值如何求呢？按照一般的求法那么有</p>
<script type="math/tex; mode=display">
|A - \lambda E| = \begin{bmatrix}
1 - \lambda & - { 1 \over 2a}  \\ 
{-} {1 \over 2a}  & 1 - \lambda & - { 1 \over 2a}  \\ 
& - {1 \over 2a} & \ddots & \ddots \\ 
& & \ddots & \ddots & -{1 \over 2a} \\ 
& & & -{1 \over 2a} & 1- \lambda
\end{bmatrix} _ {n \times n}</script><p>可是这个行列式也不好求啊！！！没关系，我们记其为$D_n$，注意到这个行列式的值是可以写成递推式的，如果我们按第一行展开，那么就有</p>
<script type="math/tex; mode=display">
D_n=(1-\lambda)D_{n-1}-(-{1\over2a})(-{1\over2a})D_{n-2}=(1-\lambda)D_{n-1}-{1\over4a^2}D_{n-2}</script><p>那么可以发现$D_n$应当是可以展开为关于$(1-\lambda)$的表达式。诶，这句话怎么似曾相识？没错，这就是切比雪夫多项式啊！！！我在GCN的必备知识中有提到过，详情见<a href="http://tsotfsk.top/2019/12/17/GCN要用到的数学知识系列(四">切比雪夫多项式</a>/)。如果你能想到这里，那我觉得你已经在这个问题上前进了一大步~，那我们现在知道这个和切比雪夫多项式有一样的结构了，那接下来呢？接下来我该如何去求解$\lambda$的值呢？</p>
<script type="math/tex; mode=display">
T_n(x)=2xT_{n-1}(t)-T_{n-2}(x)</script><p>那么显然这里可以让$2x=(1-\lambda)$，但是$D_{n-2}$前面的系数不为$1$诶，No Problem！！​注意到矩阵的特征值可以做线性变换，也就是说我们可以取$A’=-2aA$，那么$\lambda’=-2a\lambda$</p>
<p>那么我们可以求$A’$的特征值，显然可以得到$A’$对应的$D’$，那么就有</p>
<script type="math/tex; mode=display">
{ D' _ n = ( - 2 a { - } \lambda' ) D'  _ { n - 1 } - D'  _ { n - 2 } }</script><p>那么我们令$2x = - 2a - \lambda’$，同时有$D’(0)=1,D’(1)=2x$，那么这里就是二型切比雪夫多项式，注意到其通解为</p>
<script type="math/tex; mode=display">
D'_n(x) = { sin((n+1) cos^{-1}(x)) \over sin(cos^{ -1 }x)}</script><p>显然可知$D’(x)$对应的$n$个零点分别是</p>
<script type="math/tex; mode=display">
x=cos({k \pi \over n+1}) \space \space \space \space \space \space k=1 , 2, \cdots, n</script><p>那么我们所求的$A$的$\lambda$就有</p>
<script type="math/tex; mode=display">
\lambda_k=-{1\over2a}(-2a-2cos({k \pi \over n+1}))=1+{1\over a}cos({k \pi \over n+1})</script><p>$a&gt;0$，那么显然</p>
<script type="math/tex; mode=display">
min(\lambda_k)=\lambda_n=1+{1\over a}cos({n \pi \over n+1})=1-{1\over a}cos({\pi \over n+1}) \geq 0</script><p>那么可以知道</p>
<script type="math/tex; mode=display">
a \leq cos({\pi \over n+1})</script><p>那么$a$的最大值也就是原式的最大值就是$cos({\pi \over n+1})$，证毕，撒花。</p>
<h2 id="待探索部分"><a href="#待探索部分" class="headerlink" title="待探索部分"></a><strong>待探索部分</strong></h2><p>注意到$(1)$式中出现了许多诸如$\sqrt \lambda$，$\sqrt {1- \lambda}$，$\sqrt \mu$，$\sqrt {1-\mu}$的项，而且注意到$\lambda$，$\mu\in[0,1]$，那么聪明的小伙伴肯定想到了！！！三角换元。如果我们令$\lambda=sin^2 \alpha, \mu=sin^2 \beta $，那么$(1)$式就变成了</p>
<script type="math/tex; mode=display">
2sin \alpha = 2cos\alpha sin\beta = 2cos\beta</script><p>这样我们就消掉了根号，可是接下来呢？我们要求的其中任意一项。注意到$sin\alpha=cos\beta$那么就有$cos \alpha = sin \beta$</p>
<p>也就是$2sin^2 \beta=2cos\beta$，也就是说$1-2sin^2 \beta=1-2cos\beta$，也就是$cos2\beta=1-2cos\beta$</p>
<p>到这里似乎也没思路呢，那么我们不妨先就此打住，转而去看看看$n=5$的情况，</p>
<script type="math/tex; mode=display">
\begin{align*}
2\sqrt \lambda = 2 \sqrt{(1-\lambda)\mu}=2 \sqrt{(1-\mu)\gamma}=2 \sqrt{1-\gamma} \tag{2}
\end{align*}</script><p>这个式子可以得到</p>
<script type="math/tex; mode=display">
2sin \alpha = 2cos\alpha sin\beta = 2cos\beta sin\nu=2cos\nu \tag{3}</script><p>采用相同的方式去代换，消掉$\alpha$和$\beta$，$\cdots$</p>
]]></content>
      <categories>
        <category>数学杂谈</category>
      </categories>
      <tags>
        <tag>切比雪夫多项式</tag>
        <tag>二次型</tag>
        <tag>半正定矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM与GRU详解</title>
    <url>/2019/12/24/LSTM%E4%B8%8EGRU%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<blockquote>
<p>虽然标题写的是详解，但是依然不是新手入门向，尤其最近有DDL要赶，所以我只是结合自己的理解把我觉得有些难理解的稍微总结一下，未来有机会再来写详解。</p>
</blockquote>
<a id="more"></a>
<h2 id="LSTM-Long-Short-Term-Memory"><a href="#LSTM-Long-Short-Term-Memory" class="headerlink" title="LSTM(Long Short Term Memory)"></a>LSTM(Long Short Term Memory)</h2><p>LSTM的出现是解决RNN的长期依赖导致的梯度爆炸/消失的问题，那么梯度爆炸和梯度消失为什么会出现呢？</p>
<p><img src="/2019/12/24/LSTM%E4%B8%8EGRU%E8%AF%A6%E8%A7%A3/RNN序列图.png" alt></p>
<p>如上图，这是一个典型的RNN展开若干步的结构(图片取自<a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">cs231n</a> 第十节的课件)，在反向传播过程中，我们最关心的$dW$前的系数会出现多个$W$，如果这个序列很长，那么出现$W^N$，假设这个矩阵可以做特征值分解，那么显然</p>
<script type="math/tex; mode=display">
W^N=U^{-1}\Sigma^{N}U</script><ul>
<li><p><strong>当最大的特征值大于1的时候，就容易引起梯度爆炸</strong></p>
<p>处理梯度爆炸的方式很简单，可以考虑梯度裁切。设置一个阈值$T$，比如当$||dW||\ge T$那么就让$dW={T\over||W||} \times dW$</p>
</li>
<li><p><strong>当最大的特征值小于1的时候，就容易导致梯度消失</strong></p>
<p>这个不太好解决，LSTM的出现正是为了缓解这种问题，注意到越靠前的部分$W$乘的次数越多，此时$dW$就很少，如果序列很长，那么很容易忽略开始的信息，也就是无法长期依赖的问题。</p>
</li>
</ul>
<p>LSTM的计算过程如下</p>
<p><img src="/2019/12/24/LSTM%E4%B8%8EGRU%E8%AF%A6%E8%A7%A3/LSTM计算.png" alt></p>
<p>LSTM的这种传播方式可以解决一些长期依赖的问题，它相比RNN的特点是除了隐态$h(t)$还引入了$c(t)$，我们称之为细胞状态。而且内部结构更加复杂。有小伙伴可能要问了，$c(t)$相比于$h(t)$的区别在哪呢？为什么要引入这么个状态呢？我们从其结构图讲起</p>
<p><img src="/2019/12/24/LSTM%E4%B8%8EGRU%E8%AF%A6%E8%A7%A3/LSTM结构.png" alt></p>
<p>注意$h(t)$和$c(t)$的关系，$o$是经过$\sigma$激活函数后得到的，属于$(0,1)$范围内。而$c(t)$要过一个$tanh$激活函数是包含正负，可以更好的做梯度传播，那么显然$h(t)=o\bigodot tanh(c_t)$，就是从细胞状态$c(t)$中选择一部分出来用作展示，也可以说$c(t)$相比$h(t)$包含更多的信息，同时注意到$c(t)$并不包含复杂的运算，只有一个式子用来计算它，它主要用作综合当前细胞状态。除此之外，它还有一个很明显的特点，也正是LSTM用它的原因，见下图</p>
<p><img src="/2019/12/24/LSTM%E4%B8%8EGRU%E8%AF%A6%E8%A7%A3/高速公路.png" alt></p>
<p>红线表示梯度反向传播的路线，在这条路上，只有矩阵的数乘以及相加，所以其反向传播很快，那么它是如何缓解梯度消失的问题呢？</p>
<blockquote>
<p>占坑，有空来填，因为要写反向传播的公式，我现在脑壳疼，所谓缓解，当然是体现在反向传播的数学公式上。但这个结构太复杂，所以反向传播的公式也很长，有空来补公式。</p>
</blockquote>
<p>注意这里是缓解，但是并不能避免哦，实践还是会常常将$W$的参数初始化靠近$1$，这样可以缓解梯度消失。</p>
<p>这里给出一个LSTM一个单元内的传播情况，首先解释下图中的四个门的符号简写</p>
<ul>
<li>f: Forget gate:Whether to erase cell</li>
<li>i: Input gate, whether to write to cell</li>
<li>g: Gate gate (?), How much to write to cell</li>
<li>o: Output gate, How much to reveal cell</li>
</ul>
<blockquote>
<p>实际上传播的过程有空再更新，先在这继续挖个坑，因为现在实在没时间写。</p>
</blockquote>
<h2 id="GRU-Gated-Recurrent-Unit"><a href="#GRU-Gated-Recurrent-Unit" class="headerlink" title="GRU(Gated Recurrent Unit)"></a>GRU(Gated Recurrent Unit)</h2><p>GRU的相比LSTM的话，结构简单，但是大多数应用中的效果是差不多的，简而言之就是<strong>GRU以更低的计算成本取得了和LSTM差不多的效果</strong>。</p>
<p><img src="/2019/12/24/LSTM%E4%B8%8EGRU%E8%AF%A6%E8%A7%A3/GRU.png" alt></p>
<p>那么他简化了哪里呢？</p>
<ul>
<li><strong>两个状态合为一个状态。</strong>注意到LSTM中式子中，$h(t)$不过是$c(t)$筛选的一部分，那么GRU索性把$h(t)$去掉了，这里的$h(t)$实际上相当于LSTM中的$c(t)$。</li>
<li><strong>四个门变为两个门。</strong>注意到这里$r$门相当于是$f$门，选择性的遗忘和保留一些之前的信息，然后就可以为接下来的计算提供信息。$z$门作为更新门，决定更新多少$h(t)$的信息，那么$(1-z)$不就表示保留多少更新了嘛。</li>
</ul>
<p>显然这种结构更加简单，同样也保有梯度的高速公路，运算也相对简单。</p>
]]></content>
      <categories>
        <category>经典模型详解系列</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>GRU</tag>
      </tags>
  </entry>
  <entry>
    <title>一道有趣的数学题</title>
    <url>/2019/12/18/%E4%B8%80%E9%81%93%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E5%AD%A6%E9%A2%98/</url>
    <content><![CDATA[<p>前段时间看到了一道十分有趣的数学题，如下</p>
<script type="math/tex; mode=display">
\begin{align*}
&x+y+z=1 \\
&x^2+y^2+z^2=2 \\
&x^3+y^3+z^3=3 \\
&\cdots \\
&x^5+y^5+z^5=?
\end{align*}</script><p>这规律看着挺明显哈，那$x^5+y^5+z^5$肯定等于$5$啊，这题小学僧都随便做~那很不好意思哦，这道题的答案其实是$6$(笑。</p>
<a id="more"></a>
<p>我们先从一个最简单的做法做起，显然$x,y,z$是如下方程的解</p>
<script type="math/tex; mode=display">
\begin{align*}
&(r-x)(r-y)(r-z)=0 \\
=>&r^3-(x+y+z)r^2+(xy+yz+xz)r-xyz=0  \tag{1}\\
\end{align*}</script><ul>
<li>$x+y+z=1$</li>
<li><p>$(x+y+z)^2=x^2+y^2+z^2 +2(xy+yz+xz) =&gt; xy+yz+xz={1-2 \over 2} = -{1 \over 2}$</p>
</li>
<li><p>$x^3+y^3+z^3-3xyz=(x+y+z)(x^2+y^2+z^2-(xy+yz+xz))=&gt;xyz={3-1 \times (2+{1\over2}) \over 3}={1\over6}$</p>
</li>
</ul>
<p>将上述三个式子代入$(1)$式有</p>
<script type="math/tex; mode=display">
r^3-r^2-{1\over2}r-{1\over6}=0 \tag{2}</script><p>这里$r=x,y,z$的时候等式成立，那么对$(2)$式，我们左乘个$r$就有</p>
<script type="math/tex; mode=display">
r^4-r^3-{1\over2}r^2-{1\over6}r=0</script><p>然后分别将$x,y,z$代入可以得到</p>
<script type="math/tex; mode=display">
\begin{cases}
x^4-x^3-{1\over2}x^2-{1\over6}x=0 \\
y^4-y^3-{1\over2}y^2-{1\over6}y=0 \\
z^4-z^3-{1\over2}z^2-{1\over6}z=0
\end{cases}</script><p>三个式子相加，那么有</p>
<script type="math/tex; mode=display">
(x^4+y^4+z^4)-(x^3+y^3+z^3)-{1\over2}(x^2+y^2+z^2)-{1\over6}(x+y+z)=0</script><p>那么可以得到</p>
<script type="math/tex; mode=display">
x^4+y^4+z^4=3+{1\over2}\times2+{1\over6}={25\over6}</script><p>同理我们对$(2)$式左右同乘$r^2$，那么就有</p>
<script type="math/tex; mode=display">
r^5-r^4-{1\over2}r^3-{1\over6}r^2=0</script><p>分别代入$x,y,z$相加，就有</p>
<script type="math/tex; mode=display">
(x^5+y^5+z^5)-(x^4+y^4+z^4)-{1\over2}(x^3+y^3+z^3)-{1\over6}(x^2+y^2+z^2)=0</script><p>那么就有</p>
<script type="math/tex; mode=display">
(x^5+y^5+z^5)={25\over6} + {1\over 2} \times3+{1\over6}\times2=6</script><p>这题到这里应该就结束了，但是我们要善于思考嘛，那$x^n+y^n+z^n$呢？注意到$(2)$式隐含的信息，假如我们令数列$a_n=x^n+y^n+z^n$，那么显然有$a_1=1,a_2=2,a_3=3$，此外根据刚才的运算，我们知道</p>
<script type="math/tex; mode=display">
a_{n}-a_{n-1}-{1\over2}a_{n-2}-{1\over6}a_{n-3}=0</script><p>那么依照矩阵求解递推式的原理，我们容易写出如下等式组</p>
<script type="math/tex; mode=display">
\begin{cases}
a_{n}=a_{n-1}+{1\over2}a_{n-2}+{1\over6}a_{n-3} \\
a_{n-1}=a_{n-1} \\
a_{n-2}=a_{n-2}
\end{cases}</script><p>将其写成矩阵的形式，那么有</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
a_n \\
a_{n-1} \\
a_{n-2} 
\end{bmatrix} = 
\begin{bmatrix}
1 &{1\over2} &{1\over6} \\
1 &0 &0 \\
0 &1 &0 
\end{bmatrix}
\begin{bmatrix}
a_{n-1} \\
a_{n-2} \\
a_{n-3} 
\end{bmatrix}</script><p>那么将这个式子不断迭代，我们知道</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
a_n \\
a_{n-1} \\
a_{n-2} 
\end{bmatrix} = 
\begin{bmatrix}
1 &{1\over2} &{1\over6} \\
1 &0 &0 \\
0 &1 &0 
\end{bmatrix}^{n-3}
\begin{bmatrix}
3 \\
2 \\
1
\end{bmatrix}</script><p>我们令</p>
<script type="math/tex; mode=display">
A=\begin{bmatrix}
1 &{1\over2} &{1\over6} \\
1 &0 &0 \\
0 &1 &0 
\end{bmatrix}</script><p>那么问题就转化成了求$A^n$的问题，那么我们知道矩阵的$n$次幂一般可以通过矩阵的特征值分解去求，但很可惜，这个矩阵并没有三个特征值，实际上在实数域上只有一个特征值，此外三次方程的求解比较麻烦，特征方程如下</p>
<script type="math/tex; mode=display">
\lambda^3-\lambda^2-{1\over2}\lambda-{1\over6}=0 \tag{3}</script><p>我这里就借助python的numpy库给出一个近似解</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">1</span>/<span class="number">2</span>, <span class="number">1</span>/<span class="number">6</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">eig = np.linalg.eigvals(a)</span><br></pre></td></tr></table></figure>
<p>输出的结果，整理后如下</p>
<script type="math/tex; mode=display">
\begin{align*}
&\lambda_1 = 1.43084957+0.j \\
&\lambda_2 = -0.21542478+0.2647132j\\
&\lambda_3 = -0.21542478-0.2647132j
\end{align*}</script><p>正常的话，再依据这个矩阵求出特征向量之类，然后将$A$换成$U\Sigma U^T$，那么这题基本就算完事。可针对这道题呢？这三个特征值其实对应的就是$x,y,z$的解啊！！！因为方程$3$不就是方程$(2)$嘛？所以我们兜了个大圈子，最后发现还是直接解三次方程好一点。那么结果就有了</p>
<script type="math/tex; mode=display">
x^n+y^n+z^n=1.43084957^n+(-0.21542478+0.2647132j)^n+(-0.21542478-0.2647132j)^n</script><p>有人可能要质疑了，你这个解不行啊，不够math啊。说得好！！！我李某元也是这么认为的。那我就给你一组比较math的$x，y，z$的解，其实python的一个库sympy可以用来做符号计算，要求解我们的方程组，只需要下面几条代码就可以了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sympy</span><br><span class="line">x = sympy.Symbol(<span class="string">'x'</span>)</span><br><span class="line">y = sympy.Symbol(<span class="string">'y'</span>)</span><br><span class="line">z = sympy.Symbol(<span class="string">'z'</span>)</span><br><span class="line">result = sympy.solve([x + y + z - <span class="number">1</span> , x**<span class="number">2</span> + y**<span class="number">2</span> + z**<span class="number">2</span> - <span class="number">2</span>, x**<span class="number">3</span> + y**<span class="number">3</span>+ z**<span class="number">3</span> - <span class="number">3</span>], [x, y, z])</span><br></pre></td></tr></table></figure>
<p>输出的结果我给你整理一下哈，算了，懒得整理成latex的格式了，看个大概吧，嘻嘻$\cdots$</p>
<p><img src="/2019/12/18/%E4%B8%80%E9%81%93%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E5%AD%A6%E9%A2%98/result.png" alt></p>
<p>图中的$result[0],result[1],result[2]$分别可以对应$x，y，z$的一组解，注意这个方程其实是有$6$组解的，我们这里只给出三个让大家瞅瞅啥样哈。不知道这个结果是不是你比较喜欢，反正我不喜欢…</p>
<p>好了，那么大功告成！！！！当然这道题也有更一般的解法，但是用到了群论、数论之类的，其实就是一些很基础的知识，只不过讲的高大上了点，和我的做法本质是一样的，这里不过多引入，有兴趣的可以看知乎上给出的<a href="https://zhuanlan.zhihu.com/p/96063757" target="_blank" rel="noopener">高大上的解法</a>。</p>
]]></content>
      <categories>
        <category>数学杂谈</category>
      </categories>
      <tags>
        <tag>群论</tag>
        <tag>多项式</tag>
        <tag>矩阵求解递推式</tag>
      </tags>
  </entry>
  <entry>
    <title>GCN要用到的数学知识系列(四)</title>
    <url>/2019/12/17/GCN%E8%A6%81%E7%94%A8%E5%88%B0%E7%9A%84%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E5%9B%9B)/</url>
    <content><![CDATA[<h2 id="四、切比雪夫多项式"><a href="#四、切比雪夫多项式" class="headerlink" title="四、切比雪夫多项式"></a><strong>四、切比雪夫多项式</strong></h2><blockquote>
<p>切比雪夫多项式(Chebyshev polynomials)其实有很多比较不错的性质，其证明的方法也有很多，而且都很漂亮。因为GCN只是用了第一类切比雪夫多项式，以使得在计算上进一步简化，所以这里只是介绍一下第一类切比雪夫多项式，第二类的话可以用相同的方式理解，不再赘述。文章的最后也提到了一些GCN用到的相关知识。不过个人精力有限，其实还有很多有意思的性质和应用可以谈，但本文就点到为止吧。</p>
</blockquote>
<a id="more"></a>
<h3 id="三角函数的多倍角公式"><a href="#三角函数的多倍角公式" class="headerlink" title="三角函数的多倍角公式"></a><strong>三角函数的多倍角公式</strong></h3><p>高中学习三角函数中提到的最基本的二倍角公式相信大家都烂熟于心了，但是我这里还是给出(笑</p>
<script type="math/tex; mode=display">
cos2x=2cos^2x-1</script><p>不知道大家是否有时也会好奇，既然二倍角是这样呢，那三倍角如何化成关于$cosx$的多项式呢？高中知识的话，我们可以通过将$cos3x$换成$x+2x$然后用两角和公式，然后对$cosx$和$sinx$用两角和(二倍角)公式，就可以得到了$cos3x$关于$cosx$的多项式了。推导如下</p>
<script type="math/tex; mode=display">
cos3x = cos2xcosx-sin2xsinx=(2cos^2x-1)cosx-2cosx(1-cos^2x)=4cos^3x-3cosx</script><p>那么有的小伙伴自然会有新的疑问，$cosnx$呢?对于任意的$n$，$cosnx$是否都能写成关于$cosx$的多项式呢？实际上我们根据两角和公式，$cosnx$应该也是可以通过这种方式<strong>迭代</strong>得到的，我们接下来会去验证。那既然是迭代，我们是否可以找到关于一个$cosnx$的通项公式呢？我们进一步思考，如果要找通项公式，那我们就要列出这数列中几项之间的关系，我们下面尝试做一下，记$cosnx$关于$cosx$的多项式为$T_n(t)$，其中$t$相当于$cosx$那么显然有首项为$T_0(t)=1,T_1(t)=t$</p>
<script type="math/tex; mode=display">
\begin{align*}
T_n(t) = & cosnt \\
=& cos(n-1)t \space cost-sin(n-1)t\space sint \\
=&cos(n-1)tcost-sint(sintcos(n-2)t-cost(t)sin(n-2)t) \\
=&cos(n-1)tcost-(1-cos^2t)cos(n-2)t-costsintsin(n-2)t \\
=&cos(n-1)tcost-cos(n-2)t+cost(costcos(n-2)t-sintsin(n-2)t) \\
=&cos(n-1)tcost-cos(n-2)t+costcos(n-1)t \\
=&2cos(n-1)tcost-cos(n-2)t \\
=&2tT_{n-1}(t)-T_{n-2}(t)
\end{align*}</script><p>其实这个递推公式便是切比雪夫多项式的由来。$Perfect!~$那么问题来了，我们如何求解这个递推式子呢？如果对线性代数比较了解的同学也许觉得很easy，比如可以用矩阵迭代的方式求解，然后对于$n$次幂的矩阵使用一些矩阵的加和以及特征值分解等技巧就可以得到通解，这里我们不采用这种方案，但留在脚注中以供参考<sup><a href="#fn_1" id="reffn_1">1</a></sup>。其实除了写成递推式的形式，我们还能在$T_n(t)$本身上做文章，比如将其化为微分方程的解的形式，注意到</p>
<script type="math/tex; mode=display">
\begin{align*}
& T_n^{'}(t) = { {d \space cosnx \over dx} \over {cosx \over dx}}= -{ nsinnx\over sinx} \\ 
& T_n^{''}(t)={ {d \space T_n^{'}(t) \over dx} \over {cosx \over dx}}=-{n^2sinxcosnx+ncosxsinnx \over sin^3x} = -{n^2cosnx - cosx \times -n {sinnx \over sinx} \over sin^2x} 
\end{align*}</script><p>那么观察到$T_n^{‘’}(t)$式子的右侧可以用$T_n^{‘}(t)$和$T_n^{‘’}(t)$以及$t$来表示，也就是说</p>
<script type="math/tex; mode=display">
T_n(t)=-{n^2T_n(t)- tT_n^{'}(t) \over {1-t^2}}</script><p>整理可得</p>
<script type="math/tex; mode=display">
(1-t^2)T_n(t)- tT_n^{'}(t)+n^2T_n(t)=0</script><p>那么我们如今换一个角度来看，这就变成了一个微分方程，不过这个微分方程不是常系数的，似乎也很难于求解，这里我们给出一种解法(不是最简单的，但是结果比较直观)。基于对$T_n(t)$的递推公式的思考我们可以知道，<strong>它可以展开成关于$t$的幂函数！！！</strong>那实际上我们就有了这么一个方案去选择，也就是假设</p>
<script type="math/tex; mode=display">
T_n(t)=\sum_{m=0}^{\infty}a_mt^m</script><p>那么代入微分方程，然后对应次幂前的系数相等，我们可以得到</p>
<script type="math/tex; mode=display">
a_{m+2}= {(m-n)(m+n) \over (m+1)(m+2)}a_m</script><p>那么我们通过对这个式子不断迭代就可以得出关于$a<em>m$的表达式。不过这个递推式由于间隔了一个$a</em>{m+1}$所以奇数项和偶数项要分开讨论。在此，我们给出其解的两个标准选择：</p>
<ul>
<li>$a_0=1,a_1=0$，那么有</li>
</ul>
<script type="math/tex; mode=display">
F(t)=1- {n^2 \over 2!}t^2 + {（n-2)n^2(n+2) \over 4!}t^4 - {(n-4)(n-2)n^2(n+2)(n+4) \over 6!}t^6 + \cdots</script><ul>
<li>$a_0=0,a_1=1$，那么有</li>
</ul>
<script type="math/tex; mode=display">
G(t)=x- {(n-1)(n+1) \over 3!}t^3  + {(n-3)(n-1)(n+1)(n+3) \over 5!}t^5 + \cdots</script><p>那么易知，<strong>更一般的解就是这两个函数的线性组合</strong>。对于我们这里要求解的$cosnx$来说，只需要通过代入几个$n$的展开，然后求解线性方程组，就可以得到关于对应的$F(x)$和$G(x)$的系数，这里直接给出结果。</p>
<ul>
<li>当n为偶数时，那么有</li>
</ul>
<script type="math/tex; mode=display">
T_n(t)=(-1)^{n \over 2}F(t)</script><ul>
<li>当n为奇数时，那么有</li>
</ul>
<script type="math/tex; mode=display">
T_n(t)=(-1)^{(n-1) \over 2}nG(t)</script><p>至此，我们通过求$cosnx$的方式首先引入切比雪夫多项式的递推公式，进而又引入了切比雪夫微分方程，并进一步求解了切比雪夫多项式，但注意到我们这里用的都是$t=cosx$，也就是限制了$t$的范围为$[-1,1]$，实际上，我们在推导过程中只是用了类比的思路，这里的$t$可以是任何值。</p>
<p>那么有小伙伴又要问了，你扯了这么多，这多项式有什么性质呢？GCN为啥要用它啊？</p>
<h3 id="切比雪夫多项式的性质"><a href="#切比雪夫多项式的性质" class="headerlink" title="切比雪夫多项式的性质"></a><strong>切比雪夫多项式的性质</strong></h3><ul>
<li><p><strong>正交性</strong></p>
<p>看过我前面讲述傅里叶变换的小伙伴一定对这个性质不陌生，正交性是一个非常好的性质，因为无穷多个正交函数可以做为函数空间的一组基，其实这里有正交完备性问题，但不过多展开。其实如果从$cosnx$的角度来看，我们很容易知道$T_n(t)=cosnx$，其任意两个$n$的取值所对应的的函数正交。更一般的来讲，如果是任意的$t$，那么这里是加权正交的，此处不再赘述。不理解函数正交的可以去看我<a href="http://tsotfsk.top/2019/12/14/GCN要用到的数学知识系列(一">前面的文章</a>/)。</p>
</li>
<li><p><strong>奇偶性</strong></p>
<p>容易得到当$n$为偶数的时候$T_n(t)=(-1)^{n \over 2}F(t)$，而$F(t)$是一个偶函数，那么显然$T_n(t)$也是一个偶函数，同理当$n$为奇数的时候$T_n(t)=(-1)^{(n-1) \over 2}nG(t)$，而$G(t)$是一个奇函数，那么显然，此时$T_n(t)$也是一个奇函数。</p>
</li>
<li><p><strong>$T_n(t)$最高次幂前系数为$2^{n-1}$</strong></p>
<p>这个由它的递推式$T<em>n(t)=2tT</em>{n-1}(t)-T_{n-2}(t)$很容易得到，脚注中的的矩阵乘法更直观一些。</p>
</li>
</ul>
<p>其余的还有一些，比如最小0逼近之类的有趣的性质，我们不在此赘述。</p>
<h3 id="切比雪夫多项式逼近"><a href="#切比雪夫多项式逼近" class="headerlink" title="切比雪夫多项式逼近"></a><strong>切比雪夫多项式逼近</strong></h3><p>切比雪夫近似是利用将函数展开为由切比雪夫多项式组成的各项，依需要的逼近程度决定展开的项次，可以得到很接近多项式的结果。此作法类似进行函数的傅立叶变换，只是用切比雪夫多项式代替分析中用到的三角函数，性质中的正交性我们也可以知道其与傅里叶变换的类似性。</p>
<p>注意到在切比雪夫多项式中，我们用到了</p>
<script type="math/tex; mode=display">
a_{m+2}= {(m-n)(m+n) \over (m+1)(m+2)}a_m</script><p>那么根据比值判别法，当$|t|&lt;1$的时候，这个幂级数是收敛的，也就是说如果我们对于一个定义在$[-1,1]$的函数利用切比雪夫多项式去近似，如果我们最高只用到了$a<em>mt^m$，也就是在$m$处截断，那么其误差其会接近$a</em>{m+1}t^{m+1}$的整数倍。那么其误差应该是很小的。</p>
<p>那么GCN为什么要用到这个呢？注意到GCN的一条核心表达式</p>
<script type="math/tex; mode=display">
g_\theta *x=Ug_{\theta}U^Tx</script><p>其中$U$是对称归一化的拉普拉斯矩阵特征向量组成的，当矩阵特别大时，求一个矩阵的特征值和特征向量的开销是很大的，而我们知道$g<em>{\theta}$是关于特征值$\lambda$的函数，另外注意到$L=U{\Lambda}U^T$，$L^n=U{\Lambda}^nU^T$，其中$\Lambda$是特征值组成的对角矩阵，那我们就可以通过对$g</em>{\theta}$做切比雪夫多项式的逼近，并在$K$处截断，以简化计算。最后作用的效果就是</p>
<script type="math/tex; mode=display">
g_{\theta'}(\Lambda) \approx \sum_{k=0}^{K}\theta_{k}^{'}T_k(\tilde{\Lambda})</script><p>因为切比雪夫展开的幂级数在$|t|&lt;1$收敛，我们要把特征值$\lambda$放缩到这个范围，论文中采用的是$\tilde{\Lambda}={2{\Lambda} \over \lambda_{max}}-I$，很好理解，注意到对称归一化的拉普拉斯矩阵的半正定性，其特征值都是大于等于$0$的，那么显然放缩后的特征值都在$[-1,1]$内，那么</p>
<script type="math/tex; mode=display">
g_{\theta'} *x \approx \sum_{k=0}^{K}\theta_{k}^{'}T_k(\tilde{L})x</script><p>这里$\tilde{L}={2{L} \over \lambda<em>{max}}-I$，注意这里是一个线性变换，只改变了特征值。那么这里就省去了求特征值和特征向量的过程，GCN论文中在$K=1$处截断，只保留了$\theta</em>{0}^{‘},\theta<em>{1}^{‘}$，并假设了$\lambda</em>{max} \approx 2$。后续的话都比较好理解了。</p>
<h3 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a><strong>结尾</strong></h3><p>系列文章完结撒花~~第一次写博客，文笔一般，调hexo markdown插件的bug就折腾了好久，我太难了。</p>
<p>本系列文章除了第三篇文章大篇幅借鉴了别人的观点，其余三篇均为基于事实的个人理解。欢迎有兴趣的小伙伴进一步讨论，如有错误或者更好的理解也欢迎指正和提出。未来这几天我也许会整合这四篇文章的知识，出一个GCN综述性质的文章~先在这个挖个坑…应该会来填的！！！！</p>
<blockquote id="fn_1">
<sup>1</sup>. 比如对于我们提到的切比雪夫多项式，可以得到如下矩阵迭代的形式 <a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<script type="math/tex; mode=display">
\begin{bmatrix}
T_n(t) \\
T_{n-1}(t) 
\end{bmatrix} = 
\begin{bmatrix}
2t &-1\\
1 &0 
\end{bmatrix}
\begin{bmatrix}
T_{n-1}(t) \\
T_{n-2}(t) 
\end{bmatrix}</script><script type="math/tex; mode=display">
\begin{bmatrix}
T_n(t) \\
T_{n-1}(t) 
\end{bmatrix} = 
\begin{bmatrix}
2t &-1\\
0 &1 
\end{bmatrix}^{n-1}
\begin{bmatrix}
T_{1}(t) \\
T_{0}(t) 
\end{bmatrix}=\begin{bmatrix}
2t &-1\\
1 &0 
\end{bmatrix}^{n-1}\begin{bmatrix}
t \\
1
\end{bmatrix}</script>]]></content>
      <categories>
        <category>GCN要用到的数学知识</category>
        <category>切比雪夫多项式</category>
      </categories>
      <tags>
        <tag>切比雪夫微分方程</tag>
        <tag>切比雪夫多项式</tag>
        <tag>切比雪夫逼近</tag>
      </tags>
  </entry>
  <entry>
    <title>GCN要用到的数学知识系列(三)</title>
    <url>/2019/12/15/GCN%E8%A6%81%E7%94%A8%E5%88%B0%E7%9A%84%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%89)/</url>
    <content><![CDATA[<h2 id="三、拉普拉斯矩阵"><a href="#三、拉普拉斯矩阵" class="headerlink" title="三、拉普拉斯矩阵"></a><strong>三、拉普拉斯矩阵</strong></h2><blockquote>
<p>GCN中的图傅里叶变换(Graph Fourier Transformation)和图卷积(Graph Convolution)的定义都用到了图的拉普拉斯矩阵，那么首先来介绍一下拉普拉斯矩阵。为了让读者更易理解拉普拉斯矩阵，这里借鉴了<a href="https://www.zhihu.com/question/54504471/answer/630639025" target="_blank" rel="noopener">某知乎大佬的文章</a>中的观点，因为发现他的角度实在是太妙了，但以我表达能力又无法转述，所以这里前半部分几乎是整段复制的大佬的文章，不过在一些小地方我也做了一些批注。</p>
</blockquote>
<a id="more"></a>
<p>众所周知，没有外接干预的情况下，热量从温度高传播到温度低的地方并且不可逆，根据著名的牛顿冷却定律（Newton Cool’s Law），热量传递的速度正比于温度梯度，直观上也就是某个地方A温度高，另外一个B地方温度低，这两个地方接触，那么温度高的地方的热量会以正比于他们俩温度差的速度从A流向B。</p>
<h3 id="从一维空间开始"><a href="#从一维空间开始" class="headerlink" title="从一维空间开始"></a><strong>从一维空间开始</strong></h3><p>我们先建立一个一维的温度传播的模型，假设有一个均匀的铁棒，不同位置温度不一样，现在我们刻画这个铁棒上面温度的热传播随着时间变化的关系。预先说明一下，一个连续的铁棒的热传播模型需要列<strong>温度对时间和坐标的偏微分方程</strong>来解决，我们不想把问题搞这么复杂，我们把<strong>空间离散化</strong>，假设铁棒是一个<strong>一维链条</strong>，链条上每一个单元拥有一致的温度，温度在相邻的不同的单元之间传播，如下图</p>
<p><img src="/2019/12/15/GCN%E8%A6%81%E7%94%A8%E5%88%B0%E7%9A%84%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%89)/链条.png" style="zoom:100%;"></p>
<p>对于第 $i$个单元，它只和 $i-1$与$i+1$两个单元相邻，接受它们传来的热量（或者向它们传递热量，只是正负号的差异而已），假设它当前的温度为，那么就有</p>
<script type="math/tex; mode=display">
{d\phi_i  \over dt} = k(\phi_{i+1} - \phi_i) - k(\phi_{i} - \phi_{i-1})</script><p>$k$和单元的比热容、质量有关是个常数。右边第一项是下一个单元向本单元的热量流入导致温度升高，第二项是本单元向上一个单元的热量流出导致温度降低。做一点微小的数学变换可以得到：</p>
<script type="math/tex; mode=display">
{d\phi_i  \over dt} = k(\phi_{i+1} - \phi_i) - k(\phi_{i} - \phi_{i-1}) = 0</script><p>注意观察第二项，它是两个差分的差分，在离散空间中，相邻位置的差分推广到连续空间就是<strong>导数</strong>，那么差分的差分，就是<strong>二阶导数</strong>！</p>
<p>所以，我们可以反推出铁棒这样的连续一维空间的热传导方程就是：</p>
<script type="math/tex; mode=display">
{\partial \phi_i  \over \partial t} = k{\partial^2 \phi_i  \over \partial x^2}</script><p>同理，在高维的欧氏空间中，一阶导数就推广到<strong>梯度</strong>，二阶导数就是我们今天讨论的主角——<strong>拉普拉斯算子</strong></p>
<p>其中 $ \Delta $这个符号代表的是对各个坐标二阶导数的加和，现在的主流写法也可以写作 $\nabla^2$</p>
<p>综上所述，我们发现这样几个事实：</p>
<p>1、在欧氏空间中，某个点温度升高的速度正比于该点周围的温度分布，用拉普拉斯算子衡量。</p>
<p>2、拉普拉斯算子，是二阶导数对高维空间的推广。</p>
<p>那么，你肯定会问：你扯这么多有什么用呢？我还是看不到拉普拉斯算子和拉普拉斯矩阵还有GCN有半毛钱关系啊？</p>
<p>不要急，目前只是第一步，让我们把这个热传导模型推广导拓扑空间，你就会发现它们其实刻画的是同一回事了！</p>
<h3 id="图-Graph-上热传播模型的推广"><a href="#图-Graph-上热传播模型的推广" class="headerlink" title="图(Graph)上热传播模型的推广"></a><strong>图(Graph)上热传播模型的推广</strong></h3><p>现在，我们依然考虑热传导模型，只是这个事情不发生在欧氏空间了，发生在一个Graph上面。这个图上的每个结点（Node）是一个单元，且这个单元只和与这个结点相连的单元，也就是有边（Edge）连接的单元发生热交换。例如下图中，结点$1$只和结点$0、2、4$发生热交换，更远的例如结点5的热量要通过$4$间接的传播过来而没有直接热交换。</p>
<p><img src="/2019/12/15/GCN%E8%A6%81%E7%94%A8%E5%88%B0%E7%9A%84%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%89)/温度.png" style="zoom:100%;"></p>
<p>我们假设热量流动的速度依然满足牛顿冷却定律，研究任一结点 $i$，它的温度随着时间的变化可以用下式来刻画</p>
<script type="math/tex; mode=display">
{d\phi \over dt} = -k \sum_{j}A_{ij}(\phi_i - \phi_j)</script><p>其中$A$是这个图的邻接矩阵(Adjacency Matrix)，定义非常直观： 对于这个矩阵中的每一个元素 ，如果节点$i$和$j$相邻，也就是它们之间存在一条边，那么$A<em>{ij}=1$，否则$A</em>{ij}=0$，我们只讨论简单那情况</p>
<ul>
<li><p>这张图是无向图，$i$和$j$相邻，那么$j$和$i$也相邻，也就是$A<em>{ij}=A</em>{ji}$，这是个对称阵。</p>
</li>
<li><p>节点自己到自己是没有回环边的，也就是$A$的对角线上的元素都为0。</p>
</li>
</ul>
<p>所以不难理解上面这个公式恰好表示了只有相邻的边才能和本结点发生热交换且热量输入（输出）正比于温度差。</p>
<p>我们不妨用乘法分配律稍微对上式做一个推导</p>
<script type="math/tex; mode=display">
\begin{align*}
{d\phi \over dt} =& -k [\phi_i\sum_{j}A_{ij}-\sum_{j}A_{ij}\phi_j] \\
=& -k [deg(i) \phi_i- \sum_{j}A_{ij}\phi_j]
\end{align*}</script><p>先看右边括号里面第一项，$deg(i)$代表对这个顶点求度（degree），一个顶点的度被定义为这个顶点有多少条边连接出去，很显然，根据邻接矩阵的定义，第一项的求和正是在计算顶点$i$的度。</p>
<p>再看右边括号里面的第二项，这可以认为是邻接矩阵的第$i$行对所有顶点的温度组成的向量做了个内积。</p>
<p>为什么要作上述变化呢，我们只看一个点的温度不太好看出来，我们把所有点的温度写成向量形式再描述上述关系就一目了然了。首先可以写成这样</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
d \phi_1 \over dt \\
\vdots \\
d \phi_n \over dt 
\end{bmatrix}
=-k \begin{bmatrix}
deg(1) \times \phi_1  \\
\vdots \\
deg(n) \times \phi_n 
\end{bmatrix} + kA
\begin{bmatrix}
\phi_1  \\
\vdots \\
\phi_n 
\end{bmatrix}</script><p>然后我们定义向量$ \phi = [ \phi_1, \phi_2,\cdots,\phi_n ] ^T$，那么就有</p>
<script type="math/tex; mode=display">
{d\phi \over dt }= -k D \phi + kA \phi = -k(D-A)\phi</script><p>其中$ D=diag(deg(1),deg(2),\cdots,deg(n)) $被称为度矩阵，只有对角线上有值，且这个值表示对应的顶点度的大小。整理整理，我们得到</p>
<script type="math/tex; mode=display">
{d \phi \over dt } + kL \phi = 0</script><p>回顾刚才在连续欧氏空间的那个微分方程</p>
<script type="math/tex; mode=display">
{\partial \phi \over \partial t} - k \Delta \phi = 0</script><p>二者具有一样的形式！我们对比下二者之间的关系</p>
<ul>
<li>相同点：刻画空间温度分布随时间的变化，且这个变化满足一个相同形式的微分方程。</li>
<li>不同点：<strong>前者刻画拓扑空间有限结点</strong>，用向量$\phi$来刻画当前状态，单位时间状态的变化正比于线性变换$-L$算子作用在状态$\phi$上。<strong>后者刻画欧氏空间的连续分布</strong>，用函数$\phi(x,t)$来刻画当前状态，单位时间状态变化正比于拉普拉斯算子$\Delta$作用在状态$\phi$上。</li>
</ul>
<p>不难发现，这就是<strong>同一种变换、同一种关系在不同空间上面的体现</strong>，实质上是一回事！</p>
<p>于是我们自然而然，可以把连续空间中的热传导，推广到图（Graph）上面去，我们把图上面和欧氏空间地位相同变换，以矩阵形式体现的$L$叫做拉普拉斯（Laplacian）矩阵。</p>
<p>需要多嘴一句的是，本文开头所说的离散链条上的热传导，如果你把链条看成一个图，结点从左到右编号1，2，3…的话，也可以用图的热传导方程刻画，此时$D$除了头尾两个结点是1其他值都是2，$A$的主对角线上下两条线上值是1，其他地方是0</p>
<h3 id="推广到GCN"><a href="#推广到GCN" class="headerlink" title="推广到GCN"></a><strong>推广到GCN</strong></h3><p>现在<strong>问题已经很明朗</strong>了，只要你给定了一个空间，给定了空间中存在一种东西可以在这个空间上流动，两邻点之间流动的强度正比于它们之间的状态差异，那么<strong>何止是热量可以在这个空间流动，任何东西都可以！</strong></p>
<p>自然而然，假设在图中各个结点流动的东西不是<strong>热量</strong>，而是<strong>特征（Feature）</strong>，而是<strong>消息（Message）</strong>，那么问题自然而然就被推广到了GCN。<strong>所以GCN的实质是什么，是在一张Graph Network中特征（Feature）和消息（Message）中的流动和传播！这个传播最原始的形态就是状态的变化正比于相应空间（这里是Graph空间）拉普拉斯算子作用在当前的状态。</strong></p>
<p>抓住了这个实质，剩下的问题就是怎么去更加好的建模和解决这个问题。</p>
<p>建模方面就衍生出了各种不同的算法，你可以在这个问题上面复杂化这个模型，不一定要遵从牛顿冷却定律，你可以引入核函数、引入神经网络等方法把模型建得更非线性更能刻画复杂关系。</p>
<p>解决方面，因为很多问题在频域解决更加好算，你可以通过Fourier变换把空域问题转化为频域问题，解完了再变换回来，于是便有了所有Fourier变换中的那些故事。</p>
<p>扯了这么多，总结一下，问题的本质就是：</p>
<ul>
<li>我们有张图，图上每个结点刻画一个实体，物理学场景下这个实体是某个有温度的单元，它的状态是温度，广告和推荐的场景下这个实体是一个user，一个item，一个ad，它的状态是一个embedding的向量。</li>
<li>相邻的结点具有比不相邻结点更密切的关系，物理学场景下，这个关系是空间上的临近、接触，广告和推荐场景下这个是一种逻辑上的关系，例如用户购买、点击item，item挂载ad。</li>
<li>结点可以传播热量/消息到邻居，使得相邻的结点在温度/特征上面更接近。</li>
</ul>
<p><strong>本质上，这是一种Message Passing，是一种Induction，卷积、傅立叶都是表象和解法。</strong></p>
<h3 id="拉普拉斯矩阵的性质"><a href="#拉普拉斯矩阵的性质" class="headerlink" title="拉普拉斯矩阵的性质"></a><strong>拉普拉斯矩阵的性质</strong></h3><p>$Therom :$拉普拉斯矩阵是半正定<sup><a href="#fn_1" id="reffn_1">1</a></sup>的</p>
<p>$Proof:$</p>
<script type="math/tex; mode=display">
\begin{align*}
x^TLx =& x^TDx-x^TWx \\
=& {1\over2}( 2\sum_{i=1}^{n}d_{ii}x_i^2 -2 \sum_{i=1}^{n} \sum_{j=1}^{n} x_ix_jw_{ij})\\
=& {1\over2}(\sum_{i=1}^{n}d_{ii}x_i^2 -2 \sum_{i=1}^{n} \sum_{j=1}^{n} x_ix_jw_{ij} + \sum_{j=1}^{n}d_{jj}x_j^2) \\
=&{1\over2}\sum_{i=1}^{n} \sum_{j=1}^{n}w_{ij}(f_i-f_j)^2
\geq 0
\end{align*}</script><p>这里忍不住多唠几句，实际上这里也可以用到图的哈密顿算子$\nabla$(我不知道在图谱理论里是不是这个叫法，但是这个符号确实是这么念，也用作梯度)。图的哈密顿算子就是图的关联矩阵的变形，图的关联矩阵以边为行，节点为列，那么对边标号，并且从小到大排列，那么对每一行来说，一条边必然关联着一个两个节点，我们把编号小的节点(也就是主元)记为$1$，编号大的节点记为$-1$，那么就可以到图的哈密顿算子$\nabla$，然后图的拉普拉斯矩阵$L$实际上可以分解为$\nabla^T\nabla$，这种分解其实也叫Cholesky分解()。因为前面我们讲过，拉普拉斯矩阵等价于$\nabla^2$嘛，那么这个结论其实还是挺显然的。从这一角度也可以很容易证明其半正定</p>
<script type="math/tex; mode=display">
x^TLx = x^T \nabla^T \nabla x=(\nabla x)^T\nabla x=||\nabla x||^2\geq 0</script><p>那么现在已知拉普拉斯矩阵是半正定的了，就可以得到很有趣的结论，比如特征值非负。而且也注意到这个矩阵是不可逆的，它有一个特征值$0$，但是GCN实际上用到的并不是原始的拉普拉斯矩阵，而是对称归一化的拉普拉斯矩阵，这又是个什么玩意呢？GCN为什么要用这么个玩意呢？</p>
<p>$Define:$对称归一化的拉普拉斯矩阵</p>
<script type="math/tex; mode=display">
L_{sym}=D^{-{1 \over 2}}LD^{-{1 \over 2}}=D^{-{1 \over 2}}(D-W)D^{-{1 \over 2}}=I-D^{-{1 \over 2}}WD^{-{1 \over 2}}</script><p>那么有小伙伴一定好奇了？归一化我见多了，这是个什么鬼归一化？它把谁归一了？我们先给出这个对称化矩阵的定义</p>
<script type="math/tex; mode=display">
L_{i,j}^{sym}=
\begin{cases}
1 &if \space i=j \space and \space deg \space(v_i) \neq 0\\
-{1 \over \sqrt{deg(v_i)deg(v_j)}} &if \space i \neq j \space and  \space v_i \space is \space adjacent \space to v_j\\
0 &otherwise
\end{cases}</script><p>我们不妨在此给出一个例子，比如如下是一个矩阵通过这种对称归一化的结果</p>
<script type="math/tex; mode=display">
L = \begin{bmatrix}
3 &-1 &-1 &-1\\
-1 &2 &-1 &0\\
-1 &-1 &3 &-1\\
-1 &0 &-1 &2
\end{bmatrix}
\stackrel { 归一化 } \longrightarrow L_{sym} = \begin {bmatrix}
1 & - { 1 \over \sqrt 6 } & - { 1 \over 3} & -{  1 \over \sqrt 6 } \\    
{ -1 \over \sqrt 6 } & 1  & - { 1 \over \sqrt 6 } & 0 \\
{ -1 \over 3 } & -{ 1 \over \sqrt 6 } & 1 & - { 1 \over \sqrt 6 } \\   
{- 1 \over \sqrt 6 } & 0 & - { 1 \over \sqrt6 } & 1
\end{bmatrix}</script><p>从这个例子可以明显看出归一化是把对角线归一化，之所以这样归一化是因为注意到未归一化的拉普拉斯矩阵中，只有主对角线上的值具有差异，而其他存在边的位置的特征都是$-1$，也就是说这个矩阵也许不能很好的反映不同结点之间边的特殊性(他们地位是相同的)。而如图的归一化则将不同结点的边之间的信息通过两个节点的度来衡量，进一步加强了节点之间的交互信息。</p>
<p>这里我再啰嗦下，我现在知道这样归一化的作用了，可是这个式子$D^{-{1 \over 2}}LD^{-{1 \over 2}}$是怎么得来的呢？注意到矩阵的左乘表示行的线性组合<sup><a href="#fn_2" id="reffn_2">2</a></sup>，而$D^{-{1 \over 2}}$又是对角阵，那么$D^{-{1 \over 2}}$的第$i$行只是筛选出$L$的第$i$行并对其附加权重$1 \over \sqrt {deg(v_i)}$，右乘则表示列的线性组合，那么同理，$D^{-{1 \over 2}}$的第$j$列只是筛选出$L$的第$j$列并对其附加权重$1 \over \sqrt{deg(v_j)}$。</p>
<p>显然归一化的矩阵也是半正定的，这里的证明其实也不复杂，显然$D^{-{1 \over 2}}= D^{-{1 \over 2}^T}$，这里用哈密顿算子证明比较明了$L_{sym}=D^{-{1 \over 2}^T} \nabla^T \nabla D ^{-{1 \over 2}} = (\nabla D^{-{1 \over 2}})^T  \nabla D^{ -{1 \over 2}}$，那么其半正定性也是显然的。</p>
<p>谢谢阅读，如有错误或者疏漏欢迎指正，码字不易…</p>
<p><sup><a href="#fn_1" id="reffn_1">1</a></sup>:实对称矩阵$A$称为半正定的，如果二次型$x^TAx$半正定，即对于任意不为$0$的实列向量$x$，都有$x^TAx \geq0$</p>
]]></content>
      <categories>
        <category>GCN要用到的数学知识</category>
        <category>拉普拉斯矩阵</category>
      </categories>
      <tags>
        <tag>拉普拉斯算子</tag>
        <tag>拉普拉斯矩阵</tag>
        <tag>无向图</tag>
        <tag>热力学定律</tag>
      </tags>
  </entry>
  <entry>
    <title>GCN要用到的数学知识系列(二)</title>
    <url>/2019/12/15/GCN%E8%A6%81%E7%94%A8%E5%88%B0%E7%9A%84%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%8C)/</url>
    <content><![CDATA[<h2 id="二、卷积的定义和性质"><a href="#二、卷积的定义和性质" class="headerlink" title="二、卷积的定义和性质"></a><strong>二、卷积的定义和性质</strong></h2><blockquote>
<p>卷积(Convolution)是一个在信号与系统以及机器学习领域极为常见的一个词，本文将从离散和连续两个角度来讲解卷积的定义与性质，同时结合上一讲的傅里叶变换，我们也证明后续在了解GCN时要用到的相关定理。</p>
</blockquote>
<a id="more"></a>
<h3 id="离散型卷积"><a href="#离散型卷积" class="headerlink" title="离散型卷积"></a><strong>离散型卷积</strong></h3><p>也许你玩过大型多人在线角色扮演游戏(MMORPG)游戏，这类游戏里往往总和<strong>挂dot</strong>离不开，所谓挂dot就是指释放一个技能，让对方持续掉血，而dot的英文就是Damage over time。那么问题来了，现在如果我们挂dot的技能冷却时间是$1$秒，也就是说我们每隔$1$秒就可以对敌人挂一个dot，这个dot伤害每次命中后持续$5$秒，dot命中后每秒对敌人造成$8$点伤害，并且之后每次再挂dot的伤害是之前的$1/2$，也就是说第二次挂dot命中后就是每秒造成就是$4$点伤害$\cdots$以此类推。那么问题来了，我们现在只通过挂dot，在$ns$内可以对敌人造成多少伤害？</p>
<p>这里我们先从一个简单的例子去考虑，假设我们求$4s$内的情况，时间区间的话考虑到边界条件，我们记其为左闭右开区间，就是$[0,4)$，那么容易知道，我们分别在$0s$，$1s，2s，3s$这四个时刻挂了dot。</p>
<p>$0s$时刻挂的dot持续了$4s$，总计造成了$8+8+8+8 = 32$点伤害</p>
<p>$1s$时刻挂的dot持续了$3s$，总计造成了$4+4+4 = 12$点伤害</p>
<p>$2s$时刻挂的dot持续了$2s$，总计造成了$2+2= 4$点伤害</p>
<p>$3s$时刻挂的dot持续了$1s$，总计造成了$1$点伤害</p>
<p>所以在这$4s$内，我们造成的总伤害是$32+12+4+1= 49$ 点。</p>
<p>那这个问题和我们今天的主角卷积又有什么关系呢？不急，我们慢慢来…</p>
<p>注意到上面问题的解的关键在于时间，我们隐约察觉到$ns$内的总伤害是可以写一个关于$n$的公式出来的。那么我们现在把问题一般化，初始条件下敌方身上无dot，那么第一个dot持续$t$秒可以造成的总伤害记为$g(t)$，那么显然有</p>
<script type="math/tex; mode=display">
g(t) = \begin{cases}
{8t}  & t \in \{ 1,2,3,4,5\}\\ 
40    & t \ge 6, t \in N 
\end{cases}</script><p>但是每挂一层dot，这层dot就衰减为上一次的$1/2$，这里记初始衰减为$1$，也就是没衰减，我们可以计算$ts$挂出去的dot应该衰减到了最初的多少，记其为$f(t)$，那么有</p>
<script type="math/tex; mode=display">
\begin{align*}
& f(t)=0.5^t \\
\end{align*}</script><p>易知，我们$ts$释放的dot获得的衰减为$f(t)$，这个dot实际作用时间是$n-t$，那么$n-t$秒内的无衰减的总伤害就是$g(n-t)$，那么显然$ts$时候命中的dot在这$n$秒以内造成的伤害是$f(t)g(n-t)$，我们记要求的$ns$内总伤害为$F(n)$，那么易知</p>
<script type="math/tex; mode=display">
F(n) = \sum_{t=0}^{n-1}f(t)g(n-t)</script><p>是不是有内味儿了？那现在我们把它推广到连续的情况。</p>
<h3 id="连续型卷积"><a href="#连续型卷积" class="headerlink" title="连续型卷积"></a><strong>连续型卷积</strong></h3><p>因为现实生活中很多东西是连续的，不像游戏中的dot可能$1s$才能造成一次伤害。其实信号与系统方面的卷积也是这么个意思。此时$f(t)$就是$t$时刻衰减的程度，也可以称之为信号此刻的强度或者幅值，那么$g(n-t)$就是这个信号作用$n-t$秒内的响应，那么$ns$时刻的输出是什么呢？</p>
<script type="math/tex; mode=display">
F(n) = \int_{0}^{\infty}f(t)g(n-t) \space dt</script><p>当然定义域也可以拓展为$(-\infty, \infty)$，这里时间的负无穷可以理解为相对时间。数学中通过用*来描述卷积，也就是记函数$f(t)$与函数$g(t)$的卷积为</p>
<script type="math/tex; mode=display">
f * g = \int_{-\infty}^{\infty} f(t) g(n-t) \space dt</script><p>至此，我们已经引入了卷积，那么有好奇的小伙伴也会问了？你讲的这个卷积和机器学习中的卷积是一回事吗？</p>
<h3 id="图像处理中的卷积"><a href="#图像处理中的卷积" class="headerlink" title="图像处理中的卷积"></a><strong>图像处理中的卷积</strong></h3><p>因为我们输入的图像是一个张量<sup><a href="#fn_1" id="reffn_1">1</a></sup>，比如$32 \times 32 \times 3 $，其中图片分辨率是$32\times32$，通道数为3，比如$RGB$三个通道。注意到我们之前讲述的离散卷积，它在做的事其实也可以看做是<strong>加权</strong>和<strong>求和</strong>，图像中的卷积往往也是利用了这两个性质，这部分知识是CNN(Convolutional Neural Network)中要用到的，但是我们不在此赘述CNN的细节，只是简单的给出一个例子，让读者能明白一个卷积可以对一个图像做什么事？</p>
<p>一个图像处理中非常有名的卷积，$\pmb{prewitt}$算子，在介绍它之前，我们先引入灰度的概念。一个图像的灰度最简单的求法就是取$RGB$三个通道的平均值，也就是$Gray=(R+G+B)/3$，之后令$R=G=B=$$Gray$即可得到一个$RGB$图像的灰度图，$Gray\in [0,255]$，灰度越大的点越趋近与白色，灰度越小的点约趋近于黑色。这个$prewitt$算子其实是一个二维的张量，所以其可以作用在一个灰度图上。这个算子往往用来做边缘检测，它的形式如下</p>
<script type="math/tex; mode=display">
V=\begin{bmatrix}
-1 &0, &1 \\
-1 &0, &1 \\
-1 &0, &1
\end{bmatrix}
H=\begin{bmatrix}
1 &1 &1 \\
0 &0 &0 \\
-1 &-1&-1
\end{bmatrix}</script><p>其中$H$用来检测水平边缘，$V$用来检测竖直边缘。那么它是如何做到边缘检测的呢？首先我们需要知道如何定义边缘。比如只含有白和黑两种颜色的图，这里我懒于画图，直接用矩阵的形式来表示这幅图的灰度图</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
255 &255 &255 &255 &255 &255 &255 &255\\
255 &255 &255 &255 &255 &255 &255 &255\\
255 &255 &0 &0 &0 &0 &255 &255\\
255 &255 &0 &0 &0 &0 &255 &255\\
255 &255 &0 &0 &0 &0 &255 &255\\
255 &255 &0 &0 &0 &0 &255 &255\\
255 &255 &255 &255 &255 &255 &255 &255\\
255 &255 &255 &255 &255 &255 &255 &255
\end{bmatrix}</script><p>$Perfect!!!!$这幅图中央为$0$点的点是黑色，周围$255$的点为白色，那么显然边缘产生在255和0之间的那两个矩形。</p>
<p>其实所谓边缘就是<strong>灰度变化大于阈值的点</strong>，如何衡量变化呢？函数中我们知道可以用导数或者梯度，可图像不是连续的，我们不能求导，那该如何做呢？<strong>差分</strong>来近似就可以了！这里$prewitt$算子水平方向用$ f ‘(x) = f(x + 1) - f(x - 1) $近似计算一阶差分，将其系数提出就得到了$[-1,0,1]$，垂直方向同理。</p>
<p>那么如果我们要在水平和垂直混合检测边缘只需要让$prewitt$算子的两个矩阵像CNN一样分别滑过矩阵，得到每个点的水平和垂直变化情况，分别记为$H(x,y),V(x,y)$，然后计算$\sqrt{H(x,y)^{2} + V(x,y)^{2}}$即可，如果这个值大于阈值，那么它就属于边缘。显然$prewitt$算子是利用像素点的上下、左右相邻点灰度差在边缘达到极值这一现象来检测边缘的</p>
<p>那么其实有心的读者应该考虑到了，<strong>$prewitt$算子和图像卷积可以用来提取边缘特征，那是否有一些卷积核可以用来提取其它特征吗？直觉告诉我们应该是有的，CNN就是基于这一思想，将卷积核作为待训练的参数，进而提取特征的。</strong></p>
<h3 id="卷积在傅里叶变换中的性质"><a href="#卷积在傅里叶变换中的性质" class="headerlink" title="卷积在傅里叶变换中的性质"></a><strong>卷积在傅里叶变换中的性质</strong></h3><p>GCN主要用到的性质就是<strong>卷积的傅里叶变换等于傅里叶变换的乘积</strong>，也就是</p>
<script type="math/tex; mode=display">
\mathcal F[f*g]=\mathcal F[f] \times  \mathcal F[g]</script><p>要证明这个定理其实很简单，注意到积分上下限都是无穷，所以可以直接交换积分次序，那么就有</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathcal F[f * g] =& \int_{-\infty}^{\infty} (\int_{-\infty}^{\infty} f(\tau) g(t - \tau) \space
 d \tau) \space e^{-jwt} dt \\
 =& \int_{-\infty}^{\infty} f(\tau) e^{-jw\tau}\space d\tau 
 \int_{-\infty}^{\infty} g(t-\tau) e^{-jw(t-\tau)}\space d(t-\tau) \\
 =& \mathcal F[f] \times \mathcal F[g]
\end{align*}</script><p>显然如果将这个推广到离散傅里叶变换中去的话，已知对一个自然基下的列向量做傅里叶变换可以得到它在新的基下的坐标<sup><a href="#fn_2" id="reffn_2">2</a></sup>也就是说</p>
<script type="math/tex; mode=display">
\mathcal x=F[v]= U^Tv</script><p>其中$U$是新的基下组成的矩阵，那么两个向量$v,u$的卷积的傅里叶变换就有</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathcal F[v*u]=&\mathcal F[v] \times \mathcal F[u] \\
=& U^Tv \times U^Tu
\end{align*}</script><p>这里的乘法其实是哈达马积(Hadamard product)，注意到$U^{T} v$以及$U^{T} u$都是相同维度的列向量，所以这里的乘积实际上也就是对应元素直接相乘。</p>
<p>谢谢阅读~~~如有错误或者疏漏欢迎指正。</p>
]]></content>
      <categories>
        <category>GCN要用到的数学知识</category>
        <category>卷积的定义和性质</category>
      </categories>
      <tags>
        <tag>离散卷积</tag>
        <tag>图形处理中的卷积</tag>
        <tag>傅里叶变换</tag>
      </tags>
  </entry>
  <entry>
    <title>GCN要用到的数学知识系列(一)</title>
    <url>/2019/12/14/GCN%E8%A6%81%E7%94%A8%E5%88%B0%E7%9A%84%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%80)/</url>
    <content><![CDATA[<p>本系列文章将啰里啰嗦的介绍GCN(Graph Neural Network)里用到的一些数学知识，我尽量从简单的地方慢慢讲起，但精力有限，还是需要阅读者有一定的数学基础，比如要熟悉线性代数中的线性空间、特征值与特征向量含义、半正定矩阵的性质等，同时要对梯度有一定的了解。另外这是我第一次写博客，表达能力不佳，如果有疏漏以及不正确的地方，还望大家指正。</p>
<p>系列文章预计分成四部分</p>
<ul>
<li>傅里叶变换</li>
<li>卷积的定义和性质</li>
<li>拉普拉斯矩阵</li>
<li>切比雪夫多项式</li>
</ul>
<h3 id="一、离散型傅里叶变换"><a href="#一、离散型傅里叶变换" class="headerlink" title="一、离散型傅里叶变换"></a>一、<strong>离散型傅里叶变换</strong></h3><blockquote>
<p>大多数讲解傅里叶变换的文章都是从时域和频域的角度引入傅里叶变换，但这种讲解往往过于抽象，这里为了便于理解，从大家熟知的线性空间中向量的线性组合引入离散傅里叶变换，其实如果要了解GCN的原理，傅里叶部分只需要了解到这里就可以了。但为了更好的认识傅里叶变换，我这里也额外给出了离散傅里叶变换推广到连续性傅里叶变换的情况，并继续把结果从离散谱推广到连续谱。</p>
</blockquote>
<a id="more"></a>
<p>易知任意一个$n$维列向量$v$可以由$n$个相互正交的$n$维列向量表示<sup><a href="#fn_1" id="reffn_1">1</a></sup>，那么</p>
<script type="math/tex; mode=display">
v=x_{1}q_{1} + x_{2}q_{2}+ x_{3}q_{3} + \cdot \cdot \cdot +x_{n}q_{n} \tag{1}</script><p>其中$x_i$是标量，${q_1,\cdots,q_n}$是$n$个$n$维单位列向量，也就是模为$1$，它们组成了$n$维空间的一组标准正交基，将$(1)$式写成矩阵的形式，也就是</p>
<script type="math/tex; mode=display">
v = \begin{bmatrix} q_1 &\cdots &q_n \end{bmatrix} \cdot \tag{2}
\begin{bmatrix}x_1 \\ \vdots \\  x_n \end{bmatrix} =
\begin{bmatrix} q_1 &\cdots &q_n \end{bmatrix} \cdot x</script><p>那么在$(1)$的基础上，我们如何在给定$v$和这样的一组$q$的基础上求得$x$呢？我们知道两个向量正交说明他们的内积为$0$，所以要求$x_i$只需在$(1)$的左右两边同时左乘$q_i^T$即可，也就是</p>
<script type="math/tex; mode=display">
q_i^T \cdot v = x_1q_i^T\cdot q_1 + \cdots +x_iq_i^T\cdot q_i + \cdots + x_1q_i^T\cdot q_n \tag{3}</script><p>只有第$i$项不为$0$，且$q_i^T\cdot q_i=1$<sup><a href="#fn_3" id="reffn_3">3</a></sup>，那么有</p>
<script type="math/tex; mode=display">
q_i^T \cdot v = x_i \tag{4}</script><p>我们在此基础上可以通过$n$个这样的式子求得所有的$q_i$，我们将$n$个式子组合写成矩阵相乘的形式，也就是</p>
<script type="math/tex; mode=display">
\begin{bmatrix} q_1^T \\ \vdots  \\ q_n^T \end{bmatrix} \cdot v =\begin{bmatrix} q_1 &\cdots &q_n \end{bmatrix}^T \cdot v = 
\begin{bmatrix}x_1 \\ \vdots \\ x_n\end{bmatrix} = x \tag{5}</script><p>那么我们将$(2)$和$(5)$单独拿出来看，并借助这两个式子推广到连续性傅里叶变换</p>
<script type="math/tex; mode=display">
v = \begin{bmatrix} q_1 &\cdots &q_n \end{bmatrix} \cdot x \\
x = \begin{bmatrix} q_1 &\cdots &q_n \end{bmatrix}^T \cdot v</script><p>实际上这两个式子在做什么呢？已知一个向量$v$，其定义在自然基<sup><a href="#fn_2" id="reffn_2">2</a></sup>上，那么$v$也可以看做其本身在自然基下的坐标，然后我们给出另外一组标准正交基$q$，根据$(1)$式和$(5)$式我们就可以得到向量$v$在新的基$q$上的投影$x_i$，进而得到在这些新的基上的坐标$x$。所以矩阵中的离散傅里叶变换实际上就是<strong>将一个自然基下的向量$v$，其坐标也为$v$，转移到另外一个方便研究的直角坐标系上，新坐标系的基为$q$，坐标为$x$</strong>，那么逆变换就是反之。</p>
<p>可是这件事和连续性傅里叶变换有什么关系呢？我们又该如何将其推广到我们熟悉的连续性傅里叶变换的公式呢？</p>
<script type="math/tex; mode=display">
f(x) = a_0 + a_1cosx+\cdots + a_ncosnx + b_1sinx + \cdots +b_nsinnx \tag{6}</script><p>类比离散型傅里叶变换，直觉告诉我们可以把$a_i$和$b_i$这些理解为坐标，是待求量，而这一系列三角函数就可以看做是基。那么问题就在于这组基正交吗？这组基的模为$1$吗？函数的正交和模又是如何定义的呢？</p>
<p>注意到两个向量的正交是<strong>对应分量</strong>的乘积的<strong>和</strong>为0，那么类比到函数，函数的分量就是<strong>各个点的函数值</strong>，而求和在函数上的操作就是<strong>积分</strong>。</p>
<p>也就是说如果是向量$f$和$g$正交只需要</p>
<script type="math/tex; mode=display">
\begin{bmatrix} f_1 & \cdots & f_n \end{bmatrix} \cdot 
\begin{bmatrix} g_1 \\ \vdots \\ g_n \end{bmatrix}
= \sum_{i=1}^{n}f_i g_i = 0 \tag{7}</script><p>那么函数$f(x)$和函数$g(x)$正交的话就是</p>
<script type="math/tex; mode=display">
\int_{-\infty}^{\infty}f(x)g(x) \space dx=0</script><p>而我们所熟知的傅里叶变换指的是周期为$2\pi$的函数变换，也就是说</p>
<script type="math/tex; mode=display">
\int_{-\pi}^{\pi}f(x) g(x) \space dx = 0 \tag{8}</script><p>容易验证$1,cosx,sinx,cos2x,sin2x,\cdots,cosnx,sinnx,\cdots$任意二者的积在$-\pi$到$\pi$上的积分为$0$，那么正交是满足了，可这些函数的模又是多少呢？</p>
<p>同样是类比向量的模，一个向量的模的平方就是它与它自身内积的结果，那么容易类比到函数的模的平方就是它自身的平方的积分，这里容易验证$1,cosx,sinx,cos2x,sin2x,\cdots,cosnx,sinnx,\cdots$中$1^2$在$-\pi$到$\pi$的积分为$2\pi$，而其余任意一个函数的平方在$-\pi$到$\pi$的积分均为$\pi$，很可惜他们的模不是$1$，但我们知道模为$1$只是方便运算而已，并不影响我们整个推导过程。</p>
<p>那么我们利用向量中求$x_i$的方法，类比到函数中去。我们现在要求的是对应的三角函数前的系数，比如我们求$cosnx$前的系数$a_n$，那么我们只需要在$(6)$式左右两边同时乘以$cosnx$然后在$-\pi$到$\pi$积分就可以了，也就是</p>
<script type="math/tex; mode=display">
\int_{- \pi}^{\pi} f(x) \cdot cosnx \space dx = \int_{- \pi}^{\pi}a_0 \cdot cosnx \space dx
 \space + \cdots + \int_{- \pi}^{\pi} a_ncosnx \cdot cosnx \space dx + \cdots \tag{6}</script><p>利用前面得到正交性，化简后就可以得到</p>
<script type="math/tex; mode=display">
\int_{- \pi}^{\pi} f(x) \cdot cosnx \space dx = \int_{- \pi}^{\pi} a_ncosnx \cdot cosnx \space   dx 
= \int_{- \pi}^{\pi} a_n{ (1 + cos 2nx)\over 2} 
= \pi a_n + 0
= \pi a_n \tag{7}</script><p>那么$a<em>n$的系数就是$1/ \pi\int</em>{-\pi}^{\pi}f(x)\cdot cosnx$,同理我们也可以求得其他的系数。这里再啰嗦两句，实际上前方的系数$1/\pi$中的的$\pi$就是这个函数的模的平方，如果这里求的不是$cosnx$而是$1$的系数的话，那么分母就是$2\pi$，原因就是我的函数的模没有归一化。而这里之所以$1$这个函数的模与其他不一样实际上是因为$cosnx$和$sinnx$如果$n$代入$0$的话都是$1$，所以这里实际上是两个$\pi$。</p>
<p>其实我们所熟知的傅里叶变换在形式上并不是那么的好看，我们也可以通过欧拉公式得到一个更简洁的傅里叶变换</p>
<script type="math/tex; mode=display">
cos \theta = {e^{j\theta} + e^{-j\theta} \over 2}    \\ \tag{9}
sin \theta = {e^{j\theta} - e^{-j\theta} \over 2j}</script><p>将$(9)$中的式子代入$(6)$中即可，这里的代入并不复杂，我们直接给出结论，有兴趣的读者可以自行推导</p>
<script type="math/tex; mode=display">
f(x)=\sum_{n = -\infty}^{\infty} c_n e^{jnx} \tag{10}</script><p>注意到此时的频谱是离散谱<sup><a href="#fn_4" id="reffn_4">4</a></sup>，我们同样可以将其推广为连续谱，显然$c_n$就变成了关于$\omega$<sup><a href="#fn_5" id="reffn_5">5</a></sup>的函数，求和就变为了积分，我们此时有</p>
<script type="math/tex; mode=display">
f(x)=\int_{-\infty}^{\infty} \tilde{F} (\omega)e^{j\omega x} d\omega \tag{11}</script><p>$1/2\pi\tilde{F}(\omega)$的实际上就是我们通过对$f(x)$傅里叶变换得到的函数$F(\omega)$， 但我们在此处不证，具体求$c_n$的以及$F(\omega)$的方式与前面给出的推导类似，都是利用其正交性左乘某一个分量然后积分，有兴趣的可以自行探究。</p>
<p>那么至此，将离散的矩阵中的傅里叶变换推广到了连续的函数的傅里叶变换，我们已经完成。在这结束之时，我也顺便给出一个从连续性傅里叶变换看待离散型傅里叶变换的观点。向量<strong>$v$在就是时域我们实际观测到向量，而$x$就是转化到频域内对应的频率分量的幅度  ，如果$x_i$为$0$则说明频谱内没有$q_i$这个频率分量</strong>。</p>
<blockquote id="fn_1">
<sup>1</sup>. 比如在平面直角坐标系这个二维空间里，任意向量都可以表示为$(1,0)$和$(0,1)$的线性组合<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
]]></content>
      <categories>
        <category>GCN要用到的数学知识</category>
        <category>傅里叶变换</category>
      </categories>
      <tags>
        <tag>线性空间</tag>
        <tag>连续型傅里叶变换</tag>
        <tag>离散型傅里叶变换</tag>
      </tags>
  </entry>
</search>
